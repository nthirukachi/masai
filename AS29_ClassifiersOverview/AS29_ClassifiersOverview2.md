# AS29: Classifiers Overview - Classroom Session (Part 2)

> ğŸ“š **This is Part 2** covering: Support Vector Machine (SVM), Decision Trees
> ğŸ“˜ **Previous:** [Part 1](./AS29_ClassifiersOverview1.md) | **Next:** [Part 3](./AS29_ClassifiersOverview3.md)

---

## ğŸ“ Classroom Conversation (Continued)

**Teacher:** Okay students, Part 1 lo manaamu Classification Basics and KNN chala detail ga chusinamu. Ippudu mana second major classifier ki vasthunnam â€” **Support Vector Machine (SVM)**! Idi machine learning lo **most mathematical and sophisticated** algorithm!

---

### Topic 1: Support Vector Machine (SVM) â€” Deep Dive

#### 12-Point Structure for SVM

---

#### ğŸ”¹ Point 1: Definition

**Teacher:** Support Vector Machine (SVM) ante â€” **two classes madhya maximum separating boundary (hyperplane) kanukkovadaniki optimize chese algorithm**. Just any boundary kaadu â€” **best possible boundary** â€” which is as far away as possible from the nearest points of every class.

Simple ga cheppali ante: "SVM builds the widest possible highway between two cities, keeping maximum distance from both sides"

10-year-old ki explain cheyali ante: "Cricket ground lo two teams practice chesthunnaru â€” red team oka side, blue team oka side. SVM is the referee who draws the widest possible chalk line in the middle, keeping maximum distance from both teams' players!"

> ğŸ’¡ **Jargon Alert - Support Vector Machine (SVM)**
> Simple Explanation: Two classes madhya widest possible boundary draw chese algorithm. "Support Vectors" are the closest points from each class to the boundary â€” they "support" (define) the boundary position.
> Example: Road lo partition â€” divider maximum distance lo place chesinatu, both sides ki accidents avoid avvadaniki.

> ğŸ’¡ **Jargon Alert - Hyperplane**
> Simple Explanation: Classes ni separate chese decision boundary. 2D lo it's a LINE. 3D lo it's a PLANE. Higher dimensions lo it's a HYPERPLANE.
> Example: Cricket ground lo boundary line â€” divides two playing areas.

---

#### ğŸ”¹ Point 2: Simple Analogy

**Teacher:** Best analogy â€” **"Road Divider"** analogy:

Imagine oka road â€” left side lo cars (Class A), right side lo trucks (Class B). You want to build a divider.

- **Bad divider:** Cars ki chala daggaraga â€” accident risk high!
- **Good divider:** Cars and trucks rendu ki maximum distance lo â€” SAFEST!
- **SVM's divider:** Widest possible gap in the middle â€” **maximum margin!**

Now imagine some cars are parked wrong side â€” these are **misclassifications**. SVM lo:
- **Hard margin:** No car should be on wrong side â€” strict! (Only if perfectly separable)
- **Soft margin:** Few cars on wrong side allowed, but with penalty â€” practical!

The closest cars/trucks to the divider are **Support Vectors** â€” they define where the divider goes!

```
                Support Vector (red)
                      ğŸ”´
        ğŸ”´     ğŸ”´     |  â† Margin â†’  |     ğŸ”µ     ğŸ”µ
   Red Class           |  HYPERPLANE  |          Blue Class
        ğŸ”´     ğŸ”´     |              |     ğŸ”µ
                      ğŸ”µ
                Support Vector (blue)
```

---

#### ğŸ”¹ Point 3: Why SVM is Used

**Teacher:** SVM enduku use chestharu?

| Problem SVM Solves | How |
|--------------------|-----|
| Need maximum separation between classes | Maximizes margin â€” best generalization |
| High-dimensional data | Works well even with more features than samples |
| Non-linear data | Kernel trick transforms to higher dimensions |
| Outlier handling | Soft margin ignores/penalizes outliers |
| Binary classification | Originally designed for binary â€” very strong |

**Clever Student:** Sir, SVM ki Logistic Regression ki difference enti? Both boundary draw chesthay kadha?

**Teacher:** Excellent question! Key differences:

| Aspect | Logistic Regression | SVM |
|--------|-------------------|-----|
| Boundary optimization | Any separating line | **Maximum margin** line |
| Loss function | Log Loss (Cross Entropy) | **Hinge Loss** |
| Output | Probabilities (0 to 1) | Decision (which side) â€” no natural probabilities |
| Outlier handling | All points influence | Only **support vectors** matter |
| Non-linear | Cannot handle | **Kernel trick** handles non-linear |
| Interpretability | Moderate | Low |

**Beginner Student:** Sir, "maximum margin" ante exact ga enti?

**Teacher:** Beautiful question! Let me explain with diagram:

```mermaid
flowchart LR
    subgraph Bad["âŒ Bad Boundary (Narrow Margin)"]
        direction TB
        R1["ğŸ”´ ğŸ”´ ğŸ”´"] --- L1["â”€â”€ boundary â”€â”€"] --- B1["ğŸ”µ ğŸ”µ ğŸ”µ"]
    end
    subgraph Good["âœ… SVM Boundary (Maximum Margin)"]
        direction TB
        R2["ğŸ”´ ğŸ”´ ğŸ”´"] --- M1["â”€ â”€ â”€ support vector â”€ â”€ â”€"]
        M1 --- L2["â•â•â•â• HYPERPLANE â•â•â•â•"]
        L2 --- M2["â”€ â”€ â”€ support vector â”€ â”€ â”€"]
        M2 --- B2["ğŸ”µ ğŸ”µ ğŸ”µ"]
    end
```

Good boundary â€” widest possible gap between the closest points of each class. Narrow margin = risky. Wide margin = safe, generalizes better to new data!

---

#### ğŸ”¹ Point 4: When to Use SVM

**Teacher:** SVM best use cheyali conditions:

| Condition | Why SVM Works |
|-----------|---------------|
| High-dimensional data | Mathematical optimization handles high dims well |
| More features than samples | Works even when features > samples |
| Need best accuracy | Maximum margin gives strong generalization |
| Non-linear data | Kernel trick elegantly handles non-linearity |
| Medium-sized dataset | Best for 1,000 - 100,000 samples |
| Binary classification | Originally designed for binary â€” strongest here |

**When NOT to use SVM:**
- Very large datasets (> 100,000 samples) â€” training too slow (O(nÂ²) to O(nÂ³))
- Need probability estimates â€” SVM doesn't give natural probabilities
- Need interpretability â€” SVM is black box
- Need fast training â€” SVM training is computationally expensive
- Limited tuning time â€” kernel and parameter selection is tricky

---

#### ğŸ”¹ Point 5: Where SVM is Used (Real-World)

```mermaid
flowchart TD
    A[SVM Use Cases] --> B[Text Classification]
    A --> C[Image Recognition]
    A --> D[Bioinformatics]
    A --> E[Handwriting Recognition]
    A --> F[Medical Imaging]
    B --> B1["Spam detection<br>Sentiment analysis<br>News categorization"]
    C --> C1["Object detection<br>Face recognition<br>Character recognition"]
    D --> D1["Gene expression data<br>Protein classification<br>Few samples, many features"]
    E --> E1["Digit recognition<br>Handwritten text<br>SVM outperforms many models here!"]
    F --> F1["X-ray classification<br>Tumor detection<br>Small but high-quality datasets"]
```

**Practical Student:** Sir, industry lo SVM actually use hota hai? Neural networks zyada popular hain na?

**Teacher:** Very good observation! Neural networks definitely more popular now for large datasets. But SVM still shines in:
1. **Small datasets, high features** â€” Gene expression analysis (1000 samples, 20000 genes)
2. **Text classification** â€” Still competitive with simpler SVMs
3. **When interpretability not needed but accuracy is critical** â€” Medical diagnosis with limited data
4. **Handwriting recognition** â€” SVMs historically outperformed neural networks on MNIST

---

#### ğŸ”¹ Point 6: Is SVM the Only Way? (Alternatives Comparison)

| Algorithm | Boundary Type | Non-linear? | Memory | Interpretable? | Best Scenario |
|-----------|--------------|-------------|--------|---------------|---------------|
| **SVM** | Maximum margin hyperplane | Yes (kernels) | Low (support vectors only) | âŒ No | High-dim, accuracy-critical |
| **Logistic Regression** | Any separating line | âŒ No | Very Low | âœ… Moderate | Simple binary, probabilities |
| **KNN** | Implicit (from neighbors) | âœ… Yes | High (all data) | âŒ No | Small data, prototyping |
| **Decision Tree** | Axis-aligned rectangles | âœ… Yes | Low | âœ… High | Interpretable, mixed data |
| **Random Forest** | Ensemble of trees | âœ… Yes | Medium | âœ… Moderate | General purpose, robust |

**Debate Student:** Sir, SVM vs Neural Network â€” eppudu edi better?

**Teacher:** Really good debate topic!

| Aspect | SVM | Neural Network |
|--------|-----|----------------|
| Small data (< 10K) | âœ… Better | âŒ Needs more data |
| Large data (> 100K) | âŒ Too slow | âœ… Better |
| High features, few samples | âœ… Excellent | âŒ Overfits |
| Training time | Moderate | Very Long |
| Interpretability | Low | Very Low |
| Probability output | âŒ Not natural | âœ… Natural (softmax) |
| Non-linear patterns | âœ… Via kernels | âœ… Via hidden layers |

Rule of thumb: **Small data + high features â†’ SVM. Large data + complex patterns â†’ Neural Network.**

---

#### ğŸ”¹ Point 7: Mermaid Diagram â€” SVM Concepts

```mermaid
flowchart TD
    A["SVM Building Blocks"] --> B["ğŸ“ Hyperplane<br>Decision boundary<br>WX + B = 0"]
    A --> C["ğŸ“ Support Vectors<br>Closest points from<br>each class to boundary"]
    A --> D["â†”ï¸ Margin<br>Distance between<br>support vectors<br>= 2/||W||"]
    A --> E["ğŸ”„ Kernel<br>Function to handle<br>non-linear data"]
    
    B --> F["Position: determined<br>by W (weight) & B (bias)"]
    C --> G["Only these points<br>influence the boundary<br>Other points ignored!"]
    D --> H["Goal: MAXIMIZE margin<br>Wider margin =<br>Better generalization"]
    E --> I["Linear: straight line<br>RBF: circular/complex<br>Polynomial: curved"]
    
    style H fill:#51cf66,color:#fff
```

---

#### ğŸ”¹ Point 8: How to Use SVM (Key Concepts + Code)

**Teacher:** SVM key concepts samjham karte hain:

##### ğŸ”‘ Concept 1: Hard Margin vs Soft Margin

**Hard Margin:**
- **Zero misclassification** allowed
- Only works if data is **perfectly separable**
- Very sensitive to noise â€” one outlier can destroy boundary
- Not practical for real data!

**Soft Margin:**
- **Some misclassification** allowed with **penalty**
- Penalty controlled by parameter **C (regularization)**
- Practical for real-world noisy data!
- Trade-off between margin width and classification errors

> ğŸ’¡ **Jargon Alert - Soft Margin**
> Simple Explanation: "Kuch galtiyan hone do, lekin penalty lagao!" â€” Some errors allowed but you pay a price.
> Example: Like exam evaluation â€” spelling mistakes allowed but some marks deducted per mistake.

> ğŸ’¡ **Jargon Alert - C Parameter (Regularization)**
> Simple Explanation: C controls penalty for mistakes. Large C = strict teacher, no mistakes allowed. Small C = lenient teacher, some mistakes okay.
> Example: C = 1000 â†’ narrow margin, very few errors. C = 0.01 â†’ wide margin, more errors tolerated.

| C Value | Margin Width | Misclassification Tolerance | Risk |
|---------|-------------|---------------------------|------|
| Large C (100+) | Narrow | Almost zero | **Overfitting** |
| Medium C (1-10) | Balanced | Some allowed | **Usually best** |
| Small C (0.01) | Wide | Many allowed | **Underfitting** |

##### ğŸ”‘ Concept 2: Hinge Loss

> ğŸ’¡ **Jargon Alert - Hinge Loss**
> Simple Explanation: SVM ka loss function â€” agar classification correct hai aur margin ke andar nahi hai, then loss = 0. Agar wrong side hai ya margin violate karta hai, then penalty milti hai!
> Example: Like parking fine â€” park in correct zone = no fine. Park slightly outside = small fine. Park completely wrong = big fine!

```
Hinge Loss:
  If correctly classified AND outside margin â†’ Loss = 0 (no penalty!)
  If inside margin or misclassified â†’ Loss > 0 (penalty!)
  
  L = max(0, 1 - y * (WÂ·X + B))
  
  Where y = actual label (+1 or -1)
        WÂ·X + B = prediction score
```

##### ğŸ”‘ Concept 3: Kernel Trick

**Teacher:** Idi SVM's **superpower!** ğŸ¦¸

Problem: Data is NOT linearly separable in current dimensions â€” no straight line can separate classes.

Solution: **Transform data to higher dimensions where it BECOMES separable!**

> ğŸ’¡ **Jargon Alert - Kernel Trick**
> Simple Explanation: Data ni higher dimensions lo transform chesi, linearly separable cheyyadam â€” without actually computing the higher-dimensional coordinates! Math magic!
> Example: Imagine mixing colored balls on a table (2D) â€” red and blue mixed, no straight line separates them. Now THROW them up in the air (3D) â€” suddenly a flat sheet of paper can separate red (flying high) from blue (flying low)!

**Kernel types:**

| Kernel | Type | Boundary Shape | When to Use |
|--------|------|---------------|-------------|
| `linear` | Linear | Straight line | Linearly separable data |
| `rbf` (default) | Non-linear | Circular, complex curves | **Most common** â€” works for most data |
| `poly` | Non-linear | Polynomial curves | When polynomial relationship expected |
| `sigmoid` | Non-linear | S-shaped | Similar to neural network behavior |

```python
# ============================================
# SVM with Different Kernels
# ============================================
from sklearn.svm import SVC
from sklearn.datasets import make_classification, make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Non-linear data (two half-moons)
X, y = make_moons(n_samples=500, noise=0.2, random_state=42)

# IMPORTANT: Scale features before SVM!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Try different kernels
kernels = ['linear', 'rbf', 'poly']
for kernel in kernels:
    svm = SVC(kernel=kernel, C=1.0, random_state=42)
    svm.fit(X_train, y_train)
    
    train_acc = accuracy_score(y_train, svm.predict(X_train))
    test_acc = accuracy_score(y_test, svm.predict(X_test))
    n_sv = svm.n_support_  # Number of support vectors per class
    
    print(f"Kernel: {kernel:8s} | Train: {train_acc:.3f} | Test: {test_acc:.3f} | "
          f"Support Vectors: {n_sv}")
```

**Expected Output:**
```
Kernel: linear   | Train: 0.860 | Test: 0.850 | Support Vectors: [78 82]
Kernel: rbf      | Train: 0.990 | Test: 0.980 | Support Vectors: [25 28]
Kernel: poly     | Train: 0.975 | Test: 0.960 | Support Vectors: [35 32]
```

**Clever Student:** Sir, RBF kernel lo "gamma" parameter kya hai?

**Teacher:** Great question! `gamma` defines how far the influence of a single training example reaches.

| Gamma | Effect | Risk |
|-------|--------|------|
| Small gamma | Large influence radius â†’ Smooth boundary | Underfitting |
| Large gamma | Small influence radius â†’ Tight, complex boundary | Overfitting |

```python
# ============================================
# Effect of C and Gamma on SVM
# ============================================
from sklearn.svm import SVC
import numpy as np

X, y = make_moons(n_samples=300, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Different C and gamma combinations
params = [
    {'C': 0.01, 'gamma': 'scale', 'desc': 'Small C (lenient)'},
    {'C': 1.0,  'gamma': 'scale', 'desc': 'Medium C (balanced)'},
    {'C': 100,  'gamma': 'scale', 'desc': 'Large C (strict)'},
    {'C': 1.0,  'gamma': 0.01,   'desc': 'Small gamma (smooth)'},
    {'C': 1.0,  'gamma': 10,     'desc': 'Large gamma (complex)'},
]

for p in params:
    svm = SVC(kernel='rbf', C=p['C'], gamma=p['gamma'])
    svm.fit(X_train, y_train)
    print(f"{p['desc']:30s} | Train: {svm.score(X_train, y_train):.3f} "
          f"| Test: {svm.score(X_test, y_test):.3f}")
```

---

#### ğŸ”¹ Point 9: How SVM Works Internally

**Teacher:** SVM internal working â€” recipe steps:

```
Step 1: OPTIMIZATION SETUP
   â”œâ”€â”€ Define objective: MAXIMIZE margin (= 2/||W||)
   â”œâ”€â”€ Subject to constraint: All points correctly classified
   â”‚   (yi * (WÂ·Xi + B) â‰¥ 1 for all training points)
   â””â”€â”€ This is a constrained optimization problem!

Step 2: TRAINING (Quadratic Programming)
   â”œâ”€â”€ Solve optimization to find W and B
   â”œâ”€â”€ Only support vectors (closest points) matter for solution
   â”œâ”€â”€ Non-support-vector points can be removed without changing result!
   â””â”€â”€ Training time: O(nÂ²) to O(nÂ³) â€” slow for large n

Step 3: KERNEL TRICK (if non-linear)
   â”œâ”€â”€ Instead of explicitly computing high-dim features
   â”œâ”€â”€ Use kernel function K(xi, xj) to compute dot products
   â”œâ”€â”€ RBF kernel: K(xi,xj) = exp(-Î³ ||xi-xj||Â²)
   â””â”€â”€ Avoids "curse of dimensionality" of explicit transformation

Step 4: PREDICTION (Fast! Only need W, B, support vectors)
   â”œâ”€â”€ Compute score = WÂ·X_new + B
   â”œâ”€â”€ If score > 0 â†’ Class +1
   â”œâ”€â”€ If score < 0 â†’ Class -1
   â””â”€â”€ Prediction time: O(n_sv * d) where n_sv = support vectors, d = dimensions
```

```mermaid
flowchart TD
    A["Training Data"] --> B["Find Support Vectors<br>(closest points to boundary)"]
    B --> C["Optimize W, B to<br>MAXIMIZE margin"]
    C --> D{"Data linearly<br>separable?"}
    D -->|Yes| E["Linear SVM<br>Straight boundary"]
    D -->|No| F["Apply Kernel Trick<br>Map to higher dimensions"]
    F --> G["RBF / Poly kernel<br>Find boundary in<br>high dimensions"]
    E --> H["Store: W, B,<br>Support Vectors"]
    G --> H
    H --> I["New point arrives"]
    I --> J["Score = WÂ·X + B"]
    J --> K{Score > 0?}
    K -->|Yes| L["Class +1 âœ…"]
    K -->|No| M["Class -1 âœ…"]
```

**Curious Student:** Sir, SVM learns parameters? KNN toh lazy tha, SVM?

**Teacher:** Yes! SVM **actively learns** â€” it learns W (weights) and B (bias) during training. This is opposite of KNN:

| Aspect | KNN | SVM |
|--------|-----|-----|
| Training | No learning â€” stores data | Learns W, B through optimization |
| Prediction | Calculates distances (slow) | Evaluates WÂ·X+B (fast) |
| Memory | Stores ALL training data | Stores only support vectors |
| Model size | = Full dataset | = Support vectors only (much smaller!) |

---

#### ğŸ”¹ Point 10: Visual Summary

```
ğŸ”‘ SVM = Find Maximum Margin Boundary

ğŸ“‹ TRAINING:
   Input Data â†’ [Optimization: Maximize Margin] â†’ [Find W, B, Support Vectors] â†’ Done!

ğŸ”® PREDICTION:
   New Point â†’ [Score = WÂ·X + B] â†’ Score > 0? â†’ Class +1 or Class -1

âš™ï¸ KEY HYPERPARAMETERS:
   C = Regularization (penalty for misclassification)
   â”œâ”€â”€ Large C: Strict â†’ Narrow margin â†’ Overfitting risk
   â””â”€â”€ Small C: Lenient â†’ Wide margin â†’ Underfitting risk
   
   Kernel = How to handle non-linear data
   â”œâ”€â”€ 'linear': Straight line boundary
   â”œâ”€â”€ 'rbf': Complex curves (DEFAULT, most common)
   â””â”€â”€ 'poly': Polynomial curves
   
   gamma = Influence radius (for RBF kernel)
   â”œâ”€â”€ Small gamma: Smooth, simple boundary
   â””â”€â”€ Large gamma: Complex, tight boundary

âš ï¸ MANDATORY PREPROCESSING:
   Feature Scaling REQUIRED! (StandardScaler recommended)
```

---

#### ğŸ”¹ Point 11: Advantages & Disadvantages (WITH PROOF)

##### âœ… Advantages:

**Advantage 1: Excellent in High-Dimensional Spaces**
- **Claim:** SVM performs remarkably well with many features, even more features than samples
- **Proof:** In text classification, you might have 50,000+ word features but only 5,000 documents. SVM handles this because it finds the optimal separating hyperplane regardless of dimensionality.
- **Real-Life Analogy:** Like finding the thinnest wall that separates two rooms â€” works whether rooms are 2D, 3D, or 100D!
- **When it matters:** Bioinformatics (20,000 genes, 200 patients), text classification (huge vocabulary)

**Advantage 2: Memory Efficient**
- **Claim:** SVM only stores support vectors, not entire training dataset
- **Proof:** If training data has 100,000 points, SVM might only store 500 support vectors (0.5% of data!)
- **Real-Life Analogy:** Instead of remembering every student in school, you only remember the "borderline" students â€” those barely passing and barely failing. Others don't affect the pass/fail boundary!
- **When it matters:** Deployment on memory-constrained devices

```python
# Proof: SVM stores much less data than KNN
from sklearn.svm import SVC
import numpy as np

X = np.random.randn(10000, 20)  # 10,000 samples
y = np.random.randint(0, 2, 10000)

svm = SVC(kernel='rbf')
svm.fit(X, y)

print(f"Training samples: {len(X)}")
print(f"Support vectors: {len(svm.support_vectors_)}")
print(f"% stored: {len(svm.support_vectors_)/len(X)*100:.1f}%")
# Support vectors â‰ˆ 30-50% (much less than KNN's 100%)
```

**Advantage 3: Versatile Through Kernels**
- **Claim:** Can handle linear AND non-linear boundaries with same algorithm
- **Proof:** Just change kernel parameter â€” `kernel='linear'` vs `kernel='rbf'` â€” same SVC class!
- **Real-Life Analogy:** Like a Swiss Army knife â€” same tool, different blade for different situations!
- **When it matters:** When you don't know if data is linear or non-linear

**Advantage 4: Robust to Outliers (Soft Margin)**
- **Claim:** With soft margin (C parameter), SVM can handle noisy data and outliers
- **Proof:** Unlike hard margin SVM, soft margin allows controlled misclassifications. Small C = more tolerance for outliers.
- **Real-Life Analogy:** Like a flexible exam grader â€” gives partial marks for partially correct answers instead of only perfect answers.
- **When it matters:** Real-world data always has noise and outliers!

##### âŒ Disadvantages:

**Disadvantage 1: Slow Training**
- **Claim:** SVM training is O(nÂ²) to O(nÂ³) â€” extremely slow for large datasets
- **Proof:** Training 100K samples takes minutes to hours. 1M samples can take days!
- **Real-Life Analogy:** Building the widest possible highway is much harder than just drawing any road!
- **When it matters:** Datasets > 100K samples â€” use Random Forest or Neural Networks instead

**Disadvantage 2: Difficult Parameter Tuning**
- **Claim:** C, gamma, kernel choice â€” multiple parameters and each impacts performance significantly
- **Proof:** Wrong C or gamma can change accuracy by 20%+ â€” must use grid search with cross-validation
- **When it matters:** Need expertise and computational budget for proper tuning

**Disadvantage 3: Low Interpretability**
- **Claim:** Impossible to explain "why" a prediction was made in simple terms
- **Proof:** "Your loan was denied because support vector #347 in 50-dimensional space is 0.85 units away" â€” meaningless to a customer!
- **When it matters:** Regulated industries (banking, healthcare) where explanations are legally required

**Disadvantage 4: No Natural Probability Output**
- **Claim:** SVM outputs decision (Class A or B) but NOT probability
- **Proof:** You can enable `probability=True` in sklearn, but it uses Platt scaling internally â€” calibrated probabilities, not natural ones
- **When it matters:** Risk assessment, ranking tasks where "how confident?" matters

---

#### ğŸ”¹ Point 12: Jargon Glossary for SVM

| Term | Simple Explanation |
|------|-------------------|
| **SVM** | Algorithm that finds maximum margin boundary between classes |
| **Hyperplane** | The decision boundary â€” line in 2D, plane in 3D, hyperplane in higher D |
| **Support Vectors** | Closest data points from each class to the hyperplane â€” they define the boundary |
| **Margin** | Distance between the hyperplane and the support vectors = 2/â€–Wâ€– |
| **Hard Margin** | Zero misclassification allowed â€” only for perfectly separable data |
| **Soft Margin** | Some misclassification allowed with penalty â€” practical for real data |
| **C Parameter** | Regularization â€” controls penalty for misclassification. High C = strict, Low C = lenient |
| **Kernel** | Function to handle non-linear data by implicitly transforming to higher dimensions |
| **Kernel Trick** | Computing in high dimensions without actually transforming â€” mathematical shortcut |
| **RBF Kernel** | Most popular non-linear kernel â€” creates circular/complex boundaries |
| **Gamma** | Controls influence radius of each training point (RBF kernel) |
| **Hinge Loss** | Loss function for SVM â€” zero for correct classifications outside margin |

---

### Topic 2: Decision Trees â€” Deep Dive

**Teacher:** Ippudu mana third major classifier â€” **Decision Trees!** ğŸŒ³ Idi machine learning lo **most interpretable** algorithm! You can literally see WHY the model made a decision!

#### 12-Point Structure for Decision Trees

---

#### ğŸ”¹ Point 1: Definition

**Teacher:** Decision Tree ante â€” **data ni yes/no questions series tho split chesi, final classification leaves lo chese tree-structured model.** Root node lo start chesi, each node lo oka question ask chesi, answer based chesi left or right branches follow chesi, finally leaf node lo classification decision ivvadam.

Simple ga cheppali ante: "Decision Tree is like 20 Questions game â€” you keep asking yes/no questions until you figure out the answer!"

10-year-old ki explain cheyali ante: "Doctor ki velthav â€” Doctor aduguthadu: 'Fever undi a?' â†’ Yes â†’ 'Cough undi a?' â†’ Yes â†’ 'Chest pain undi a?' â†’ No â†’ 'You have a cold, take rest!' â€” Idi decision tree!"

> ğŸ’¡ **Jargon Alert - Decision Tree**
> Simple Explanation: Yes/No questions tho data ni split chesi, final classification leaf node lo chese tree model. Like a flowchart â€” start from top, follow arrows based on answers, reach bottom for final answer.
> Example: Loan approval â€” Is credit score > 700? â†’ Yes â†’ Is income > 50K? â†’ Yes â†’ Loan APPROVED âœ…

---

#### ğŸ”¹ Point 2: Simple Analogy

**Teacher:** Best analogy â€” **"Doctor Diagnosis"** analogy:

```
Patient comes to Doctor

Doctor: "Do you have fever?" 
  â†’ Yes:
    Doctor: "Do you have cough?"
      â†’ Yes: "You have FLU" ğŸ¤§
      â†’ No:
        Doctor: "Body pains?"
          â†’ Yes: "You have DENGUE" ğŸ¦Ÿ
          â†’ No: "You have MALARIA" ğŸ’Š
  â†’ No:
    Doctor: "Do you have headache?"
      â†’ Yes: "You have MIGRAINE" ğŸ¤•
      â†’ No: "You are HEALTHY!" ğŸ˜Š
```

This IS a decision tree! Doctor asks questions series, based on answers takes branches, reaches diagnosis at the end!

Key insight: **Questions are chosen to best separate different diseases!** "Do you have fever?" separates much better than "What's your favorite color?" â€” because fever is INFORMATIVE for diagnosis!

---

#### ğŸ”¹ Point 3: Why Decision Trees are Used

**Teacher:** Decision Trees enduku use chestharu:

| Problem DT Solves | How |
|-------------------|-----|
| Need interpretable predictions | Every path from root â†’ leaf is explainable |
| Mixed data types | Handles numerical + categorical naturally |
| No preprocessing needed | No feature scaling, no encoding required |
| Quick baseline model | Fast training, immediate results |
| Multi-class natural | Just more leaf nodes for more classes |
| Non-linear relationships | Automatically captures complex patterns |

**Beginner Student:** Sir, why Decision Trees are called "most interpretable"?

**Teacher:** Because you can literally TRACE the decision:

"Customer X was predicted to CHURN because:
1. Contract = Month-to-month (went right branch)
2. Monthly charges > $70 (went left branch)  
3. Tenure < 12 months (went left branch)
â†’ Reached leaf: CHURN (85% confidence)"

Try explaining SVM's decision: "Customer X was predicted to churn because support vector #2847 in 30-dimensional space with RBF kernel gamma=0.1 produced a decision score of -0.342" â€” meaningless! ğŸ˜µ

âš¡ That's why regulated industries (banks, hospitals) LOVE decision trees â€” they NEED to explain WHY!

---

#### ğŸ”¹ Point 4: When to Use Decision Trees

| Condition | Why DT Works |
|-----------|--------------|
| Interpretability required | You can explain every decision |
| Mixed data types | Handles numeric + categorical |
| No preprocessing time | No scaling, no encoding |
| Quick baseline needed | Fast training |
| Business decisions | "Why was loan denied?" answerable |
| Medical diagnosis | Doctor can verify logic |

**When NOT to use Decision Trees (alone):**
- Need highest accuracy â€” single trees not best, use ensemble (Random Forest) instead
- Data shifts frequently â€” trees are unstable, small changes can create completely different tree
- Diagonal patterns â€” trees create axis-aligned splits, inefficient for diagonal boundaries
- High-stakes without pruning â€” overfitting risk

---

#### ğŸ”¹ Point 5: Where Decision Trees are Used (Real-World)

```mermaid
flowchart TD
    A[Decision Tree Use Cases] --> B[Business Decisions]
    A --> C[Medical Diagnosis]
    A --> D[Customer Segmentation]
    A --> E[Fraud Detection]
    A --> F[Risk Assessment]
    B --> B1["Loan approval/rejection<br>Credit scoring<br>Marketing campaign targeting"]
    C --> C1["Disease diagnosis<br>Treatment recommendation<br>Patient risk stratification"]
    D --> D1["Customer profiling<br>Churn prediction<br>User behavior analysis"]
    E --> E1["Transaction fraud detection<br>Class imbalance handling<br>Interpretable fraud rules"]
    F --> F1["Insurance risk scoring<br>Investment risk analysis<br>Safety assessment"]
```

**Practical Student:** Sir, Decision Trees ko ensemble methods mein kaise use karte hain?

**Teacher:** Bahut important point! Single decision tree can be weak and overfit. But **ensemble methods** combine many trees for superior performance:

- **Random Forest** â€” 100s of trees, each trained on random subset of data + features. Final prediction = majority vote of all trees.
- **Gradient Boosting (XGBoost, LightGBM)** â€” Trees trained **sequentially**, each correcting errors of previous. Industry standard for tabular data!
- **Bagging** â€” Trees trained on random bootstrap samples. Reduces variance (overfitting).

> ğŸ’¡ **Jargon Alert - Ensemble Methods**
> Simple Explanation: Multiple weak models combine to make a strong model. Like asking 100 average doctors and going with majority opinion â€” better than asking 1 expert doctor!
> Example: Random Forest = 100 decision trees voting together â†’ much more accurate than single tree.

---

#### ğŸ”¹ Point 6: Alternatives Table

| Algorithm | Interpretable? | Non-linear? | Scaling Needed? | Speed | Best For |
|-----------|---------------|-------------|-----------------|-------|----------|
| **Decision Tree** | âœ… Very High | âœ… Yes | âŒ No | Fast train + predict | Interpretable decisions |
| **Random Forest** | âœ… Moderate | âœ… Yes | âŒ No | Moderate | General purpose, robust |
| **KNN** | âŒ Low | âœ… Yes | âœ… Required | No train, slow predict | Small data, prototyping |
| **SVM** | âŒ Low | âœ… Via kernel | âœ… Required | Slow train, fast predict | High-dimensional |
| **Logistic Regression** | âœ… Moderate | âŒ No | âœ… Recommended | Fast | Binary, probabilities |

---

#### ğŸ”¹ Point 7: Mermaid Diagram â€” Decision Tree Concepts

```mermaid
flowchart TD
    A["ğŸŒ³ Decision Tree Components"] --> B["ğŸ”µ Root Node<br>First question<br>All data starts here"]
    A --> C["ğŸŸ¡ Internal Nodes<br>Intermediate questions<br>Data gets split"]
    A --> D["ğŸŸ¢ Leaf Nodes<br>Final predictions<br>Classification decisions"]
    A --> E["ğŸ“ Splitting Criteria<br>How to choose<br>best question"]
    
    E --> F["Gini Index<br>Measures impurity<br>Lower = better split"]
    E --> G["Entropy<br>Measures randomness<br>Lower = more pure"]
    E --> H["Information Gain<br>Reduction in entropy<br>after split<br>Higher = better split"]
    
    style B fill:#4c6ef5,color:#fff
    style D fill:#51cf66,color:#fff
```

---

#### ğŸ”¹ Point 8: How to Use Decision Trees (Concepts + Code)

**Teacher:** Decision Tree key concepts:

##### ğŸ”‘ Splitting Criteria â€” How the Tree Decides Questions

**Teacher:** Tree eppudu split cheyali, which feature use cheyali, which threshold use cheyali â€” ivi ela decide avuthundi? Answer: **Information Gain, Gini Index, Entropy!**

> ğŸ’¡ **Jargon Alert - Gini Index (Gini Impurity)**
> Simple Explanation: Oka group lo "impurity" measure â€” group lo different classes mix unte Gini HIGH, only one class unte Gini LOW (pure!).
> Formula: Gini = 1 - Î£(páµ¢Â²) where páµ¢ = proportion of class i
> Example: Box lo 5 red, 5 blue balls â†’ Gini = 1 - (0.5Â² + 0.5Â²) = 0.5 (impure!)  
> Box lo 10 red, 0 blue balls â†’ Gini = 1 - (1.0Â² + 0.0Â²) = 0.0 (PURE! âœ…)

> ğŸ’¡ **Jargon Alert - Entropy**
> Simple Explanation: "Randomness" or "uncertainty" ka measure. Pure group = entropy 0. Mixed group = entropy HIGH.
> Formula: Entropy = -Î£(páµ¢ * logâ‚‚(páµ¢))
> Example: All same class â†’ Entropy = 0 (no uncertainty)  
> 50-50 split â†’ Entropy = 1.0 (maximum uncertainty)

> ğŸ’¡ **Jargon Alert - Information Gain**
> Simple Explanation: Split karne se kitna information mila? HIGH information gain = GOOD split!
> Formula: IG = Entropy(parent) - Weighted Average Entropy(children)
> Example: Split that perfectly separates classes â†’ IG = maximum!

**Teacher:** Numerical example:

```
Before split: [5 cats, 5 dogs] â†’ Gini = 1 - (0.5Â² + 0.5Â²) = 0.5

Split on "Has tail > 5cm":
  Left:  [4 cats, 1 dog]  â†’ Gini = 1 - (0.8Â² + 0.2Â²) = 0.32
  Right: [1 cat, 4 dogs]  â†’ Gini = 1 - (0.2Â² + 0.8Â²) = 0.32

Weighted Gini after split = (5/10)*0.32 + (5/10)*0.32 = 0.32
Gini Decrease = 0.5 - 0.32 = 0.18 â†’ GOOD split! âœ…

Alternative split on "Color = white":
  Left:  [3 cats, 3 dogs]  â†’ Gini = 1 - (0.5Â² + 0.5Â²) = 0.5  
  Right: [2 cats, 2 dogs]  â†’ Gini = 1 - (0.5Â² + 0.5Â²) = 0.5

Weighted Gini = 0.5 â†’ Gini Decrease = 0.0 â†’ BAD split! âŒ
```

**Tree chooses "Has tail > 5cm" because it reduces impurity more!**

##### ğŸ”‘ Greedy Algorithm

> ğŸ’¡ **Jargon Alert - Greedy Algorithm**
> Simple Explanation: "Right now ka best decision le lo, future ke baare mein mat socho!" At each node, tree picks the BEST split at THAT MOMENT without considering how it affects future nodes.
> Example: Like choosing road at every junction â€” take the road that looks best NOW, even if overall path may not be optimal. Fast but not always perfect!

**Critique Student:** Sir, greedy algorithm optimal nahi hai toh kaise trust kare?

**Teacher:** Very valid concern! You're right â€” greedy is NOT globally optimal. But:
1. **Finding globally optimal tree is NP-hard** â€” impossibly slow for any reasonably sized dataset
2. **In practice**, greedy approach gives **good enough** results
3. **Ensemble methods** (Random Forest) compensate by combining many greedy trees
4. **Pruning** helps remove bad greedy decisions

##### ğŸ”‘ Code Example

```python
# ============================================
# Decision Tree Classifier â€” Complete Example
# ============================================
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# Load Iris dataset (3 classes of flowers)
iris = load_iris()
X, y = iris.data, iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# âš ï¸ NO feature scaling needed for Decision Trees!
# Decision Trees use threshold comparisons, not distances

# Train Decision Tree
dt = DecisionTreeClassifier(
    max_depth=3,           # Limit depth to prevent overfitting
    min_samples_split=10,  # Minimum 10 samples to split a node
    min_samples_leaf=5,    # Minimum 5 samples in each leaf
    criterion='gini',      # Use Gini impurity (default)
    random_state=42
)
dt.fit(X_train, y_train)

# Evaluate
y_pred = dt.predict(X_test)
print(f"Training Accuracy: {dt.score(X_train, y_train):.3f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"Tree Depth: {dt.get_depth()}")
print(f"Number of Leaves: {dt.get_n_leaves()}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, 
                            target_names=iris.target_names))

# Visualize the tree â€” THIS is why DT is interpretable!
plt.figure(figsize=(20, 10))
plot_tree(dt, 
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True, 
          rounded=True,
          fontsize=10)
plt.title("Decision Tree for Iris Classification", fontsize=16)
plt.tight_layout()
plt.show()
```

---

#### ğŸ”¹ Point 9: How Decision Trees Work Internally

```
Step 1: START AT ROOT (all training data)
   â”œâ”€â”€ For each feature and each possible threshold:
   â”‚   â”œâ”€â”€ Calculate Gini impurity (or entropy) of resulting split
   â”‚   â””â”€â”€ Pick the split with HIGHEST information gain
   â”œâ”€â”€ Split data into left and right children
   â””â”€â”€ This is the GREEDY choice at this node

Step 2: RECURSE on each child node
   â”œâ”€â”€ Apply same splitting logic
   â”œâ”€â”€ Keep splitting until STOPPING CRITERIA met:
   â”‚   â”œâ”€â”€ max_depth reached
   â”‚   â”œâ”€â”€ min_samples_split not met
   â”‚   â”œâ”€â”€ min_samples_leaf not met
   â”‚   â”œâ”€â”€ Node is PURE (all same class)
   â”‚   â””â”€â”€ No further improvement possible
   â””â”€â”€ When stopped â†’ This node becomes a LEAF

Step 3: LEAF NODES = PREDICTIONS
   â”œâ”€â”€ Each leaf stores the majority class of its training data
   â””â”€â”€ Also stores class probabilities (e.g., 85% Class A, 15% Class B)

Step 4: PREDICTION (traverse tree)
   â”œâ”€â”€ Start at root
   â”œâ”€â”€ At each node, evaluate the condition (e.g., "age > 50?")
   â”œâ”€â”€ Go left (yes) or right (no)
   â”œâ”€â”€ Continue until reaching a leaf
   â””â”€â”€ Return leaf's majority class as prediction
```

---

#### ğŸ”¹ Point 10: Pruning â€” Preventing Overfitting

**Teacher:** Idi CRITICAL concept! Decision Trees prone to overfitting â€” **pruning** prevents this!

> ğŸ’¡ **Jargon Alert - Pruning**
> Simple Explanation: Tree lo unnecessary branches cut cheyyadam â€” overfitting reduce cheyyadaniki. Like a gardener pruning a real tree â€” removing unnecessary branches makes the tree healthier!
> Example: max_depth=3 means tree can only go 3 levels deep â€” no more questions after 3!

**Two types of pruning:**

| Type | When | How | Example |
|------|------|-----|---------|
| **Pre-pruning** | DURING training | Set limits before tree is built | `max_depth=5, min_samples_split=20` |
| **Post-pruning** | AFTER training | Build full tree, then trim branches | Cost-complexity pruning (ccp_alpha) |

```python
# ============================================
# Overfitting Problem and Pruning Solution
# ============================================
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# âŒ Unpruned tree â€” OVERFITTING!
deep_tree = DecisionTreeClassifier()  # No limits!
deep_tree.fit(X_train, y_train)
print(f"Unpruned Tree:")
print(f"  Depth: {deep_tree.get_depth()}")
print(f"  Train: {deep_tree.score(X_train, y_train):.3f}")
print(f"  Test:  {deep_tree.score(X_test, y_test):.3f}")

# âœ… Pruned tree â€” GENERALIZED!
pruned_tree = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10
)
pruned_tree.fit(X_train, y_train)
print(f"\nPruned Tree:")
print(f"  Depth: {pruned_tree.get_depth()}")
print(f"  Train: {pruned_tree.score(X_train, y_train):.3f}")
print(f"  Test:  {pruned_tree.score(X_test, y_test):.3f}")
```

**Expected Output:**
```
Unpruned Tree:
  Depth: 20+
  Train: 1.000 (memorized!)
  Test:  0.750 (bad generalization!)

Pruned Tree:
  Depth: 5
  Train: 0.880
  Test:  0.860 (much better generalization!)
```

**Clever Student:** Sir, pruning parameters kya choose karein?

**Teacher:** Cross-validation use cheyandi!

```python
# Finding optimal max_depth using cross-validation
from sklearn.model_selection import cross_val_score
import numpy as np

depths = range(1, 21)
cv_scores = []

for d in depths:
    dt = DecisionTreeClassifier(max_depth=d, random_state=42)
    scores = cross_val_score(dt, X, y, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

best_depth = depths[np.argmax(cv_scores)]
print(f"Optimal max_depth: {best_depth}")
print(f"Best CV accuracy: {max(cv_scores):.3f}")
```

---

#### ğŸ”¹ Point 11: Advantages & Disadvantages (WITH PROOF)

##### âœ… Advantages:

**Advantage 1: Highly Interpretable**
- **Claim:** You can explain EXACTLY why each prediction was made
- **Proof:** Tree path from root to leaf IS the explanation: "Loan denied because credit_score < 650 AND income < 30K AND debt_ratio > 0.5"
- **Real-Life Analogy:** Like a doctor explaining step-by-step why they diagnosed flu: "Because you have fever AND cough AND no chest pain"
- **When it matters:** Banking (loan decisions), Healthcare (diagnosis), Legal (decisions must be explainable)

**Advantage 2: No Feature Scaling Required**
- **Claim:** Decision Trees work directly with raw features â€” no normalization needed
- **Proof:** Trees split on thresholds: "Is income > 50000?" â€” this comparison is the same whether income is in units of 1 or 1000. Only ORDER matters, not magnitude.
- **Real-Life Analogy:** Like sorting students by height â€” you just compare "who is taller?" â€” doesn't matter if you measure in cm or inches!
- **When it matters:** When you want quick results without preprocessing pipeline

**Advantage 3: Handles Mixed Data Types**
- **Claim:** Can split on categorical AND numerical features naturally
- **Proof:** Categorical: "Is country == 'India'?" Numerical: "Is age > 30?" â€” both work!
- **When it matters:** Real datasets often have mixed types. Other algorithms (KNN, SVM) need encoding for categorical features.

**Advantage 4: Very Fast Predictions**
- **Claim:** Prediction = just following one path from root to leaf â€” O(log n) time
- **Proof:** Tree with depth 10 = at most 10 comparisons per prediction, regardless of training set size!
- **When it matters:** Real-time applications, mobile deployment

##### âŒ Disadvantages:

**Disadvantage 1: Prone to Overfitting**
- **Claim:** Unrestricted trees memorize training data including noise
- **Proof:** Unpruned tree â†’ Training accuracy 100%, Test accuracy 70% â€” classic overfitting!
- **When it matters:** Always! Must use pruning or ensemble methods

**Disadvantage 2: Unstable**
- **Claim:** Small changes in data can create completely different tree
- **Proof:** Remove or change a few training samples â†’ tree structure can change drastically
- **Real-Life Analogy:** Like a house of cards â€” move one card and the whole structure changes!
- **When it matters:** When data shifts over time or you need consistent predictions

**Disadvantage 3: Cannot Learn Diagonal Boundaries**
- **Claim:** Trees always create axis-aligned splits â€” inefficient for diagonal patterns
- **Proof:** If true boundary is y = x (diagonal line), tree needs MANY rectangular splits to approximate it
- **When it matters:** When features have interactions that create diagonal patterns

**Disadvantage 4: Biased Toward Features with Many Values**
- **Claim:** Features with more unique values get preferred for splits
- **Proof:** Feature with 1000 unique values has more possible split points than feature with 2 values â€” more chances to find a "good" split by luck
- **When it matters:** When you have features with very different cardinalities

---

#### ğŸ”¹ Point 12: Jargon Glossary for Decision Trees

| Term | Simple Explanation |
|------|-------------------|
| **Decision Tree** | Tree-shaped model that makes yes/no decisions to classify data |
| **Root Node** | Top of the tree â€” first question asked, all data starts here |
| **Internal Node** | Middle nodes â€” intermediate questions that split data |
| **Leaf Node** | Bottom of the tree â€” final prediction made here |
| **Split** | Dividing data into sub-groups based on a feature threshold |
| **Gini Index** | Impurity measure â€” 0 = pure (one class), 0.5 = most impure (50-50) |
| **Entropy** | Uncertainty measure â€” 0 = certain (one class), 1 = most uncertain (50-50) |
| **Information Gain** | How much entropy decreases after a split â€” higher = better split |
| **Pruning** | Removing unnecessary branches to prevent overfitting |
| **Pre-pruning** | Setting limits BEFORE tree is built (max_depth, min_samples) |
| **Post-pruning** | Trimming branches AFTER full tree is built |
| **Greedy Algorithm** | Picks best split at each node without considering future â€” fast but not globally optimal |
| **Overfitting** | Tree memorizes training data including noise â€” poor generalization |

---

## ğŸ“ Teacher Summary (Part 2)

**Teacher:** Okay students, Part 2 summary:

### Key Takeaways
1. **SVM** finds the **maximum margin boundary** between classes â€” widest possible highway!
2. **Support Vectors** are the closest points from each class â€” they DEFINE the boundary
3. **C parameter** controls trade-off: Large C = strict (narrow margin), Small C = lenient (wide margin)
4. **Kernel Trick** transforms non-linear data to higher dimensions where it's separable â€” RBF most common
5. **Hinge Loss** = SVM's loss function â€” zero for correct classifications outside margin
6. **Decision Trees** use yes/no questions to split data â€” most interpretable algorithm!
7. **Gini Index / Entropy / Information Gain** decide which question to ask at each node
8. **Pruning** prevents overfitting â€” pre-pruning (set limits) or post-pruning (trim after)
9. **Decision Trees DON'T need feature scaling** â€” threshold comparisons, not distances
10. **Ensemble methods** (Random Forest, XGBoost) combine many trees for superior performance

### Common Mistakes
- **Mistake 1:** Using SVM without feature scaling â†’ Poor performance, optimization struggles
- **Mistake 2:** Using SVM on very large dataset â†’ Training takes forever, use Random Forest instead
- **Mistake 3:** Not tuning C and kernel in SVM â†’ Suboptimal performance, must grid search
- **Mistake 4:** Unpruned Decision Tree â†’ Overfitting, memorizes training data
- **Mistake 5:** Expecting best accuracy from single Decision Tree â†’ Use ensemble methods!
- **Mistake 6:** Using Decision Trees for diagonal boundary patterns â†’ Very inefficient axis-aligned splits

---

> ğŸ“˜ **Continue to [Part 3](./AS29_ClassifiersOverview3.md) for Comparison, Practical Tips, and When to Use Which Classifier!**
