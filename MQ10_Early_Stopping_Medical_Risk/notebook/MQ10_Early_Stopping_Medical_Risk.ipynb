{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¥ MQ10: Early Stopping & Regularization with Medical Data\n",
                "\n",
                "### ðŸ§© Problem Statement\n",
                "- **What**: Train an AI to detect breast cancer from medical data without \"memorizing\" the answers.\n",
                "- **Why**: If a model simply memorizes the training data (overfitting), it will fail when it sees a new patient. In medicine, this could mean missing a diagnosis.\n",
                "- **Real-World Relevance**: Medical AI systems MUST be robust. They cannot afford to only work on data they've seen before.\n",
                "\n",
                "### ðŸªœ Steps to Solve the Problem\n",
                "1.  **Import Libraries**: Get the tools we need (TensorFlow, Scikit-Learn).\n",
                "2.  **Load & Preprocess**: Get the data, split it into Practice (Train), Mock Exam (Val), and Final Exam (Test). Standardize it.\n",
                "3.  **Build Model**: Create a brain with 2 hidden layers.\n",
                "4.  **Add Protections**: Add **Weight Decay** (Simplicity Rule) and **Early Stopping** (Stop when not learning).\n",
                "5.  **Train**: Run the training loop.\n",
                "6.  **Evaluate**: See the results.\n",
                "\n",
                "### ðŸŽ¯ Expected Output\n",
                "- Ideally, the training will **stop early** (e.g., at epoch 30 instead of 100).\n",
                "- The Validation AUC (Accuracy) should be high (> 0.95).\n",
                "- We will see a graph showing where it stopped."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Import Libraries and Set Seeds\n",
                "#### 2.1 What the line does\n",
                "Imports necessary libraries and sets random seeds for reproducibility.\n",
                "#### 2.2 Why it is used\n",
                "To ensure that every time we run this notebook, we get the exact same results. This is crucial for teaching and debugging.\n",
                "#### 2.3 When to use it\n",
                "At the start of EVERY machine learning project.\n",
                "#### 2.4 Where to use it\n",
                "First code cell.\n",
                "#### 2.5 How to use it\n",
                "`random.seed(42)`\n",
                "#### 2.6 How it works internally\n",
                "Computer random numbers aren't real magic; they are math sequences. Setting the seed sets the starting number of that sequence.\n",
                "#### 2.7 Output with sample examples\n",
                "No visible output, but internally the random generators are fixed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from tensorflow.keras import layers, models, regularizers, callbacks\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Set Random Seeds\n",
                "os.environ['PYTHONHASHSEED'] = '0'\n",
                "random.seed(42)\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Load and Split Data\n",
                "#### 2.1 What the line does\n",
                "Loads the Breast Cancer dataset and splits it into **Train**, **Validation**, and **Test** sets.\n",
                "#### 2.2 Why it is used\n",
                "We need three separate buckets of data:\n",
                "- **Train (70%)**: To teach the model.\n",
                "- **Validation (15%)**: To check progress during training (Mock Exam).\n",
                "- **Test (15%)**: Final unbiased evaluation (Final Exam).\n",
                "#### 2.3 When to use it\n",
                "Before any training begins.\n",
                "#### 2.4 Where to use it\n",
                "Data preparation stage.\n",
                "#### 2.5 How to use it\n",
                "We use `train_test_split` twice.\n",
                "#### 2.6 How it works internally\n",
                "It shuffles the data and then cuts it based on the percentage (`test_size`). The `stratify=y` ensures we keep the same ratio of Sick/Healthy patients in all buckets.\n",
                "#### 2.7 Output with sample examples\n",
                "Arrays of numbers representing the data split."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "data = load_breast_cancer()\n",
                "X = data.data\n",
                "y = data.target\n",
                "\n",
                "# 1st Split: Train (70%) vs Temp (30%)\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X, y, test_size=0.3, stratify=y, random_state=42\n",
                ")\n",
                "\n",
                "# 2nd Split: Validation (15%) vs Test (15%)\n",
                "X_val, X_test, y_val, y_test = train_test_split(\n",
                "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Standardize Features\n",
                "#### 2.1 What the line does\n",
                "Scales all features so they have Mean = 0 and Standard Deviation = 1.\n",
                "#### 2.2 Why it is used\n",
                "Some features are big numbers (Area = 1000), some are small (Symmetry = 0.1). Neural networks get confused by big mismatches. Scaling makes them all technically equal.\n",
                "#### 2.3 When to use it\n",
                "For almost all neural networks using tabular data.\n",
                "#### 2.4 Where to use it\n",
                "After splitting the data.\n",
                "#### 2.5 How to use it\n",
                "`scaler.fit_transform(X_train)`\n",
                "#### 2.6 How it works internally\n",
                "Calculates average and spread of the training data, then subtracts average and divides by spread for every number.\n",
                "#### 2.7 Output with sample examples\n",
                "All numbers will be roughly between -3 and +3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "# Fit ONLY on Training data, then transform all\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training Data Shape: {X_train_scaled.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Build the Model (with L2 Regularization)\n",
                "#### 3.1 What it does\n",
                "Creates a neural network with two hidden layers (64 & 32 neurons).\n",
                "#### 3.2 Why it is used\n",
                "We add `kernel_regularizer=regularizers.l2(1e-4)`. This is **Weight Decay**.\n",
                "- It penalizes the model for having \"large\" weights.\n",
                "- Large weights usually mean the model is memorizing noise (Overfitting).\n",
                "- Small weights mean the model is looking for smoother, simpler patterns.\n",
                "#### 3.3 When to use it\n",
                "When you have a small dataset (like this one) and want to prevent overfitting.\n",
                "#### 3.4 Where to use it\n",
                "Inside the `Dense` layer definition.\n",
                "#### 3.5 How to use it\n",
                "`kernel_regularizer=regularizers.l2(0.0001)`\n",
                "#### 3.6 How it affects execution internally\n",
                "It adds a small penalty term to the Loss function: `Loss = Error + (0.0001 * Weights^2)`.\n",
                "#### 3.7 Output impact with examples\n",
                "The model learns slightly slower but generalizes better."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = models.Sequential()\n",
                "\n",
                "# Hidden Layer 1: 64 Neurons, ReLU, L2 Regularization\n",
                "model.add(layers.Dense(\n",
                "    64, \n",
                "    activation='relu', \n",
                "    input_shape=(X_train_scaled.shape[1],),\n",
                "    kernel_regularizer=regularizers.l2(1e-4)\n",
                "))\n",
                "\n",
                "# Hidden Layer 2: 32 Neurons, ReLU, L2 Regularization\n",
                "model.add(layers.Dense(\n",
                "    32, \n",
                "    activation='relu',\n",
                "    kernel_regularizer=regularizers.l2(1e-4)\n",
                "))\n",
                "\n",
                "# Output Layer: 1 Neuron (Sigmoid for binary class)\n",
                "model.add(layers.Dense(1, activation='sigmoid'))\n",
                "\n",
                "# Compile\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
                "    loss='binary_crossentropy',\n",
                "    metrics=['AUC', 'accuracy']\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Define Early Stopping\n",
                "#### 3.1 What it does\n",
                "Creates a watcher that stops training if the Validation Loss stops improving.\n",
                "#### 3.2 Why it is used\n",
                "To save time and prevent overfitting. If the model stops learning, why keep teaching it?\n",
                "#### 3.3 When to use it\n",
                "Always with iterative training.\n",
                "#### 3.4 Where to use it\n",
                "Defined before `model.fit()`.\n",
                "#### 3.5 How to use it (Arguments)\n",
                "- `monitor='val_loss'`: Watch the mock exam score.\n",
                "- `patience=4`: If it doesn't improve for 4 epochs in a row, STOP.\n",
                "- `restore_best_weights=True`: Go back in time to the moment it was best.\n",
                "#### 3.6 How it affects execution internally\n",
                "Checks the metric after every epoch.\n",
                "#### 3.7 Output impact with examples\n",
                "Training might stop at epoch 35 instead of completing 100."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "early_stopping = callbacks.EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=4,\n",
                "    restore_best_weights=True,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Train the Model\n",
                "#### 2.1 What the line does\n",
                "Starts the training loop where the model learns from data.\n",
                "#### 2.2 Why it is used\n",
                "This is the actual \"learning\" phase.\n",
                "#### 2.3 When to use it\n",
                "After compiling.\n",
                "#### 2.4 Where to use it\n",
                "It returns a `history` object we can plot.\n",
                "#### 2.5 How to use it\n",
                "Pass the training data, validation data, and the `callbacks` list.\n",
                "#### 2.6 How it works internally\n",
                "Iterates through data, calculates error, updates weights using Adam, repeats.\n",
                "#### 2.7 Output with sample examples\n",
                "A progress bar showing Loss/AUC for every epoch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting training...\")\n",
                "history = model.fit(\n",
                "    X_train_scaled, y_train,\n",
                "    epochs=100,  # We set this high, counting on Early Stopping to stop it\n",
                "    batch_size=32,\n",
                "    validation_data=(X_val_scaled, y_val),\n",
                "    callbacks=[early_stopping],\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Evaluate and Visualize\n",
                "#### 2.1 What the line does\n",
                "Plots the training history and calculates final scores on the Test set.\n",
                "#### 2.2 Why it is used\n",
                "To verify if the model actually generalized well.\n",
                "#### 2.3 When to use it\n",
                "After training stops.\n",
                "#### 2.4 Where to use it\n",
                "Final analysis.\n",
                "#### 2.5 How to use it\n",
                "Use Matplotlib to draw lines for Loss and AUC.\n",
                "#### 2.6 How it works internally\n",
                "Reads the `history.history` dictionary.\n",
                "#### 2.7 Output with sample examples\n",
                "A graph showing the lines going down (Loss) and up (AUC)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Evaluation\n",
                "test_loss, test_auc, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
                "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
                "print(f\"Test AUC: {test_auc:.4f}\")\n",
                "print(f\"Stopped at Epoch: {len(history.history['loss'])}\")\n",
                "\n",
                "# Plotting\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# Loss Plot\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['loss'], label='Train Loss')\n",
                "plt.plot(history.history['val_loss'], label='Val Loss')\n",
                "plt.title('Loss over Epochs')\n",
                "plt.legend()\n",
                "\n",
                "# AUC Plot\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['auc'], label='Train AUC')\n",
                "plt.plot(history.history['val_auc'], label='Val AUC')\n",
                "plt.title('AUC over Epochs')\n",
                "plt.legend()\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}