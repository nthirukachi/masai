{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ccef4f",
   "metadata": {},
   "source": [
    "# üß© Problem Statement: The Ultimate Model Face-Off\n",
    "\n",
    "### 1. Problem Statement\n",
    "In the world of Machine Learning, choosing the right \"brain\" (algorithm) for your problem is the most important decision. \n",
    "\n",
    "The problem is that we don't know which model works best unless we compare them side-by-side.\n",
    "\n",
    "### 2. Real-World Analogy: Selecting a Sports Team\n",
    "Imagine picking players via a tryout. Each player has a different style (LR, DT, RF, SVM, KNN).\n",
    "\n",
    "### 3. Steps to Solve\n",
    "1. Load Data\n",
    "2. Scale Features\n",
    "3. Train 5 Models\n",
    "4. Evaluate & Rank\n",
    "\n",
    "### üéØ Expected Output\n",
    "A Leaderboard showing the Accuracy champion!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113aa98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f58f31a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b419760",
   "metadata": {},
   "outputs": [],
   "source": [
    "üöÄ MQ12: CLASSIFICATION MODEL COMPARISON LEADERBOARD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95207ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A project to compare 5 major classification algorithms side-by-side.\n",
    "Models: Logistic Regression, Decision Tree, Random Forest, SVM, KNN\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8475413",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Import pandas for data manipulation\n",
    "#### 2.2 WHY: To store our results in a nice table format\n",
    "#### 2.3 WHEN: Any time you need to handle tabular data\n",
    "#### 2.4 WHERE: Data processing and results storage\n",
    "#### 2.5 HOW: import pandas as pd\n",
    "#### 2.6 INTERNAL: Loads the library into memory\n",
    "#### 2.7 OUTPUT: Access to DataFrame objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62fd3a",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Import numpy for mathematical operations\n",
    "#### 2.2 WHY: Needed for numerical array handling\n",
    "#### 2.3 WHEN: Working with arrays or math functions\n",
    "#### 2.4 WHERE: Global use\n",
    "#### 2.5 HOW: import numpy as np\n",
    "#### 2.6 INTERNAL: High-performance array library\n",
    "#### 2.7 OUTPUT: Access to np tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2843ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873420e",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Import matplotlib and seaborn for visualization\n",
    "#### 2.2 WHY: To create the \"Leaderboard\" chart\n",
    "#### 2.3 WHEN: After getting performance metrics\n",
    "#### 2.4 WHERE: Result reporting\n",
    "#### 2.5 HOW: import matplotlib.pyplot as plt\n",
    "#### 2.6 INTERNAL: Drawing engines for charts\n",
    "#### 2.7 OUTPUT: Visual plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3475c8",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Import scikit-learn tools\n",
    "#### 2.2 WHY: The industry standard for ML models and evaluation\n",
    "#### 2.3 WHEN: Creating any ML pipeline\n",
    "#### 2.4 WHERE: Training and evaluation\n",
    "#### 2.5 HOW: from sklearn... import ...\n",
    "#### 2.6 INTERNAL: Implementation of algorithms\n",
    "#### 2.7 OUTPUT: Trained models and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1286fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e2ed3",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Import the 5 models we want to compare\n",
    "#### 2.2 WHY: Each has different strengths and weaknesses\n",
    "#### 2.3 WHEN: Comparing algorithms\n",
    "#### 2.4 WHERE: ML Model Arena\n",
    "#### 2.5 HOW: from sklearn.ensemble import RandomForestClassifier\n",
    "#### 2.6 INTERNAL: Logic for LR, DT, RF, SVM, KNN\n",
    "#### 2.7 OUTPUT: Model templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec050e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "OUTPUT_PATH = \"outputs/sample_outputs\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "print(\"=\"*60)\n",
    "print(\"üèÜ THE ULTIMATE CLASSIFICATION MODEL FACE-OFF\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef19a2",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Load the Breast Cancer dataset\n",
    "#### 2.2 WHY: Standard classification dataset (Target: Malignant vs Benign)\n",
    "#### 2.3 WHEN: For learning and testing\n",
    "#### 2.4 WHERE: Sklearn datasets\n",
    "#### 2.5 HOW: load_breast_cancer()\n",
    "#### 2.6 INTERNAL: Returns a Bunch object (dictionary-like)\n",
    "#### 2.7 OUTPUT: Data and labels\n",
    "\n",
    "### ‚öôÔ∏è Argument Explanation\n",
    "#### 3.1-3.7 ARGUMENT: return_X_y=True\n",
    "#### 3.1 WHAT: Returns data (X) and target (y) separately\n",
    "#### 3.2 WHY: Skips extra steps of extracting from the Bunch object\n",
    "#### 3.3 WHEN: Just need raw data for modeling\n",
    "#### 3.4 WHERE: Argument in loader function\n",
    "#### 3.5 HOW: return_X_y=True\n",
    "#### 3.6 INTERNAL: Faster extraction logic\n",
    "#### 3.7 OUTPUT: Tuple (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb63f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(f\"‚úÖ Data Loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f69b8",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Split data into 80% Train and 20% Test\n",
    "#### 2.2 WHY: Fair evaluation on unseen data\n",
    "#### 2.3 WHEN: Before training\n",
    "#### 2.4 WHERE: Model pipeline\n",
    "#### 2.5 HOW: train_test_split(X, y)\n",
    "#### 2.6 INTERNAL: Random shuffling and splitting\n",
    "#### 2.7 OUTPUT: 4 sets of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cd371",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Scale features to a standard range\n",
    "#### 2.2 WHY: Models like SVM and KNN are sensitive to size of numbers\n",
    "#### 2.3 WHEN: Essential for distance-based models\n",
    "#### 2.4 WHERE: Preprocessing\n",
    "#### 2.5 HOW: StandardScaler().fit_transform(X)\n",
    "#### 2.6 INTERNAL: Subtracts mean, divides by standard deviation\n",
    "#### 2.7 OUTPUT: Normalized data (Z-score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a04c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"‚úÖ Features Scaled (Mean=0, STD=1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167dda3",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Create a dictionary of models to be comparison\n",
    "#### 2.2 WHY: Allows us to loop through them automatically\n",
    "#### 2.3 WHEN: Comparing multiple settings/models\n",
    "#### 2.4 WHERE: Model definition\n",
    "#### 2.5 HOW: models = {\"Name\": ModelObject()}\n",
    "#### 2.6 INTERNAL: Storing objects to pointers\n",
    "#### 2.7 OUTPUT: Collection of models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM (linear)\": SVC(kernel='linear', probability=True),\n",
    "    \"KNN (K=5)\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a072cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "print(\"\\nüöÄ Training Models and Computing Scores...\")\n",
    "for name, model in models.items():\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 2.1 WHAT: Fit the model on training data\n",
    "    # 2.2 WHY: Teacher teaches student\n",
    "    # 2.3 WHEN: During training phase\n",
    "    # 2.4 WHERE: ML loop\n",
    "    # 2.5 HOW: model.fit(X, y)\n",
    "    # 2.6 INTERNAL: Optimizes weights/splits for the algorithm\n",
    "    # 2.7 OUTPUT: Trained model\n",
    "    # --------------------------------------------------------------------------\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    # --------------------------------------------------------------------------\n",
    "    # 2.1 WHAT: Make predictions on test data\n",
    "    # 2.2 WHY: Exam time!\n",
    "    # 2.3 WHEN: Evaluation phase\n",
    "    # 2.4 WHERE: Testing\n",
    "    # 2.5 HOW: model.predict(X_test)\n",
    "    # 2.6 INTERNAL: Applies learned logic to new samples\n",
    "    # 2.7 OUTPUT: Predicted labels (0 or 1)\n",
    "    # --------------------------------------------------------------------------\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    # Save to list\n",
    "    results_list.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "    print(f\"   ‚úì {name} completed (Acc: {accuracy:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57a01e",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Create a DataFrame from the results list\n",
    "#### 2.2 WHY: Easy to sort, format and display as a leaderboard\n",
    "#### 2.3 WHEN: After finishing all experiments\n",
    "#### 2.4 WHERE: Final reporting\n",
    "#### 2.5 HOW: pd.DataFrame(data_list)\n",
    "#### 2.6 INTERNAL: Memory table creation\n",
    "#### 2.7 OUTPUT: Results Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ MODEL LEADERBOARD (Sorted by Accuracy)\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4e160",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Save the leaderboard to a CSV file\n",
    "#### 2.2 WHY: Record keeping\n",
    "#### 2.3 WHEN: Success!\n",
    "#### 2.4 WHERE: Output folder\n",
    "#### 2.5 HOW: df.to_csv(\"path\")\n",
    "#### 2.6 INTERNAL: Writes buffer to disk\n",
    "#### 2.7 OUTPUT: File on disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"{OUTPUT_PATH}/model_comparison.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd30f20",
   "metadata": {},
   "source": [
    "### üîπ Line Explanation\n",
    "#### 2.1 WHAT: Create a Bar Chart of Model Accuracies\n",
    "#### 2.2 WHY: Visual representation of the champion\n",
    "#### 2.3 WHEN: Reporting phase\n",
    "#### 2.4 WHERE: Visual Summary\n",
    "#### 2.5 HOW: sns.barplot(...)\n",
    "#### 2.6 INTERNAL: Maps data to bar height\n",
    "#### 2.7 OUTPUT: Accuracy Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot = sns.barplot(x=\"Accuracy\", y=\"Model\", data=results_df, palette=\"magma\")\n",
    "plt.title(\"Classification Model Accuracy Comparison\", fontsize=15)\n",
    "plt.xlim(0.8, 1.0)  # Zoom in for clarity\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "for p in plot.patches:\n",
    "    width = p.get_width()\n",
    "    plt.text(width + 0.005, p.get_y() + p.get_height()/2, f'{width:.2%}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_PATH}/accuracy_leaderboard.png\", dpi=150)\n",
    "print(f\"\\n‚úÖ Leaderboard chart saved to: {OUTPUT_PATH}/accuracy_leaderboard.png\")\n",
    "top_model = results_df.iloc[0][\"Model\"]\n",
    "top_acc = results_df.iloc[0][\"Accuracy\"]\n",
    "print(f\"\\nüéâ THE CHAMPION: {top_model} with {top_acc:.2%} Accuracy!\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
