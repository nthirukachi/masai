
    COMPARISON: ReLU vs Leaky ReLU in Neural Networks
    
    In this experiment, we trained two identical 2-layer neural networks
    on the same binary classification task, differing only in their hidden
    layer activation function: one using standard ReLU and the other using
    Leaky ReLU (alpha=0.01).
    
    DEAD NEURON ANALYSIS:
    The ReLU network experienced 0 dead neurons (0.0% of hidden
    layer), while the Leaky ReLU network had 0 dead neurons 
    (0.0%). This demonstrates the "dying ReLU" problem in action.
    When a ReLU neuron receives consistently negative inputs, its gradient
    becomes zero, and the weights stop updating - the neuron is permanently
    "dead." Leaky ReLU prevents this by allowing a small gradient (1%) to
    flow even for negative inputs, keeping all neurons alive and learning.
    
    ACCURACY RESULTS:
    ReLU achieved 73.10% training accuracy, while Leaky ReLU
    achieved 72.70% accuracy. The difference is attributed to:
    1. Dead neurons reducing effective network capacity with ReLU
    2. All neurons contributing to learning with Leaky ReLU
    
    WHEN TO USE EACH:
    - ReLU: Simpler computation, good for most cases, use with careful
      weight initialization (like He initialization)
    - Leaky ReLU: When you observe dying neurons, deep networks, or when
      dealing with data that may produce many negative pre-activations
    
    CONCLUSION:
    Leaky ReLU provides a robust solution to the dying ReLU problem with
    minimal computational overhead. For this task, it achieved better or
    equal accuracy while maintaining all neurons in an active learning state.
    