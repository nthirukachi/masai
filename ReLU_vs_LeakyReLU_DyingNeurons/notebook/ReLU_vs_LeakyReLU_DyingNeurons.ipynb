{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# ReLU vs Leaky ReLU: Investigating the Dying ReLU Problem\n\n## \ud83e\udde9 Problem Statement\n\n### Simple Explanation (Like Teaching a 10-Year-Old)\n\nImagine you have workers in a factory:\n- **ReLU Worker**: \"If the number is positive, I pass it. If negative, I output 0.\"\n- **Problem**: Some workers get lazy and ALWAYS output 0 - they become \"dead\"!\n\n**Leaky ReLU** fixes this by letting a tiny bit (1%) through for negative numbers.\n\n### Technical Definition\nThe **dying ReLU problem** occurs when neurons get stuck outputting zero because:\n1. ReLU outputs 0 for all negative inputs\n2. When output is 0, gradient is also 0\n3. Zero gradient means weights never update\n4. The neuron is permanently \"dead\"\n\n## \ud83e\ude9c Steps to Solve\n1. Implement ReLU and Leaky ReLU functions from scratch\n2. Build a 2-layer neural network class\n3. Train with ReLU (200 epochs)\n4. Train with Leaky ReLU (200 epochs)\n5. Compare dead neurons and accuracy\n\n## \ud83c\udfaf Expected Output\n- Training loss curves for both activations\n- Dead neuron count for each version\n- Accuracy comparison\n- Written analysis (200-300 words)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 1: Importing Libraries\n\n### Line: `import numpy as np`\n\n| Point | Explanation |\n|-------|-------------|\n| 2.1 WHAT | Imports NumPy library with nickname \"np\" |\n| 2.2 WHY | Provides fast array operations for matrix math. Alternative: pure Python lists (100x slower) |\n| 2.3 WHEN | Always at the start of ML scripts |\n| 2.4 WHERE | All machine learning and data science projects |\n| 2.5 HOW | `import numpy as np` |\n| 2.6 INTERNAL | Python loads module into memory, creates alias \"np\" |\n| 2.7 OUTPUT | No visible output, enables np.array(), np.dot(), etc. |\n\n### Line: `import matplotlib.pyplot as plt`\n\n| Point | Explanation |\n|-------|-------------|\n| 2.1 WHAT | Imports plotting library |\n| 2.2 WHY | We need to visualize training loss curves |\n| 2.3 WHEN | When creating any visualization |\n| 2.4 WHERE | Data analysis, ML model evaluation |\n| 2.5 HOW | `plt.plot()`, `plt.show()` |\n| 2.6 INTERNAL | Creates figure objects for drawing |\n| 2.7 OUTPUT | Displays visual graphs |\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\nprint(\"Libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 2: ReLU Activation Function\n\n### What is ReLU?\n**ReLU** = Rectified Linear Unit = \"Keep positives, zero out negatives\"\n\n### Mathematical Formula\n```\nrelu(z) = max(0, z)\n```\n\n### Real-Life Analogy \ud83d\udeaa\nThink of ReLU like a **one-way door**:\n- Positive numbers pass through unchanged\n- Negative numbers get blocked (become 0)\n\n```mermaid\ngraph LR\n    A[Input: -3] --> B{ReLU}\n    B --> C[Output: 0]\n    D[Input: 5] --> E{ReLU}\n    E --> F[Output: 5]\n```\n\n### Line-by-Line Explanation: `return np.maximum(0, z)`\n\n| Point | Explanation |\n|-------|-------------|\n| 2.1 WHAT | Returns element-wise maximum of 0 and z |\n| 2.2 WHY | Implements ReLU: keep positives, zero negatives. Alternative: `np.where(z > 0, z, 0)` |\n| 2.3 WHEN | During forward propagation at each hidden neuron |\n| 2.4 WHERE | Hidden layers of neural networks |\n| 2.5 HOW | `relu(z_array)` - pass any numpy array |\n| 2.6 INTERNAL | NumPy broadcasts 0 to match array shape, compares element-wise |\n| 2.7 OUTPUT | Array with same shape, negatives replaced by 0 |\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def relu(z):\n    '''\n    ReLU Activation Function\n    \n    Simple: If positive, pass through. If negative, output 0.\n    Formula: relu(z) = max(0, z)\n    '''\n    return np.maximum(0, z)\n\n\ndef relu_derivative(z):\n    '''\n    Derivative of ReLU\n    \n    - 1 if z > 0 (gradient flows through)\n    - 0 if z <= 0 (gradient STOPS - causes dying neurons!)\n    '''\n    return (z > 0).astype(float)\n\n\n# Test ReLU\nprint(\"=== Testing ReLU ===\")\ntest_values = np.array([-3, -1, 0, 1, 5])\nprint(f\"Input:           {test_values}\")\nprint(f\"ReLU output:     {relu(test_values)}\")\nprint(f\"ReLU derivative: {relu_derivative(test_values)}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 3: Leaky ReLU Activation Function\n\n### Why Leaky ReLU?\nReLU has a critical flaw: when z <= 0, the derivative is 0, so **the neuron stops learning forever** (\"dead neuron\").\n\nLeaky ReLU fixes this by allowing a small \"leak\" for negative values.\n\n### Mathematical Formula\n```\nleaky_relu(z) = z       if z > 0\n              = alpha*z if z <= 0\n```\nWhere alpha = 0.01 (1% leak)\n\n### Real-Life Analogy \ud83d\udeaa\ud83d\udca8\nLike a door with a tiny crack:\n- Positive values pass through fully\n- Negative values still get through (just 1%)\n\n### Key Difference: Why This Prevents Dead Neurons\n\n| Activation | Derivative for z < 0 | What Happens |\n|------------|---------------------|--------------|\n| ReLU | 0 | Gradient stops, neuron dies |\n| Leaky ReLU | 0.01 (alpha) | Gradient flows, neuron stays alive! |\n\n### Parameter: `alpha`\n\n| Point | Explanation |\n|-------|-------------|\n| 3.1 WHAT | The \"leak\" factor (how much negative values pass through) |\n| 3.2 WHY | Prevents dead neurons by keeping gradient non-zero |\n| 3.3 WHEN | During forward and backward propagation |\n| 3.4 WHERE | Hidden layers of neural network |\n| 3.5 HOW | Typical value: 0.01 (1% leak) |\n| 3.6 INTERNAL | Multiplies negative values by alpha instead of zeroing |\n| 3.7 OUTPUT | Higher alpha = more gradient flow, less sparsity |\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def leaky_relu(z, alpha=0.01):\n    '''\n    Leaky ReLU Activation Function\n    \n    Simple: If positive, pass through. If negative, let 1% through.\n    Formula: leaky_relu(z) = z if z > 0, else alpha * z\n    \n    The 'leak' (alpha=0.01) keeps neurons alive!\n    '''\n    return np.where(z > 0, z, alpha * z)\n\n\ndef leaky_relu_derivative(z, alpha=0.01):\n    '''\n    Derivative of Leaky ReLU\n    \n    - 1 if z > 0\n    - alpha if z <= 0 (gradient STILL flows - no dead neurons!)\n    '''\n    return np.where(z > 0, 1, alpha)\n\n\n# Test Leaky ReLU\nprint(\"=== Testing Leaky ReLU ===\")\ntest_values = np.array([-3, -1, 0, 1, 5])\nprint(f\"Input:                 {test_values}\")\nprint(f\"Leaky ReLU output:     {leaky_relu(test_values)}\")\nprint(f\"Leaky ReLU derivative: {leaky_relu_derivative(test_values)}\")\nprint()\nprint(\"Notice: For -3, ReLU gives 0, but Leaky ReLU gives -0.03 (1% of -3)\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 4: Sigmoid Activation (Output Layer)\n\n### What is Sigmoid?\nSquashes any number into range (0, 1) - perfect for probabilities!\n\n### Mathematical Formula\n```\nsigmoid(z) = 1 / (1 + e^(-z))\n```\n\n### Real-Life Analogy \ud83c\udf21\ufe0f\nLike a \"probability converter\":\n- Very negative inputs \u2192 close to 0 (unlikely)\n- Very positive inputs \u2192 close to 1 (very likely)\n- Zero \u2192 exactly 0.5 (50-50 chance)\n\n### Why We Use It\nFor binary classification (yes/no, 0/1), we need output between 0 and 1. Sigmoid is perfect!\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def sigmoid(z):\n    '''\n    Sigmoid Activation Function\n    \n    Squashes any value to range (0, 1)\n    Used for binary classification output layer\n    '''\n    z = np.clip(z, -500, 500)  # Prevent overflow\n    return 1 / (1 + np.exp(-z))\n\n\n# Test Sigmoid\nprint(\"=== Testing Sigmoid ===\")\ntest_values = np.array([-10, -1, 0, 1, 10])\nprint(f\"Input:          {test_values}\")\nprint(f\"Sigmoid output: {np.round(sigmoid(test_values), 4)}\")\nprint()\nprint(\"Notice: -10 -> ~0, 0 -> 0.5, 10 -> ~1\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 5: Neural Network Class\n\n### Architecture\n```\nInput Layer (10)  -->  Hidden Layer (20)  -->  Output Layer (1)\n   x1, x2, ..., x10       h1, h2, ..., h20          y_hat\n```\n\n### Real-Life Analogy \ud83c\udfed\nThink of this as a team of workers:\n- 10 input workers receive the raw data\n- 20 hidden workers process the data (using ReLU or Leaky ReLU)\n- 1 output worker gives the final answer (using Sigmoid)\n\nEach worker does: `output = activation(weighted_sum_of_inputs + bias)`\n\n### Key Concepts\n- **Weights (W)**: How much each input matters\n- **Bias (b)**: A constant offset\n- **Forward Propagation**: Data flows input \u2192 hidden \u2192 output\n- **Backward Propagation**: Learning by adjusting weights based on errors\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class TwoLayerNeuralNetwork:\n    '''\n    A Simple 2-Layer Neural Network for Binary Classification\n    \n    Architecture: Input(10) -> Hidden(20) -> Output(1)\n    \n    Hidden layer can use ReLU or Leaky ReLU (our comparison!)\n    Output layer uses Sigmoid (for probabilities)\n    '''\n    \n    def __init__(self, input_size, hidden_size, output_size, activation='relu', alpha=0.01):\n        '''\n        Initialize the Neural Network\n        \n        Parameters:\n        -----------\n        input_size: Number of input features (10)\n        hidden_size: Number of hidden neurons (20)\n        output_size: Number of outputs (1 for binary)\n        activation: 'relu' or 'leaky_relu'\n        alpha: Leak factor for Leaky ReLU (default 0.01)\n        '''\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.activation = activation\n        self.alpha = alpha\n        \n        # Xavier initialization for weights\n        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n        self.b1 = np.zeros((1, hidden_size))\n        \n        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n        self.b2 = np.zeros((1, output_size))\n        \n        # Storage for forward pass values\n        self.z1 = None  # Pre-activation hidden\n        self.a1 = None  # Post-activation hidden\n        self.z2 = None  # Pre-activation output\n        self.a2 = None  # Post-activation output (predictions)\n    \n    def forward(self, X):\n        '''Forward Propagation: Pass data through the network'''\n        # Layer 1: Input -> Hidden\n        self.z1 = np.dot(X, self.W1) + self.b1\n        \n        # Apply activation\n        if self.activation == 'relu':\n            self.a1 = relu(self.z1)\n        else:\n            self.a1 = leaky_relu(self.z1, self.alpha)\n        \n        # Layer 2: Hidden -> Output\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = sigmoid(self.z2)\n        \n        return self.a2\n    \n    def backward(self, X, y, learning_rate):\n        '''Backward Propagation: Learn from mistakes'''\n        m = X.shape[0]\n        \n        # Output layer error\n        dz2 = self.a2 - y.reshape(-1, 1)\n        dW2 = np.dot(self.a1.T, dz2) / m\n        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n        \n        # Hidden layer error (THIS IS WHERE DYING RELU HAPPENS!)\n        if self.activation == 'relu':\n            dz1 = np.dot(dz2, self.W2.T) * relu_derivative(self.z1)\n        else:\n            dz1 = np.dot(dz2, self.W2.T) * leaky_relu_derivative(self.z1, self.alpha)\n        \n        dW1 = np.dot(X.T, dz1) / m\n        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n        \n        # Update weights\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n    \n    def compute_loss(self, y_true, y_pred):\n        '''Binary Cross-Entropy Loss'''\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        y_true = y_true.reshape(-1, 1)\n        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return loss\n    \n    def count_dead_neurons(self, X):\n        '''Count neurons that output 0 for ALL samples'''\n        self.forward(X)\n        dead_neurons = np.sum(np.all(self.a1 == 0, axis=0))\n        return dead_neurons\n\n\nprint(\"Neural Network class defined successfully!\")\nprint(\"Ready to compare ReLU vs Leaky ReLU!\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 6: Training Function\n\n### What Happens During Training?\n1. **Forward Pass**: Make predictions\n2. **Compute Loss**: Measure how wrong we are\n3. **Backward Pass**: Calculate gradients\n4. **Update Weights**: Adjust to be less wrong\n\nThis repeats for each epoch (200 times total).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def train_network(X, y, activation, n_epochs=200, learning_rate=0.01, alpha=0.01):\n    '''\n    Train a Neural Network and Track Loss History\n    \n    Parameters:\n    -----------\n    X: Training features\n    y: Training labels\n    activation: 'relu' or 'leaky_relu'\n    n_epochs: Number of training iterations (200)\n    learning_rate: Step size for weight updates (0.01)\n    alpha: Leak factor for Leaky ReLU (0.01)\n    \n    Returns:\n    --------\n    nn: Trained network\n    loss_history: List of loss values per epoch\n    '''\n    nn = TwoLayerNeuralNetwork(\n        input_size=X.shape[1],\n        hidden_size=20,\n        output_size=1,\n        activation=activation,\n        alpha=alpha\n    )\n    \n    loss_history = []\n    \n    for epoch in range(n_epochs):\n        y_pred = nn.forward(X)\n        loss = nn.compute_loss(y, y_pred)\n        loss_history.append(loss)\n        nn.backward(X, y, learning_rate)\n        \n        if (epoch + 1) % 50 == 0:\n            print(f\"  Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}\")\n    \n    return nn, loss_history\n\n\ndef compute_accuracy(nn, X, y):\n    '''Compute classification accuracy'''\n    predictions = nn.forward(X)\n    predicted_classes = (predictions >= 0.5).astype(int)\n    accuracy = np.mean(predicted_classes.flatten() == y) * 100\n    return accuracy\n\n\nprint(\"Training functions defined!\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 7: Generate Dataset\n\n### Dataset Description\n- 1000 samples\n- 10 features (can be negative - important for demonstrating dying ReLU!)\n- Binary labels based on: x0 + x1 - x2 > 0\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Generate dataset\nnp.random.seed(42)\nX_train = np.random.randn(1000, 10)  # 1000 samples, 10 features\ny_train = (X_train[:, 0] + X_train[:, 1] - X_train[:, 2] > 0).astype(int)\n\nprint(\"=== Dataset Generated ===\")\nprint(f\"Samples: {X_train.shape[0]}\")\nprint(f\"Features: {X_train.shape[1]}\")\nprint(f\"Class distribution: {np.sum(y_train == 1)} positive, {np.sum(y_train == 0)} negative\")\nprint()\nprint(\"Sample data (first 3 rows):\")\nprint(X_train[:3, :3])",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 8: Train with ReLU\n\nNow we train a neural network using standard ReLU activation.\nWatch for:\n- How the loss decreases\n- How many neurons might \"die\"\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "print(\"=\" * 50)\nprint(\"Training with ReLU Activation\")\nprint(\"=\" * 50)\n\nrelu_nn, relu_loss = train_network(\n    X_train, y_train,\n    activation='relu',\n    n_epochs=200,\n    learning_rate=0.01\n)\n\nprint()\nprint(\"ReLU training complete!\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 9: Train with Leaky ReLU\n\nNow we train another network using Leaky ReLU (alpha=0.01).\nThe tiny \"leak\" should prevent neurons from dying.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "print(\"=\" * 50)\nprint(\"Training with Leaky ReLU Activation\")\nprint(\"=\" * 50)\n\nleaky_relu_nn, leaky_relu_loss = train_network(\n    X_train, y_train,\n    activation='leaky_relu',\n    n_epochs=200,\n    learning_rate=0.01,\n    alpha=0.01\n)\n\nprint()\nprint(\"Leaky ReLU training complete!\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 10: Analysis and Comparison\n\n### Dead Neuron Analysis\nA neuron is \"dead\" if it outputs 0 for ALL training samples.\n\n### Expected Results\n- ReLU: May have some dead neurons\n- Leaky ReLU: Should have 0 dead neurons (the leak prevents death!)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Dead Neuron Analysis\nrelu_dead = relu_nn.count_dead_neurons(X_train)\nleaky_dead = leaky_relu_nn.count_dead_neurons(X_train)\n\nprint(\"=\" * 50)\nprint(\"DEAD NEURON ANALYSIS\")\nprint(\"=\" * 50)\nprint()\nprint(f\"ReLU Version:\")\nprint(f\"  Dead neurons: {relu_dead} out of 20 ({relu_dead/20*100:.1f}%)\")\nprint()\nprint(f\"Leaky ReLU Version:\")\nprint(f\"  Dead neurons: {leaky_dead} out of 20 ({leaky_dead/20*100:.1f}%)\")\n\n# Accuracy Comparison\nrelu_accuracy = compute_accuracy(relu_nn, X_train, y_train)\nleaky_accuracy = compute_accuracy(leaky_relu_nn, X_train, y_train)\n\nprint()\nprint(\"=\" * 50)\nprint(\"ACCURACY COMPARISON\")\nprint(\"=\" * 50)\nprint()\nprint(f\"ReLU Accuracy:       {relu_accuracy:.2f}%\")\nprint(f\"Leaky ReLU Accuracy: {leaky_accuracy:.2f}%\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 11: Visualization - Training Loss Curves\n\nLet's plot the training loss curves to compare how both activations learn over time.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Plot training loss curves\nplt.figure(figsize=(10, 6))\nplt.plot(relu_loss, label='ReLU', color='blue', linewidth=2)\nplt.plot(leaky_relu_loss, label='Leaky ReLU', color='orange', linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\nplt.title('Training Loss: ReLU vs Leaky ReLU', fontsize=14, fontweight='bold')\nplt.legend(fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Loss curves plotted!\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n## Section 12: Conclusion and Key Takeaways\n\n### Summary of Results\n\n| Metric | ReLU | Leaky ReLU |\n|--------|------|------------|\n| Dead Neurons | Could be higher | Typically 0 |\n| Accuracy | Varies | Similar or better |\n| Gradient Flow | Stops at 0 | Always flows |\n\n### When to Use Each\n\n**Use ReLU when:**\n- Most cases - it's simple and works well\n- You have good weight initialization (Xavier/He)\n- Shallower networks\n\n**Use Leaky ReLU when:**\n- Deep networks where dying neurons are a concern\n- Data with many negative pre-activations\n- You observe dead neurons with ReLU\n\n### Key Points for Interviews\n1. ReLU outputs 0 for negative inputs\n2. This can cause \"dead neurons\" - neurons that never learn\n3. Leaky ReLU allows small gradient (alpha) for negative inputs\n4. This prevents dead neurons while keeping most ReLU benefits\n5. Typical alpha value is 0.01 (1%)\n"
    }
  ]
}