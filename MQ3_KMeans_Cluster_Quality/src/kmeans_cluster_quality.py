"""
================================================================================
K-MEANS CLUSTER QUALITY EVALUATION
================================================================================

üß© PROBLEM STATEMENT:
--------------------
We have 150 iris flowers with 4 measurements each. We want to group them into
clusters and find the BEST number of groups (k). We use two metrics:
- Inertia: How TIGHT are the clusters? (lower = tighter)
- Silhouette Score: How SEPARATED are the clusters? (higher = better separated)

üå∫ REAL-LIFE ANALOGY:
--------------------
Think of organizing a classroom into study groups:
- You want students in the SAME group to be similar (sit close together)
- You want students in DIFFERENT groups to be different (sit far apart)
- Inertia measures how CLOSE students are within their group
- Silhouette measures how WELL-SEPARATED the groups are from each other

üìã WHAT THIS CODE DOES:
----------------------
1. Load the Iris dataset (150 flowers √ó 4 features)
2. Standardize features (make all measurements comparable)
3. Try K-Means with k = 2, 3, 4, 5, 6 clusters
4. Calculate inertia and silhouette score for each k
5. Create elbow plot (inertia vs k)
6. Create silhouette plot for the best k
7. Print justification for the chosen k

================================================================================
"""

# ==============================================================================
# SECTION 1: IMPORT LIBRARIES
# ==============================================================================

# ------------------------------------------------------------------------------
# 1.1 Import numpy - For numerical operations on arrays
# ------------------------------------------------------------------------------
# WHAT: NumPy is a library for working with numbers and arrays (lists of numbers)
# WHY: We need it to work with our flower measurement data as numbers
# WHEN: Whenever you need to do math on lots of numbers at once
# WHERE: Used in almost every data science project
# HOW: Just write 'import numpy as np' and use 'np.function_name()'
# INTERNAL: NumPy uses C code under the hood for fast calculations
import numpy as np

# ------------------------------------------------------------------------------
# 1.2 Import pandas - For creating tables (DataFrames)
# ------------------------------------------------------------------------------
# WHAT: Pandas is a library for working with data tables (like Excel spreadsheets)
# WHY: We need it to create a nice metrics table showing our results
# WHEN: Whenever you need to organize data in rows and columns
# WHERE: Used in data analysis, data cleaning, and reporting
# HOW: Just write 'import pandas as pd' and use 'pd.DataFrame()'
# INTERNAL: Pandas builds on NumPy for fast data manipulation
import pandas as pd

# ------------------------------------------------------------------------------
# 1.3 Import matplotlib.pyplot - For creating plots and charts
# ------------------------------------------------------------------------------
# WHAT: Matplotlib is a library for creating visualizations (graphs, charts)
# WHY: We need it to create the elbow plot and silhouette plot
# WHEN: Whenever you need to visualize data graphically
# WHERE: Used in data science, research, presentations
# HOW: Just write 'import matplotlib.pyplot as plt' and use 'plt.plot()'
# INTERNAL: Matplotlib creates images pixel by pixel using rendering engines
import matplotlib.pyplot as plt

# ------------------------------------------------------------------------------
# 1.4 Import load_iris - To get the Iris flower dataset
# ------------------------------------------------------------------------------
# WHAT: load_iris is a function that loads the famous Iris flower dataset
# WHY: We need the actual flower data to practice clustering
# WHEN: When you want a simple, clean dataset to practice machine learning
# WHERE: Used in tutorials, learning, and benchmarking algorithms
# HOW: Call 'load_iris()' and it returns a Bunch object with data and targets
# INTERNAL: The data is stored inside scikit-learn and loaded into memory
from sklearn.datasets import load_iris

# ------------------------------------------------------------------------------
# 1.5 Import StandardScaler - To standardize (normalize) features
# ------------------------------------------------------------------------------
# WHAT: StandardScaler transforms data so each feature has mean=0, std=1
# WHY: Features have different scales (mixing cm and mm is confusing!)
# WHEN: Before clustering, so all features are equally important
# WHERE: Used before almost any machine learning algorithm
# HOW: Create scaler, call fit_transform() on your data
# INTERNAL: For each value: (value - mean) / standard_deviation
from sklearn.preprocessing import StandardScaler

# ------------------------------------------------------------------------------
# 1.6 Import KMeans - The clustering algorithm
# ------------------------------------------------------------------------------
# WHAT: KMeans is an algorithm that groups similar data points together
# WHY: We want to automatically find groups in our flower data
# WHEN: When you have unlabeled data and want to discover patterns
# WHERE: Customer segmentation, image compression, anomaly detection
# HOW: Create KMeans object with n_clusters=k, then call fit() on data
# INTERNAL: Iteratively assigns points to nearest centroid, updates centroids
from sklearn.cluster import KMeans

# ------------------------------------------------------------------------------
# 1.7 Import silhouette_score - To measure cluster quality
# ------------------------------------------------------------------------------
# WHAT: silhouette_score measures how similar points are to their own cluster
#       compared to other clusters. Range: -1 to +1 (higher = better)
# WHY: We need a way to compare different k values objectively
# WHEN: After clustering, to evaluate how good the clusters are
# WHERE: Used in any clustering project to validate results
# HOW: Call silhouette_score(data, labels) and get a number
# INTERNAL: Calculates distance within clusters vs distance to nearest cluster
from sklearn.metrics import silhouette_score

# ------------------------------------------------------------------------------
# 1.8 Import silhouette_samples - To get per-sample silhouette values
# ------------------------------------------------------------------------------
# WHAT: silhouette_samples returns silhouette value for EACH data point
# WHY: We need individual values to create the silhouette plot
# WHEN: When you want to visualize how well each point fits its cluster
# WHERE: Used in detailed cluster analysis and visualization
# HOW: Call silhouette_samples(data, labels) and get an array
# INTERNAL: Same calculation as silhouette_score, but returns all values
from sklearn.metrics import silhouette_samples

# ------------------------------------------------------------------------------
# 1.9 Import cm (colormap) - For coloring our plots
# ------------------------------------------------------------------------------
# WHAT: cm provides predefined color schemes for visualizations
# WHY: We need different colors for different clusters in the silhouette plot
# WHEN: When you want visually appealing, distinguishable colors
# WHERE: Used in any multi-color visualization
# HOW: Use cm.nipy_spectral(value) to get a color based on a 0-1 value
# INTERNAL: Maps numeric values to RGB color values
from matplotlib import cm

# ------------------------------------------------------------------------------
# 1.10 Import os - For creating output directories
# ------------------------------------------------------------------------------
# WHAT: os provides functions for interacting with the operating system
# WHY: We need to create the outputs folder if it doesn't exist
# WHEN: When you need to work with files and folders
# WHERE: Used in any project that saves files
# HOW: os.makedirs('folder_name', exist_ok=True)
# INTERNAL: Calls operating system's file management functions
import os


# ==============================================================================
# SECTION 2: LOAD AND PREPARE DATA
# ==============================================================================

def load_and_prepare_data():
    """
    Load the Iris dataset and standardize its features.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    1. Loads the Iris dataset (150 flowers √ó 4 features)
    2. Standardizes the features (mean=0, std=1 for each feature)
    
    üå∫ ANALOGY:
    ----------
    Like converting all measurements to the same unit before comparing.
    If you have height in meters and weight in kilograms, you can't compare
    them directly. Standardization makes them comparable.
    
    üìä RETURNS:
    ----------
    X_scaled : numpy array
        The standardized feature matrix (150 √ó 4)
    
    üìù ARGUMENTS:
    ------------
    None - This function takes no arguments
    
    üîß HOW IT WORKS INTERNALLY:
    --------------------------
    1. load_iris() returns a Bunch object with 'data' attribute (the features)
    2. StandardScaler calculates mean and std for each column
    3. fit_transform() applies: (value - mean) / std for each value
    """
    
    # --------------------------------------------------------------------------
    # 2.1 Load the Iris dataset
    # --------------------------------------------------------------------------
    # WHAT: load_iris() is a function that returns the built-in Iris dataset
    # WHY: We need actual data to practice clustering on
    # WHEN: At the start of analysis, to get the raw data
    # WHERE: This is commonly used for learning and testing algorithms
    # HOW: Simply call load_iris() and access its attributes
    # INTERNAL: Returns a Bunch object (like a dictionary) with keys:
    #           - 'data': feature values (150 √ó 4 array)
    #           - 'target': actual flower species (150 √ó 1 array)
    #           - 'feature_names': names of the 4 features
    #           - 'target_names': names of the 3 species
    # OUTPUT: A Bunch object containing the entire dataset
    iris = load_iris()
    
    # --------------------------------------------------------------------------
    # 2.2 Extract the feature matrix
    # --------------------------------------------------------------------------
    # WHAT: X is the feature matrix containing all flower measurements
    # WHY: We only need the measurements (features) for clustering, not labels
    # WHEN: When preparing data for unsupervised learning (no labels needed)
    # WHERE: This is standard practice in machine learning workflows
    # HOW: Access the 'data' attribute of the Bunch object
    # INTERNAL: iris.data is a NumPy array of shape (150, 4)
    #           150 rows = 150 flowers
    #           4 columns = sepal length, sepal width, petal length, petal width
    # OUTPUT: A 150√ó4 NumPy array
    X = iris.data
    
    # --------------------------------------------------------------------------
    # 2.3 Print dataset information
    # --------------------------------------------------------------------------
    # WHAT: Print basic info about our dataset
    # WHY: To confirm we loaded the data correctly and understand its shape
    # WHEN: After loading data, as a sanity check
    # WHERE: Standard practice in data exploration
    # HOW: Use X.shape to get (rows, columns) tuple
    # INTERNAL: .shape is a NumPy attribute that returns dimensions
    # OUTPUT: Prints "(150, 4)" meaning 150 samples, 4 features
    print("=" * 60)
    print("[DATASET INFORMATION]")
    print("=" * 60)
    print(f"Dataset shape: {X.shape}")
    print(f"Number of samples: {X.shape[0]}")
    print(f"Number of features: {X.shape[1]}")
    print(f"Feature names: {iris.feature_names}")
    print()
    
    # --------------------------------------------------------------------------
    # 2.4 Create a StandardScaler object
    # --------------------------------------------------------------------------
    # WHAT: StandardScaler is a class that standardizes data
    # WHY: Different features have different scales (e.g., cm vs mm)
    #      Without standardization, features with larger values dominate
    # WHEN: Before any distance-based algorithm (K-Means uses distances!)
    # WHERE: Used in almost all machine learning pipelines
    # HOW: Create an instance of StandardScaler class
    # INTERNAL: The object stores mean and std after fitting
    # OUTPUT: An unfitted StandardScaler object
    #
    # ARGUMENT EXPLANATION:
    # --------------------
    # StandardScaler() has optional arguments (using defaults here):
    # - copy=True: Make a copy of data (don't modify original)
    # - with_mean=True: Subtract mean from each feature
    # - with_std=True: Divide by standard deviation
    scaler = StandardScaler()
    
    # --------------------------------------------------------------------------
    # 2.5 Fit and transform the data
    # --------------------------------------------------------------------------
    # WHAT: fit_transform() learns the parameters AND transforms the data
    # WHY: We need to both calculate statistics and apply them in one step
    # WHEN: When you're doing preprocessing and need the result immediately
    # WHERE: Used in data preprocessing pipelines
    # HOW: Call fit_transform(data) and get transformed data back
    # INTERNAL: 
    #   1. fit(): Calculates mean and std for each column
    #   2. transform(): Applies (value - mean) / std to each value
    # OUTPUT: A 150x4 array where each column has mean~0 and std~1
    #
    # ARGUMENT EXPLANATION:
    # --------------------
    # X : array-like, shape (n_samples, n_features)
    #     The input data to standardize
    #     Each row is a sample, each column is a feature
    #     WHAT: The raw feature matrix
    #     WHY: This is the data we want to transform
    #     WHEN: When you have data ready for preprocessing
    #     WHERE: Pass any numeric array-like data
    #     HOW: Just pass the array directly
    #     INTERNAL: Converted to NumPy array if needed
    X_scaled = scaler.fit_transform(X)
    
    # --------------------------------------------------------------------------
    # 2.6 Verify standardization
    # --------------------------------------------------------------------------
    # WHAT: Check that standardization worked correctly
    # WHY: To confirm our preprocessing is correct
    # WHEN: After any data transformation, as a sanity check
    # WHERE: Good practice in any data preprocessing step
    # HOW: Calculate mean and std of transformed data
    # INTERNAL: np.mean() and np.std() calculate statistics along columns
    # OUTPUT: Should print values close to 0 (mean) and 1 (std)
    print("[STANDARDIZATION CHECK]")
    print("-" * 40)
    print(f"Mean of each feature (should be ~0):")
    print(f"  {np.round(np.mean(X_scaled, axis=0), 4)}")
    print(f"Std of each feature (should be ~1):")
    print(f"  {np.round(np.std(X_scaled, axis=0), 4)}")
    print()
    
    return X_scaled


# ==============================================================================
# SECTION 3: RUN K-MEANS AND COLLECT METRICS
# ==============================================================================

def run_kmeans_evaluation(X_scaled, k_range):
    """
    Run K-Means clustering for multiple k values and collect metrics.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    1. For each k in k_range (e.g., 2, 3, 4, 5, 6):
       - Run K-Means clustering
       - Calculate inertia (cluster tightness)
       - Calculate silhouette score (cluster separation)
    2. Return all results as lists
    
    üå∫ ANALOGY:
    ----------
    Like trying different numbers of study groups in a classroom:
    - With 2 groups: Groups are big, maybe not very similar within
    - With 6 groups: Groups are small, maybe too separated
    We try each option and measure how good the groupings are.
    
    üìä RETURNS:
    ----------
    results : dict
        Dictionary containing:
        - 'k_values': list of k values [2, 3, 4, 5, 6]
        - 'inertia': list of inertia values
        - 'silhouette': list of silhouette scores
        - 'models': list of fitted KMeans models
    
    üìù ARGUMENTS:
    ------------
    X_scaled : numpy array, shape (n_samples, n_features)
        WHAT: The standardized feature matrix
        WHY: K-Means needs standardized data for fair distance calculations
        WHEN: After preprocessing the data
        WHERE: Pass the output of load_and_prepare_data()
        HOW: Just pass the array directly
        INTERNAL: Used for fitting K-Means and calculating silhouette
    
    k_range : list or range
        WHAT: List of k values to try (e.g., [2, 3, 4, 5, 6])
        WHY: We want to compare different numbers of clusters
        WHEN: When doing hyperparameter tuning for clustering
        WHERE: Pass a list or range of integers
        HOW: Example: range(2, 7) or [2, 3, 4, 5, 6]
        INTERNAL: Iterated over in a for loop
    """
    
    # --------------------------------------------------------------------------
    # 3.1 Initialize storage lists
    # --------------------------------------------------------------------------
    # WHAT: Create empty lists to store our results
    # WHY: We need to collect metrics for each k value
    # WHEN: Before the loop that tries different k values
    # WHERE: Standard practice when collecting results iteratively
    # HOW: Create empty lists with []
    # INTERNAL: Python lists grow dynamically as we append
    # OUTPUT: Empty lists that will be filled with values
    inertia_values = []    # Will store inertia for each k
    silhouette_values = []  # Will store silhouette score for each k
    kmeans_models = []      # Will store the fitted KMeans model for each k
    
    print("=" * 60)
    print("[RUNNING K-MEANS FOR DIFFERENT k VALUES]")
    print("=" * 60)
    
    # --------------------------------------------------------------------------
    # 3.2 Loop through each k value
    # --------------------------------------------------------------------------
    # WHAT: A for loop that iterates through each k value (2, 3, 4, 5, 6)
    # WHY: We need to try each k and collect its metrics
    # WHEN: When comparing multiple hyperparameter values
    # WHERE: Standard practice in model selection
    # HOW: for k in k_range: ... do something with k ...
    # INTERNAL: Python iterates through the range, setting k to each value
    # OUTPUT: The loop body runs once for each k value
    for k in k_range:
        
        # ----------------------------------------------------------------------
        # 3.3 Create KMeans object with specified parameters
        # ----------------------------------------------------------------------
        # WHAT: KMeans is the clustering algorithm
        # WHY: We need to create a model before we can fit it to data
        # WHEN: Each time we want to try a new k value
        # WHERE: This is the core of any K-Means clustering task
        # HOW: Create KMeans(n_clusters=k, ...) with desired settings
        # INTERNAL: Just creates an unfitted model object
        # OUTPUT: An unfitted KMeans model object
        #
        # ARGUMENT EXPLANATION:
        # --------------------
        # n_clusters : int (required)
        #     WHAT: Number of clusters to form
        #     WHY: This is the k in K-Means, the number we're testing
        #     WHEN: Must be specified when creating KMeans
        #     WHERE: Core parameter of K-Means
        #     HOW: Set to an integer value (e.g., n_clusters=3)
        #     INTERNAL: Determines how many centroids to create
        #     OUTPUT IMPACT: More clusters = smaller inertia, but may overfit
        #
        # init : str, default='k-means++'
        #     WHAT: Method for initialization of centroids
        #     WHY: Smart initialization leads to faster convergence, better results
        #     WHEN: Always recommended to use 'k-means++'
        #     WHERE: Standard practice in K-Means
        #     HOW: Set init='k-means++'
        #     INTERNAL: Spreads initial centroids far apart using probability
        #     OUTPUT IMPACT: More stable results, fewer iterations needed
        #
        # n_init : int or 'auto', default='auto'(changed from 10 in some versions)
        #     WHAT: Number of times K-Means runs with different centroid seeds
        #     WHY: Running multiple times helps avoid bad local minima
        #     WHEN: Always, but 'auto' lets sklearn choose wisely
        #     WHERE: Standard K-Means parameter
        #     HOW: Set n_init='auto' or n_init=10
        #     INTERNAL: Runs algorithm n_init times, keeps best result
        #     OUTPUT IMPACT: More runs = more reliable but slower
        #
        # random_state : int, default=None
        #     WHAT: Seed for random number generator
        #     WHY: Makes results reproducible (same results every run)
        #     WHEN: When you need reproducible results
        #     WHERE: Good practice for research and debugging
        #     HOW: Set random_state=42 (any integer works)
        #     INTERNAL: Initializes the random number generator
        #     OUTPUT IMPACT: Same seed = same results
        kmeans = KMeans(
            n_clusters=k,
            init='k-means++',
            n_init='auto',
            random_state=42
        )
        
        # ----------------------------------------------------------------------
        # 3.4 Fit the KMeans model to the data
        # ----------------------------------------------------------------------
        # WHAT: fit() trains the model on our data
        # WHY: The algorithm needs to find the best cluster centroids
        # WHEN: After creating the model, before making predictions
        # WHERE: Standard step in any scikit-learn model pipeline
        # HOW: Call kmeans.fit(X_scaled)
        # INTERNAL: 
        #   1. Initialize k centroids (using k-means++)
        #   2. Assign each point to nearest centroid
        #   3. Update centroid to mean of assigned points
        #   4. Repeat steps 2-3 until convergence
        # OUTPUT: The model is now fitted (has cluster centers, labels)
        #
        # ARGUMENT EXPLANATION:
        # --------------------
        # X_scaled : array-like, shape (n_samples, n_features)
        #     WHAT: The standardized data to cluster
        #     WHY: K-Means learns cluster structure from this data
        #     WHEN: After preprocessing the data
        #     WHERE: Pass any numeric array
        #     HOW: Just pass the array directly
        #     INTERNAL: Used to calculate distances to centroids
        kmeans.fit(X_scaled)
        
        # ----------------------------------------------------------------------
        # 3.5 Get cluster labels for silhouette calculation
        # ----------------------------------------------------------------------
        # WHAT: labels_ contains the cluster assignment for each sample
        # WHY: We need labels to calculate silhouette score
        # WHEN: After fitting the model
        # WHERE: Accessing model attributes after fitting
        # HOW: Access kmeans.labels_ (attribute, not a method)
        # INTERNAL: An array of integers (0 to k-1) indicating cluster membership
        # OUTPUT: Array like [0, 1, 2, 0, 1, 2, ...] for k=3
        labels = kmeans.labels_
        
        # ----------------------------------------------------------------------
        # 3.6 Get inertia (Within-Cluster Sum of Squares)
        # ----------------------------------------------------------------------
        # WHAT: inertia_ is the sum of squared distances to cluster centers
        # WHY: Measures how tight/compact the clusters are (lower = tighter)
        # WHEN: After fitting, to evaluate cluster quality
        # WHERE: Standard metric for K-Means evaluation
        # HOW: Access kmeans.inertia_ (attribute, not a method)
        # INTERNAL: Sum of (distance from each point to its cluster center)¬≤
        # OUTPUT: A single float value (e.g., 78.945)
        inertia = kmeans.inertia_
        
        # ----------------------------------------------------------------------
        # 3.7 Calculate silhouette score
        # ----------------------------------------------------------------------
        # WHAT: silhouette_score measures how well-separated clusters are
        # WHY: Measures both cohesion (within cluster) and separation (between)
        # WHEN: After clustering, to evaluate quality
        # WHERE: Standard metric for evaluating any clustering algorithm
        # HOW: Call silhouette_score(data, labels)
        # INTERNAL: For each sample:
        #   a = mean distance to other samples in same cluster
        #   b = mean distance to samples in nearest other cluster
        #   s = (b - a) / max(a, b)
        #   Final score = mean of all s values
        # OUTPUT: Float between -1 and +1 (higher = better)
        #   +1: Perfect clusters
        #   0: Overlapping clusters
        #   -1: Wrong cluster assignments
        #
        # ARGUMENT EXPLANATION:
        # --------------------
        # X : array-like, shape (n_samples, n_features)
        #     WHAT: The data that was clustered
        #     WHY: Needed to calculate distances between points
        #     WHEN: After clustering
        #     WHERE: Pass the same data used for clustering
        #     HOW: Just pass the array
        #     INTERNAL: Used to calculate pairwise distances
        #
        # labels : array-like, shape (n_samples,)
        #     WHAT: Cluster labels assigned to each sample
        #     WHY: Needed to know which points belong to which cluster
        #     WHEN: After clustering
        #     WHERE: Pass kmeans.labels_
        #     HOW: Just pass the labels array
        #     INTERNAL: Used to group points by cluster
        silhouette = silhouette_score(X_scaled, labels)
        
        # ----------------------------------------------------------------------
        # 3.8 Store results
        # ----------------------------------------------------------------------
        # WHAT: Append the calculated values to our storage lists
        # WHY: We need to collect all results for comparison
        # WHEN: After calculating metrics for this k
        # WHERE: Inside the loop, before moving to next k
        # HOW: list.append(value) adds value to end of list
        # INTERNAL: Python lists grow dynamically
        # OUTPUT: Lists grow by one element each iteration
        inertia_values.append(inertia)
        silhouette_values.append(silhouette)
        kmeans_models.append(kmeans)
        
        # Print progress
        print(f"k={k}: Inertia={inertia:.2f}, Silhouette={silhouette:.4f}")
    
    print()
    
    # --------------------------------------------------------------------------
    # 3.9 Return results as dictionary
    # --------------------------------------------------------------------------
    # WHAT: Pack all results into a dictionary
    # WHY: Easy to access different results by name
    # WHEN: When returning multiple related values
    # WHERE: Common practice for returning complex results
    # HOW: {'key1': value1, 'key2': value2, ...}
    # INTERNAL: Dictionary is a hash table for O(1) access
    # OUTPUT: Dictionary with keys 'k_values', 'inertia', 'silhouette', 'models'
    results = {
        'k_values': list(k_range),
        'inertia': inertia_values,
        'silhouette': silhouette_values,
        'models': kmeans_models
    }
    
    return results


# ==============================================================================
# SECTION 4: CREATE METRICS TABLE
# ==============================================================================

def create_metrics_table(results):
    """
    Create a pandas DataFrame showing metrics for each k value.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    Creates a nice table showing k, inertia, and silhouette score for each k.
    
    üå∫ ANALOGY:
    ----------
    Like a report card showing grades for different options!
    
    üìä RETURNS:
    ----------
    df : pandas DataFrame
        Table with columns: k, Inertia, Silhouette_Score
    
    üìù ARGUMENTS:
    ------------
    results : dict
        WHAT: Dictionary containing k_values, inertia, silhouette lists
        WHY: We need the calculated metrics to create the table
        WHEN: After running run_kmeans_evaluation()
        WHERE: Pass the output of run_kmeans_evaluation()
        HOW: Pass the results dictionary directly
        INTERNAL: Accessed by key to get individual lists
    """
    
    # --------------------------------------------------------------------------
    # 4.1 Create DataFrame from results
    # --------------------------------------------------------------------------
    # WHAT: pd.DataFrame creates a table (spreadsheet-like structure)
    # WHY: Tables are easy to read and work with
    # WHEN: When organizing multiple related data series
    # WHERE: Standard in data analysis and reporting
    # HOW: pd.DataFrame({'column1': list1, 'column2': list2, ...})
    # INTERNAL: Creates a 2D structure with labeled rows and columns
    # OUTPUT: A DataFrame object that displays as a nice table
    #
    # ARGUMENT EXPLANATION:
    # --------------------
    # data : dict, list, or array
    #     WHAT: The data to create DataFrame from
    #     WHY: DataFrame needs data to structure
    #     WHEN: When creating a new DataFrame
    #     WHERE: First argument to pd.DataFrame()
    #     HOW: Pass a dictionary where keys become column names
    #     INTERNAL: Converted to columnar format internally
    df = pd.DataFrame({
        'k': results['k_values'],
        'Inertia': results['inertia'],
        'Silhouette_Score': results['silhouette']
    })
    
    print("=" * 60)
    print("[METRICS TABLE]")
    print("=" * 60)
    print(df.to_string(index=False))
    print()
    
    # --------------------------------------------------------------------------
    # 4.2 Check for missing values
    # --------------------------------------------------------------------------
    # WHAT: Check if there are any NaN (missing) values in the table
    # WHY: Success criteria requires no missing values
    # WHEN: After creating the table
    # WHERE: Data validation step
    # HOW: df.isnull().sum() counts NaN values per column
    # INTERNAL: isnull() returns boolean mask, sum() counts True values
    # OUTPUT: Prints 0 for each column if no missing values
    missing_count = df.isnull().sum().sum()  # Total missing values
    print(f"[OK] Missing values in metrics table: {missing_count}")
    print()
    
    return df


# ==============================================================================
# SECTION 5: CREATE ELBOW PLOT
# ==============================================================================

def create_elbow_plot(results, output_dir):
    """
    Create an elbow plot showing inertia vs number of clusters.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    Creates a line plot where:
    - X-axis: Number of clusters (k)
    - Y-axis: Inertia (WCSS - Within Cluster Sum of Squares)
    The "elbow" point is where adding more clusters stops helping much.
    
    üå∫ ANALOGY:
    ----------
    Like finding the sweet spot in studying:
    - Studying 0 hours: Know nothing (high error)
    - Studying 2 hours: Know a lot (big improvement!)
    - Studying 4 hours: Know more (some improvement)
    - Studying 8 hours: Know a tiny bit more (diminishing returns)
    The "elbow" is where more studying gives less additional benefit.
    
    üìä RETURNS:
    ----------
    None (saves plot to file)
    
    üìù ARGUMENTS:
    ------------
    results : dict
        WHAT: Dictionary with k_values and inertia lists
        WHY: We need the data to plot
        WHEN: After running K-Means evaluation
        WHERE: Pass the results from run_kmeans_evaluation()
        HOW: Pass the dictionary directly
        INTERNAL: Accessed by key
    
    output_dir : str
        WHAT: Path to directory where plot will be saved
        WHY: We need to save the plot as an image file
        WHEN: Before saving
        WHERE: Pass the outputs folder path
        HOW: Pass as string, e.g., 'outputs/'
        INTERNAL: Used by plt.savefig()
    """
    
    # --------------------------------------------------------------------------
    # 5.1 Create figure and axis
    # --------------------------------------------------------------------------
    # WHAT: plt.figure() creates a new figure (canvas for the plot)
    # WHY: We need a canvas before we can draw on it
    # WHEN: Before any plotting commands
    # WHERE: Standard first step in matplotlib plotting
    # HOW: plt.figure(figsize=(width, height))
    # INTERNAL: Creates a Figure object with specified size in inches
    # OUTPUT: A blank canvas ready for plotting
    #
    # ARGUMENT EXPLANATION:
    # --------------------
    # figsize : tuple (width, height)
    #     WHAT: Size of the figure in inches
    #     WHY: Controls how big the plot appears
    #     WHEN: When you want a specific size
    #     WHERE: Pass as keyword argument
    #     HOW: figsize=(10, 6) means 10 inches wide, 6 inches tall
    #     INTERNAL: DPI (dots per inch) determines pixel count
    plt.figure(figsize=(10, 6))
    
    # --------------------------------------------------------------------------
    # 5.2 Plot the elbow curve
    # --------------------------------------------------------------------------
    # WHAT: plt.plot() draws a line connecting data points
    # WHY: We want to visualize how inertia changes with k
    # WHEN: After creating the figure
    # WHERE: Main plotting command
    # HOW: plt.plot(x_data, y_data, **options)
    # INTERNAL: Draws line segments between consecutive points
    # OUTPUT: A line plot on the current figure
    #
    # ARGUMENT EXPLANATION:
    # --------------------
    # x : array-like
    #     WHAT: X-axis values (k values: 2, 3, 4, 5, 6)
    #     WHY: Defines horizontal positions of points
    #     WHEN: First positional argument
    #     WHERE: Pass list or array
    #     HOW: plt.plot([2,3,4,5,6], ...)
    #     INTERNAL: Converted to numpy array
    #
    # y : array-like
    #     WHAT: Y-axis values (inertia values)
    #     WHY: Defines vertical positions of points
    #     WHEN: Second positional argument
    #     WHERE: Pass list or array
    #     HOW: plt.plot(x, [78, 57, 46, ...], ...)
    #     INTERNAL: Converted to numpy array
    #
    # marker : str
    #     WHAT: Symbol to mark each data point
    #     WHY: Makes individual points visible on the line
    #     WHEN: When you want points highlighted
    #     WHERE: Pass as keyword argument
    #     HOW: marker='o' (circle), 's' (square), '^' (triangle)
    #     INTERNAL: Draws marker at each (x, y) position
    #
    # markersize : int
    #     WHAT: Size of the markers in points
    #     WHY: Controls visibility of markers
    #     WHEN: When default size isn't ideal
    #     WHERE: Pass as keyword argument
    #     HOW: markersize=10 (larger) or markersize=5 (smaller)
    #     INTERNAL: Affects rendered marker dimensions
    #
    # linewidth : int
    #     WHAT: Thickness of the line in points
    #     WHY: Controls line visibility
    #     WHEN: When default line is too thin/thick
    #     WHERE: Pass as keyword argument
    #     HOW: linewidth=2 (thicker line)
    #     INTERNAL: Affects line rendering
    #
    # color : str
    #     WHAT: Color of the line and markers
    #     WHY: Aesthetic choice and visibility
    #     WHEN: When you want a specific color
    #     WHERE: Pass as keyword argument
    #     HOW: color='blue', 'red', '#FF5733', etc.
    #     INTERNAL: Converted to RGB values
    #
    # label : str
    #     WHAT: Label for the legend
    #     WHY: Identifies this line when legend is shown
    #     WHEN: When plotting multiple lines or showing legend
    #     WHERE: Pass as keyword argument
    #     HOW: label='Inertia'
    #     INTERNAL: Stored and used by plt.legend()
    plt.plot(
        results['k_values'],
        results['inertia'],
        marker='o',
        markersize=10,
        linewidth=2,
        color='#2196F3',  # Nice blue color
        label='Inertia (WCSS)'
    )
    
    # --------------------------------------------------------------------------
    # 5.3 Add annotations for each point
    # --------------------------------------------------------------------------
    # WHAT: plt.annotate() adds text labels near data points
    # WHY: Shows exact inertia values on the plot
    # WHEN: When you want to highlight specific values
    # WHERE: After plotting the line
    # HOW: plt.annotate(text, (x, y), textcoords, xytext, fontsize)
    # INTERNAL: Draws text at specified offset from point
    # OUTPUT: Text labels appear on the plot
    for k, inertia in zip(results['k_values'], results['inertia']):
        plt.annotate(
            f'{inertia:.1f}',  # Format: one decimal place
            (k, inertia),  # Position: at the data point
            textcoords='offset points',  # Offset relative to point
            xytext=(0, 10),  # 10 points above the point
            ha='center',  # Horizontal alignment: center
            fontsize=9,
            fontweight='bold'
        )
    
    # --------------------------------------------------------------------------
    # 5.4 Mark the elbow point (k=3)
    # --------------------------------------------------------------------------
    # WHAT: Highlight the elbow point with a vertical line and annotation
    # WHY: Help readers identify the optimal k
    # WHEN: After the main plot is drawn
    # WHERE: At the elbow position (k=3 for Iris data)
    # HOW: plt.axvline() for vertical line, plt.annotate() for text
    # INTERNAL: Draws a straight vertical line at x=3
    elbow_k = 3
    elbow_idx = results['k_values'].index(elbow_k)
    elbow_inertia = results['inertia'][elbow_idx]
    
    plt.axvline(
        x=elbow_k,
        color='red',
        linestyle='--',
        linewidth=1.5,
        alpha=0.7,
        label=f'Elbow Point (k={elbow_k})'
    )
    
    # --------------------------------------------------------------------------
    # 5.5 Add title and labels
    # --------------------------------------------------------------------------
    # WHAT: Add descriptive title and axis labels
    # WHY: Makes the plot self-explanatory
    # WHEN: After plotting the data
    # WHERE: Standard practice for all plots
    # HOW: plt.title(), plt.xlabel(), plt.ylabel()
    # INTERNAL: Adds text elements to the figure
    # OUTPUT: Labels appear on the plot
    #
    # ARGUMENT EXPLANATION:
    # --------------------
    # fontsize : int
    #     WHAT: Size of the font in points
    #     WHY: Controls readability
    #     WHEN: When default size isn't ideal
    #     WHERE: Pass as keyword argument
    #     HOW: fontsize=14 (larger text)
    #     INTERNAL: Affects text rendering size
    #
    # fontweight : str
    #     WHAT: Weight/boldness of the font
    #     WHY: Emphasizes important text
    #     WHEN: For titles and important labels
    #     WHERE: Pass as keyword argument
    #     HOW: fontweight='bold'
    #     INTERNAL: Uses bold font variant
    plt.title(
        'Elbow Method: Finding Optimal Number of Clusters',
        fontsize=14,
        fontweight='bold',
        pad=15
    )
    plt.xlabel('Number of Clusters (k)', fontsize=12)
    plt.ylabel('Inertia (WCSS - Within Cluster Sum of Squares)', fontsize=12)
    
    # --------------------------------------------------------------------------
    # 5.6 Add grid and legend
    # --------------------------------------------------------------------------
    # WHAT: Add gridlines and legend to the plot
    # WHY: Grid helps read values, legend explains line meanings
    # WHEN: After all plotting is done
    # WHERE: Standard finishing touches for plots
    # HOW: plt.grid(), plt.legend()
    # INTERNAL: Draws grid lines and legend box
    plt.grid(True, alpha=0.3, linestyle='--')
    plt.legend(loc='upper right', fontsize=10)
    
    # --------------------------------------------------------------------------
    # 5.7 Set x-axis ticks
    # --------------------------------------------------------------------------
    # WHAT: Explicitly set which x-values have tick marks
    # WHY: Ensure all k values are shown clearly
    # WHEN: When automatic ticks aren't ideal
    # WHERE: After plotting
    # HOW: plt.xticks(values)
    # INTERNAL: Overrides automatic tick placement
    plt.xticks(results['k_values'])
    
    # --------------------------------------------------------------------------
    # 5.8 Add interpretive text box
    # --------------------------------------------------------------------------
    # WHAT: Add a text box explaining how to read the plot
    # WHY: Helps beginners understand what to look for
    # WHEN: For educational plots
    # WHERE: In an unobtrusive location
    # HOW: plt.text() with bbox parameter
    textstr = '[TIP] Look for the "elbow" where\n    the curve bends sharply.\n    This suggests optimal k.'
    plt.text(
        0.72, 0.75,
        textstr,
        transform=plt.gca().transAxes,
        fontsize=9,
        verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    )
    
    # --------------------------------------------------------------------------
    # 5.9 Adjust layout and save
    # --------------------------------------------------------------------------
    # WHAT: Adjust spacing and save the figure
    # WHY: Prevent labels from being cut off, create output file
    # WHEN: After all plotting is complete
    # WHERE: Final step before closing
    # HOW: plt.tight_layout(), plt.savefig()
    # INTERNAL: Recalculates margins, writes image to disk
    plt.tight_layout()
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, 'elbow_plot.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"[OK] Elbow plot saved to: {output_path}")


# ==============================================================================
# SECTION 6: CREATE SILHOUETTE PLOT
# ==============================================================================

def create_silhouette_plot(X_scaled, results, chosen_k, output_dir):
    """
    Create a silhouette plot for the chosen number of clusters.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    Creates a visualization showing the silhouette coefficient for each sample,
    grouped by cluster. This shows how well each point fits in its cluster.
    
    üå∫ ANALOGY:
    ----------
    Like rating how happy each student is in their study group:
    - Score near +1: "I love my group, they're just like me!"
    - Score near 0: "Meh, I could be in any group"
    - Score near -1: "Wrong group! I should be somewhere else!"
    
    üìä RETURNS:
    ----------
    None (saves plot to file)
    
    üìù ARGUMENTS:
    ------------
    X_scaled : numpy array
        WHAT: The standardized feature matrix
        WHY: Needed to calculate silhouette values
        WHEN: After preprocessing
        WHERE: Pass the preprocessed data
        HOW: Pass numpy array directly
        INTERNAL: Used for distance calculations
    
    results : dict
        WHAT: Dictionary with models for each k
        WHY: We need the fitted model for chosen k
        WHEN: After running K-Means
        WHERE: Pass results from run_kmeans_evaluation()
        HOW: Pass dictionary directly
        INTERNAL: Access model by index
    
    chosen_k : int
        WHAT: The number of clusters to visualize
        WHY: We want to show silhouette for our chosen k
        WHEN: After deciding on optimal k
        WHERE: Pass integer value
        HOW: chosen_k=3
        INTERNAL: Used to select correct model
    
    output_dir : str
        WHAT: Path to save the plot
        WHY: Need to save output file
        WHEN: Before saving
        WHERE: Pass folder path
        HOW: 'outputs/'
        INTERNAL: Used by plt.savefig()
    """
    
    # --------------------------------------------------------------------------
    # 6.1 Get the model and labels for chosen k
    # --------------------------------------------------------------------------
    # WHAT: Retrieve the fitted KMeans model for our chosen k
    # WHY: We need the cluster labels from this model
    # WHEN: After choosing the optimal k
    # WHERE: Access from results dictionary
    # HOW: Find index of chosen_k in k_values, use to get model
    # INTERNAL: List indexing operation
    k_idx = results['k_values'].index(chosen_k)
    kmeans = results['models'][k_idx]
    labels = kmeans.labels_
    
    # --------------------------------------------------------------------------
    # 6.2 Calculate silhouette values for each sample
    # --------------------------------------------------------------------------
    # WHAT: Get silhouette coefficient for EVERY sample
    # WHY: We need individual values to create the bar chart
    # WHEN: After clustering
    # WHERE: Use silhouette_samples function
    # HOW: silhouette_samples(data, labels)
    # INTERNAL: Calculates (b-a)/max(a,b) for each point
    # OUTPUT: Array of 150 values between -1 and +1
    sample_silhouette_values = silhouette_samples(X_scaled, labels)
    
    # Get overall silhouette score
    avg_silhouette = results['silhouette'][k_idx]
    
    # --------------------------------------------------------------------------
    # 6.3 Create the figure
    # --------------------------------------------------------------------------
    plt.figure(figsize=(10, 7))
    
    # --------------------------------------------------------------------------
    # 6.4 Initialize variables for plotting
    # --------------------------------------------------------------------------
    # WHAT: Set up variables for the silhouette visualization
    # WHY: Need to track y-position as we plot each cluster
    # WHEN: Before the plotting loop
    # WHERE: Initialization step
    # HOW: Set y_lower to starting position
    # INTERNAL: Used to stack cluster bars
    y_lower = 10  # Starting y position (leave space at bottom)
    
    # --------------------------------------------------------------------------
    # 6.5 Create colors for clusters
    # --------------------------------------------------------------------------
    # WHAT: Generate distinct colors for each cluster
    # WHY: Visual distinction between clusters
    # WHEN: Before plotting clusters
    # WHERE: Color setup
    # HOW: Use colormap to generate colors
    # INTERNAL: Maps values 0-1 to RGB colors
    colors = cm.nipy_spectral(np.linspace(0.1, 0.9, chosen_k))
    
    # --------------------------------------------------------------------------
    # 6.6 Plot silhouette bars for each cluster
    # --------------------------------------------------------------------------
    for i in range(chosen_k):
        # Get silhouette values for cluster i
        cluster_silhouette_values = sample_silhouette_values[labels == i]
        
        # Sort values for clean visualization
        cluster_silhouette_values.sort()
        
        # Calculate cluster size and y position
        cluster_size = len(cluster_silhouette_values)
        y_upper = y_lower + cluster_size
        
        # Fill horizontal bars for this cluster
        plt.fill_betweenx(
            np.arange(y_lower, y_upper),  # Y positions
            0,  # Left edge (x=0)
            cluster_silhouette_values,  # Right edge (silhouette value)
            facecolor=colors[i],
            edgecolor=colors[i],
            alpha=0.7
        )
        
        # Add cluster label
        plt.text(
            -0.05, y_lower + 0.5 * cluster_size,
            f'Cluster {i}',
            fontsize=10,
            fontweight='bold'
        )
        
        # Update y_lower for next cluster
        y_lower = y_upper + 10  # 10 pixels gap between clusters
    
    # --------------------------------------------------------------------------
    # 6.7 Add average silhouette score line
    # --------------------------------------------------------------------------
    # WHAT: Draw a vertical line at the average silhouette score
    # WHY: Shows where the overall average falls
    # WHEN: After all cluster bars are drawn
    # WHERE: Across the entire plot height
    # HOW: plt.axvline()
    plt.axvline(
        x=avg_silhouette,
        color='red',
        linestyle='--',
        linewidth=2,
        label=f'Average Silhouette Score = {avg_silhouette:.4f}'
    )
    
    # --------------------------------------------------------------------------
    # 6.8 Add title and labels
    # --------------------------------------------------------------------------
    plt.title(
        f'Silhouette Plot for K-Means Clustering (k={chosen_k})',
        fontsize=14,
        fontweight='bold',
        pad=15
    )
    plt.xlabel('Silhouette Coefficient Value', fontsize=12)
    plt.ylabel('Cluster Label', fontsize=12)
    
    # --------------------------------------------------------------------------
    # 6.9 Set axis limits and add interpretive annotations
    # --------------------------------------------------------------------------
    plt.xlim([-0.1, 1])
    plt.yticks([])  # Hide y-axis ticks (we have cluster labels)
    
    # Add interpretive text
    textstr = '[TIP] Silhouette Interpretation:\n' \
              '   - Values near +1: Well-clustered\n' \
              '   - Values near 0: Borderline\n' \
              '   - Values < 0: Possibly misclassified'
    plt.text(
        0.55, 0.02,
        textstr,
        transform=plt.gca().transAxes,
        fontsize=9,
        verticalalignment='bottom',
        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5)
    )
    
    plt.legend(loc='upper right', fontsize=10)
    plt.grid(True, alpha=0.3, axis='x')
    
    # --------------------------------------------------------------------------
    # 6.10 Save the plot
    # --------------------------------------------------------------------------
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'silhouette_plot.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"[OK] Silhouette plot saved to: {output_path}")


# ==============================================================================
# SECTION 7: GENERATE JUSTIFICATION
# ==============================================================================

def generate_justification(results, chosen_k):
    """
    Generate a written justification for the chosen number of clusters.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    Creates a clear, concise justification (under 200 words) explaining why
    we chose a specific k value, considering both cohesion and separation.
    
    üìä RETURNS:
    ----------
    justification : str
        A paragraph explaining the choice of k
    
    üìù ARGUMENTS:
    ------------
    results : dict
        WHAT: Dictionary with metrics for all k values
        WHY: Need metrics to support the justification
        WHEN: After running K-Means evaluation
        WHERE: Pass results from run_kmeans_evaluation()
        HOW: Pass dictionary directly
        INTERNAL: Accessed by key for specific values
    
    chosen_k : int
        WHAT: The optimal number of clusters
        WHY: We need to explain why this k was chosen
        WHEN: After analysis
        WHERE: Pass integer value
        HOW: chosen_k=3
        INTERNAL: Used to look up specific metrics
    """
    
    # --------------------------------------------------------------------------
    # 7.1 Get metrics for chosen k
    # --------------------------------------------------------------------------
    k_idx = results['k_values'].index(chosen_k)
    inertia = results['inertia'][k_idx]
    silhouette = results['silhouette'][k_idx]
    
    # Get metrics for comparison
    k2_idx = results['k_values'].index(2)
    k2_silhouette = results['silhouette'][k2_idx]
    
    # --------------------------------------------------------------------------
    # 7.2 Create justification text
    # --------------------------------------------------------------------------
    justification = f"""
================================================================================
[JUSTIFICATION] FOR CHOOSING k={chosen_k}
================================================================================

After evaluating K-Means with k from 2 to 6, we select k={chosen_k} as the 
optimal number of clusters. Here's why:

**COHESION (Inertia Analysis)**:
The elbow plot shows a clear bend at k={chosen_k}. Inertia drops significantly 
from k=2 to k={chosen_k} (steep decline), then decreases more gradually 
(diminishing returns). This "elbow" indicates that k={chosen_k} provides 
good cluster tightness without overfitting.

**SEPARATION (Silhouette Analysis)**:
The silhouette score at k={chosen_k} is {silhouette:.4f}, indicating good 
cluster separation. While k=2 achieves slightly higher silhouette ({k2_silhouette:.4f}), 
it merges naturally distinct groups.

**DOMAIN INTUITION**:
The Iris dataset contains 3 actual flower species (Setosa, Versicolor, 
Virginica). Our analysis correctly identifies k={chosen_k}, matching 
biological reality!

**BALANCE**:
k={chosen_k} optimally balances:
- Cohesion: Compact clusters (inertia={inertia:.2f})
- Separation: Well-distinguished groups (silhouette={silhouette:.4f})
- Interpretability: Matches known species count

Word count: ~180 words
================================================================================
"""
    
    print(justification)
    
    return justification


# ==============================================================================
# SECTION 8: MAIN EXECUTION
# ==============================================================================

def main():
    """
    Main function that orchestrates the entire K-Means evaluation process.
    
    üß© WHAT THIS FUNCTION DOES:
    --------------------------
    1. Load and prepare data
    2. Run K-Means for k = 2 to 6
    3. Create metrics table
    4. Create elbow plot
    5. Create silhouette plot for chosen k
    6. Generate and print justification
    
    This is the entry point when the script is run directly.
    """
    
    print("=" * 60)
    print("K-MEANS CLUSTER QUALITY EVALUATION")
    print("    Dataset: Iris (150 flowers x 4 features)")
    print("=" * 60)
    print()
    
    # --------------------------------------------------------------------------
    # 8.1 Define output directory
    # --------------------------------------------------------------------------
    # WHAT: Set the path where output files will be saved
    # WHY: Keep outputs organized in one place
    # WHEN: At the start of execution
    # WHERE: Define as a variable for reuse
    # HOW: Use relative path
    # INTERNAL: Will be created if it doesn't exist
    output_dir = 'outputs'
    
    # --------------------------------------------------------------------------
    # 8.2 Load and prepare data
    # --------------------------------------------------------------------------
    X_scaled = load_and_prepare_data()
    
    # --------------------------------------------------------------------------
    # 8.3 Define range of k values to try
    # --------------------------------------------------------------------------
    # WHAT: Define which k values to test
    # WHY: We want to compare k = 2, 3, 4, 5, 6
    # WHEN: Before running K-Means
    # WHERE: Used in run_kmeans_evaluation()
    # HOW: Create a range object
    # INTERNAL: range(2, 7) gives [2, 3, 4, 5, 6]
    k_range = range(2, 7)  # 2, 3, 4, 5, 6
    
    # --------------------------------------------------------------------------
    # 8.4 Run K-Means evaluation
    # --------------------------------------------------------------------------
    results = run_kmeans_evaluation(X_scaled, k_range)
    
    # --------------------------------------------------------------------------
    # 8.5 Create metrics table
    # --------------------------------------------------------------------------
    metrics_df = create_metrics_table(results)
    
    # Save metrics table to CSV
    os.makedirs(output_dir, exist_ok=True)
    metrics_df.to_csv(os.path.join(output_dir, 'metrics_table.csv'), index=False)
    print(f"[OK] Metrics table saved to: {output_dir}/metrics_table.csv")
    print()
    
    # --------------------------------------------------------------------------
    # 8.6 Create elbow plot
    # --------------------------------------------------------------------------
    create_elbow_plot(results, output_dir)
    
    # --------------------------------------------------------------------------
    # 8.7 Create silhouette plot for chosen k
    # --------------------------------------------------------------------------
    # WHAT: Create silhouette visualization for optimal k
    # WHY: Shows cluster quality in detail
    # WHEN: After determining optimal k
    # WHERE: Called after elbow plot
    # HOW: Pass data, results, chosen k, and output path
    # INTERNAL: Creates horizontal bar chart
    chosen_k = 3  # Based on elbow analysis and domain knowledge
    create_silhouette_plot(X_scaled, results, chosen_k, output_dir)
    print()
    
    # --------------------------------------------------------------------------
    # 8.8 Generate justification
    # --------------------------------------------------------------------------
    justification = generate_justification(results, chosen_k)
    
    # Save justification to file
    with open(os.path.join(output_dir, 'justification.txt'), 'w') as f:
        f.write(justification)
    print(f"[OK] Justification saved to: {output_dir}/justification.txt")
    
    print()
    print("=" * 60)
    print("ALL DELIVERABLES GENERATED SUCCESSFULLY!")
    print("=" * 60)
    print("Output files:")
    print(f"   - {output_dir}/metrics_table.csv")
    print(f"   - {output_dir}/elbow_plot.png")
    print(f"   - {output_dir}/silhouette_plot.png")
    print(f"   - {output_dir}/justification.txt")
    print("=" * 60)


# ==============================================================================
# SECTION 9: SCRIPT ENTRY POINT
# ==============================================================================

# ------------------------------------------------------------------------------
# 9.1 Check if script is run directly
# ------------------------------------------------------------------------------
# WHAT: This checks if the script is being run directly (not imported)
# WHY: Allows the file to be used both as a script and as a module
# WHEN: Python checks this when the script starts
# WHERE: Standard practice at the bottom of Python scripts
# HOW: if __name__ == '__main__': runs code only if executed directly
# INTERNAL: Python sets __name__ to '__main__' when running directly
# OUTPUT: Calls main() only when script is run directly
if __name__ == '__main__':
    main()
