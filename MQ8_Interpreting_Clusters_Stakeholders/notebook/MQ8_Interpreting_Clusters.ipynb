{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Problem Statement\n",
    "- **Problem:** How do we interpret the results of K-Means clustering for business stakeholders?\n",
    "- **Why it matters:** Raw cluster centers (e.g., \"Standard Deviation = 1.2\") are meaningless to marketing teams. We must translate them into \"Dollars\" and \"Scores\".\n",
    "\n",
    "### ðŸªœ Steps to Solve the Problem\n",
    "1. Load Data\n",
    "2. Scale Features (StandardScaler)\n",
    "3. Fit K-Means ($K=5$)\n",
    "4. **Inverse Transform Centroids** (The Key Step)\n",
    "5. Profile and Visualize\n",
    "\n",
    "### ðŸŽ¯ Expected Output\n",
    "- A Cluster Profile Table with readable averages.\n",
    "- A PCA Plot showing the logical separation of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Imports\n",
    "#### 2.1 What the line does\n",
    "Imports necessary libraries for data manipulation (pandas), math (numpy), plotting (matplotlib, seaborn), and machine learning (sklearn).\n",
    "#### 2.2 Why it is used\n",
    "We need `pandas` for tables, `StandardScaler` for preprocessing, `KMeans` for clustering, and `PCA` for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Load Dataset\n",
    "#### 2.1 What the line does\n",
    "Loads 'Mall_Customers.csv' if capable, otherwise creates a synthetic dataset for demonstration.\n",
    "#### 2.2 Why it is used\n",
    "To provide the input data for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from local path, otherwise create sample data\n",
    "try:\n",
    "    data_path = '../data/Mall_Customers.csv' \n",
    "    if os.path.exists(data_path):\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Loaded dataset from {data_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except:\n",
    "    print(\"Dataset not found. Creating sample Mall Customer data.\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    ids = np.arange(1, n_samples + 1)\n",
    "    genders = np.random.choice(['Male', 'Female'], n_samples)\n",
    "    ages = np.random.randint(18, 70, n_samples)\n",
    "    income = np.concatenate([\n",
    "        np.random.normal(25, 5, 40), np.random.normal(55, 10, 80), np.random.normal(90, 10, 80)\n",
    "    ]).astype(int)\n",
    "    score = np.concatenate([\n",
    "        np.random.normal(80, 10, 40), np.random.normal(50, 10, 80), \n",
    "        np.random.normal(20, 10, 40), np.random.normal(85, 10, 40)\n",
    "    ]).astype(int)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'CustomerID': ids,\n",
    "        'Gender': genders,\n",
    "        'Age': ages,\n",
    "        'Annual Income (k$)': income,\n",
    "        'Spending Score (1-100)': score\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Feature Scaling\n",
    "#### 2.1 What the line does\n",
    "Selects relevant features ($) and scales them to Mean=0, Std=1.\n",
    "#### 2.2 Why it is used\n",
    "K-Means uses distance. Income (range 0-140) implies larger distances than Score (0-100). Scaling ensures fair weighting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Annual Income (k$)', 'Spending Score (1-100)']\n",
    "X = df[features]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ K-Means Clustering\n",
    "#### 2.1 What the line does\n",
    "Initializes K-Means with 5 clusters (optimal for this data) and fits it.\n",
    "#### 2.6 How it works internally\n",
    "It places 5 random centroids, assigns points to nearest centroid, moves centroids to mean of points, and repeats until stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Inverse Transformation (VITAL STEP)\n",
    "#### 2.1 What the line does\n",
    "Takes the centroid coordinates (which are in Z-score format, e.g., 1.5) and transforms them back to original units (e.g., 90k).\n",
    "#### 2.2 Why it is used\n",
    "Business stakeholders cannot interpret Z-scores. They need to see real values to name the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_scaled = kmeans.cluster_centers_\n",
    "centroids_original = scaler.inverse_transform(centroids_scaled)\n",
    "\n",
    "# Create Profile Table\n",
    "cluster_profile = pd.DataFrame(centroids_original, columns=features)\n",
    "cluster_profile['Cluster_ID'] = range(5)\n",
    "cluster_profile['Count'] = df['Cluster'].value_counts().sort_index().values\n",
    "cluster_profile['Percent'] = (cluster_profile['Count'] / len(df)) * 100\n",
    "cluster_profile = cluster_profile[['Cluster_ID', 'Count', 'Percent'] + features].round(2)\n",
    "\n",
    "print(\"--- Cluster Profile (Original Scale) ---\")\n",
    "print(cluster_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ PCA Visualization\n",
    "#### 2.1 What the line does\n",
    "Reduces the 2D data (Income, Score) to 2 Principal Components. (Here features are already 2D, but this works even for 10D data).\n",
    "#### 2.2 Why it is used\n",
    "To plot the clusters on a flat screen and inspect separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "df['PCA1'] = X_pca[:, 0]\n",
    "df['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=100, alpha=0.8)\n",
    "\n",
    "# Annotate Centroids\n",
    "centroids_pca = pca.transform(centroids_scaled)\n",
    "for i in range(5):\n",
    "    plt.text(centroids_pca[i, 0], centroids_pca[i, 1]+0.2, f'Cluster {i}', \n",
    "             fontsize=12, fontweight='bold', color='black', ha='center')\n",
    "\n",
    "plt.title('Customer Segments (PCA Projection)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
