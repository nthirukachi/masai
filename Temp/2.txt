Decision Trees & Random Forests: Splitting Criteria, Pruning & Ensembles - Pre-Read Notes

Prerequisites: Noneâ€”this is your starting point! Basic understanding of classification and regression concepts is helpful but not required.
What You'll Gain from This Pre-Read

After reading, you'll be able to:

    Recognize how decision trees make decisions and why splitting criteria matter
    Understand why pruning prevents overfitting and improves model generalization
    Follow discussions about ensemble methods like Random Forests
    Ask informed questions about when to use these techniques in real-world applications

Think of this as: Learning the alphabet before writing essaysâ€”you're building the foundation.
What This Pre-Read Covers

This pre-read will:

    Introduce core concepts: splitting criteria, pruning, and ensemble methods
    Explain why these techniques matter for building accurate ML models
    Show simple, foundational examples
    Build your curiosity and orientation

Part 1: The Big Picture - Why Does This Matter?

Ever wondered how Netflix decides which show to recommend next, or how a bank determines whether to approve your loan application? Behind these decisions often lies a powerful machine learning technique called decision treesâ€”and when multiple trees work together, they become even more powerful as Random Forests.

Where You'll Use This:

Job roles:

    Data Scientists use these techniques daily to build predictive models for business problems
    Machine Learning Engineers deploy Random Forests in production systems for everything from fraud detection to medical diagnosis
    Business Analysts leverage these interpretable models to explain predictions to stakeholders

Real products:

    Amazon uses ensemble methods to predict product demand and optimize inventory
    Healthcare systems use Random Forests to predict patient readmission risks
    Spotify employs these techniques in music recommendation algorithms alongside other methods

What you can build:

    Customer churn prediction systems that identify who might cancel their subscription
    Credit risk assessment tools that evaluate loan applications
    Medical diagnosis assistants that help identify diseases from symptoms

Think of it like this: Imagine you're trying to decide whether to go hiking tomorrow. You might ask a series of yes/no questions: "Is it going to rain? Is the temperature above 60Â°F? Do I have the weekend free?" Each answer leads you closer to a decision. A decision tree works exactly the same wayâ€”it asks questions about data and follows different paths based on the answers. Splitting criteria help determine which questions to ask, pruning removes unnecessary questions, and ensembles are like asking multiple friends for their hiking decision and going with the majority vote.

Limitation: This analogy works for understanding the basic question-and-answer flow, but breaks down when considering the mathematical optimization involved in choosing the best questions to ask.
Part 2: Your Roadmap Through This Topic

Here's what we'll explore together:
1. Splitting Criteria

You'll discover how decision trees decide which question to ask at each stepâ€”should we split data based on age, income, or another feature? We'll explore the mathematical measures like Gini impurity and entropy that guide these decisions.
2. The Problem of Overfitting

You'll see how decision trees can become too complex, memorizing training data rather than learning general patternsâ€”like a student who memorizes answers instead of understanding concepts.
3. Pruning Techniques

We'll explore how pruning acts like editing a rough draft, removing unnecessary complexity to create a cleaner, more generalizable model that performs better on new data.
4. Ensemble Methods

You'll understand how combining multiple decision trees creates Random Forestsâ€”where the "wisdom of the crowd" produces more accurate and robust predictions than any single tree.
5. Accuracy vs. Robustness

Finally, we'll see how ensembles balance the tradeoff between fitting training data perfectly and generalizing well to new, unseen dataâ€”the holy grail of machine learning.

The journey: We'll start with understanding how individual trees make decisions, see why they need refinement through pruning, then discover how combining many trees creates powerful ensemble models that are both accurate and robust.
Part 3: Key Terms to Listen For

These are the essential terms you'll encounter. Don't memorize themâ€”just get familiar with what they mean.
Splitting Criterion

A mathematical measure used to decide which feature and threshold to use when dividing data at each node of a decision tree.

Example: When deciding whether to split customers by "age < 30" or "income < $50k", the splitting criterion helps choose which split creates more homogeneous groups.
Gini Impurity

A measure of how mixed or "impure" a dataset is, where 0 means all samples belong to one class (perfectly pure) and higher values indicate more mixing.

Think of it as: Measuring how messy a sock drawer isâ€”if all socks are the same color, it's pure (Gini = 0); if colors are mixed randomly, it's impure.
Entropy

An information theory concept measuring the amount of uncertainty or disorder in a dataset, used as an alternative to Gini impurity for splitting decisions.

In practice: Higher entropy means more uncertainty about which class a sample belongs to; splitting aims to reduce this uncertainty.
Pruning

The process of removing branches from a decision tree to reduce its complexity and prevent overfitting to training data.

Think of it as: Trimming a hedge to remove wild overgrowth and reveal the underlying structureâ€”making it cleaner and more maintainable.
Overfitting

When a model learns the training data too well, including its noise and peculiarities, resulting in poor performance on new data.

Example: A tree that creates a separate leaf for every training sample performs perfectly on training data but fails completely on new data.
Ensemble Method

A machine learning technique that combines predictions from multiple models to produce a more accurate and robust final prediction.

Think of it as: Asking multiple doctors for a diagnosis instead of relying on just oneâ€”the consensus is often more reliable than any individual opinion.
Random Forest

An ensemble method that builds many decision trees on random subsets of data and features, then combines their predictions through voting or averaging.

In practice: Each tree sees a slightly different view of the problem, and their collective wisdom produces predictions that are more accurate and stable than any single tree.

ðŸ’¡ Key Insight: These concepts work together like a quality control processâ€”splitting criteria help build trees, pruning refines individual trees, and ensembles combine multiple trees to achieve both high accuracy and robustness.
Part 4: Concepts in Action
Seeing Splitting Criteria in Action

The scenario: Imagine you're building a model to predict whether customers will purchase a product. You have data on 100 customers with features like age, income, and previous purchases. At the root node, you need to decide: should we split by age or income first?

Our approach: We'll use Gini impurity to measure which split creates more homogeneous groups. The split with the lowest weighted average Gini impurity wins.

The code:

# Step 1: Calculate Gini impurity for a potential split
def gini_impurity(labels):
    # Count occurrences of each class
    total = len(labels)
    if total == 0:
        return 0

    # Calculate proportion of each class and sum of squared proportions
    class_counts = {}
    for label in labels:
        class_counts[label] = class_counts.get(label, 0) + 1

    impurity = 1.0
    for count in class_counts.values():
        probability = count / total
        impurity -= probability ** 2

    return impurity

# Step 2: Example - split by age < 30
young_customers = [1, 1, 1, 0, 1]  # 4 purchased, 1 didn't
old_customers = [0, 0, 0, 1, 0]     # 1 purchased, 4 didn't

# Step 3: Calculate weighted Gini for this split
gini_young = gini_impurity(young_customers)  # 0.32
gini_old = gini_impurity(old_customers)      # 0.32
weighted_gini = (5/10) * gini_young + (5/10) * gini_old  # 0.32

# Result: This split achieves Gini = 0.32
print(f"Weighted Gini Impurity: {weighted_gini}")

What's happening here: We're evaluating a potential split by calculating how pure (homogeneous) each resulting group would be. In this example, splitting by age < 30 creates two groups that are fairly pureâ€”young customers mostly purchase, old customers mostly don't. A Gini of 0.32 is decent, but we'd compare this to other potential splits to find the best one.

The output/result:

Weighted Gini Impurity: 0.32

Key takeaway: The splitting criterion quantifies which split best separates the data into homogeneous groups, guiding the tree construction process toward more predictive decisions.

âš ï¸ Common Misconception: Many beginners think Gini impurity measures accuracy. It doesn'tâ€”it measures homogeneity (purity) of groups. A split might reduce impurity but still lead to incorrect predictions if the data itself is noisy.
Example: Pruning in the Real World

The situation: A fintech startup built a decision tree to predict loan defaults. Their initial tree had 127 leaf nodes and achieved 98% accuracy on training data. However, when deployed, it only achieved 72% accuracy on new loan applicationsâ€”a clear case of overfitting.

The challenge: The tree had learned specific patterns from the training data, including noise and outliers, rather than general principles. For example, it created a rule: "If applicant's name starts with 'J' AND they applied on a Tuesday AND their income is exactly $47,231, predict default." This is obviously meaningless for new applicants.

How Pruning Applied:

Step 1: Post-pruning Analysis The team used validation data to evaluate each branch. They calculated the accuracy improvement each branch providedâ€”some branches only improved training accuracy by 0.1% but added significant complexity.

Step 2: Cost-Complexity Pruning They applied cost-complexity pruning (also called weakest link pruning), which balances tree accuracy against tree size. Branches that provided minimal accuracy gain relative to their complexity were removed.

Step 3: Validation-Based Selection Using cross-validation, they tested trees of different sizes and selected one with 23 leaf nodes that maintained 94% training accuracy but achieved 88% accuracy on new dataâ€”a massive improvement in generalization.

The outcome: The pruned tree not only performed better on new data but was also easier to interpret and explain to loan officers. The model went from being a "black box" that memorized patterns to a practical tool that captured genuine risk factors.

What this shows: Pruning transforms an overfit model into a generalizable one by removing spurious patterns that don't transfer to new data. The slight decrease in training accuracy (98% â†’ 94%) is more than compensated by the large increase in real-world performance (72% â†’ 88%).

ðŸŽ¯ Real-World Application: Pruning is crucial in regulated industries like finance and healthcare where models must be both accurate and interpretable. Simpler trees are easier to audit and explain to regulators.
Example: Random Forests for Robustness

The situation: A healthcare company needed to predict hospital readmission risk using patient data. A single decision tree achieved 76% accuracy but was unstableâ€”small changes in training data caused large changes in the tree structure and predictions.

The challenge: Medical decisions require consistent, reliable predictions. A model that gives different predictions when trained on slightly different data samples is dangerous in healthcare settings.

How Random Forests Applied:

Step 1: Bootstrap Sampling Instead of building one tree on all data, they built 100 trees, each trained on a random sample (with replacement) of the original data. This is called bootstrap aggregating or "bagging."

Step 2: Random Feature Selection At each split in each tree, instead of considering all features, they randomly selected âˆš(total features) to consider. For their 16 features, each split only considered 4 randomly chosen features. This decorrelated the trees.

Step 3: Majority Voting For a new patient, all 100 trees make predictions, and the final prediction is the majority vote. If 68 trees predict "high risk" and 32 predict "low risk," the patient is classified as high risk.

The outcome: The Random Forest achieved 84% accuracyâ€”8 percentage points higher than a single tree. More importantly, the predictions were stable: retraining with slightly different data produced nearly identical predictions, crucial for clinical trust.

What this shows: Ensembles trade a bit of interpretability for significant gains in accuracy and robustness. By combining many "weak learners" that each see the problem differently, Random Forests achieve the wisdom-of-the-crowd effect, reducing both bias and variance.

ðŸŽ¯ Real-World Application: Random Forests are widely used in production systems because they're robust to outliers, handle mixed data types well, require minimal hyperparameter tuning, and rarely overfit when configured properly.
Part 5: How This Topic Connects

Understanding where this fits in the bigger picture:

Builds on:

    Basic classification concepts (predicting categories like yes/no, spam/not spam)
    Understanding of training vs. test data and the concept of generalization

Enables:

    Build production-ready machine learning models that work reliably on new data
    Understand tradeoffs between model complexity and generalization performance
    Implement feature importance analysis to understand what drives predictions
    Debug and improve models that are underperforming in production

Related concepts you might explore:

    Gradient Boosting (XGBoost, LightGBM): Another powerful ensemble method that builds trees sequentially, with each tree correcting errors of previous trees
    Feature Engineering: Techniques to create better input features that help trees make more accurate splits
    Cross-Validation: Methods to reliably estimate model performance and tune hyperparameters like tree depth and number of trees

Part 6: Questions to Keep in Mind

These questions don't have single "right" answersâ€”they're meant to spark your thinking and curiosity.

1. Tradeoff exploration: Why might a data scientist choose a single decision tree over a Random Forest for a particular project, even knowing Random Forests are typically more accurate?

2. Application scenario: If you were building a fraud detection system for credit card transactions where false positives (blocking legitimate purchases) are costly, how might you use ensemble methods differently than in a spam filter where false positives (marking good emails as spam) are less critical?

3. Feature importance paradox: Random Forests can rank feature importance, but what challenges might arise when features are highly correlated? For example, if "annual income" and "monthly income" are both in the dataset, how might this affect the importance rankings?

Reflect: Think about a prediction problem in your life or work. Would you prefer a simple, interpretable model that you can fully explain, or a complex, accurate model that's harder to interpret? What factors would guide your choice?
Quick Self-Check: Did It Click?

After reading, you should be able to:

    Explain in one sentence why splitting criteria like Gini impurity matter for building decision trees
    Define pruning and explain how it helps prevent overfitting in your own words
    Give at least one example of where ensemble methods like Random Forests are used in real applications
    Identify the tradeoff between a single decision tree and a Random Forest
    Recognize these concepts when they come up in discussions about machine learning models

If you checked fewer than 4 boxes: That's okay! Re-read the sections that felt unclear, particularly the "Concepts in Action" examples. Focus on understanding the "big picture" rather than memorizing specific formulas.

If you checked all boxes: Excellent! You're oriented and ready to explore this topic deeper through hands-on practice and detailed study.
How to Read These Notes Effectively

DO:

    âœ… Read activelyâ€”pause to think about the examples and how they relate to problems you know
    âœ… Try running the code snippet if you can (it's simple Python that demonstrates Gini calculation)
    âœ… Jot down questions that ariseâ€”they'll guide your deeper learning
    âœ… Connect concepts to things you already know (like voting systems for ensembles)
    âœ… Skim once for the big picture, then read carefully on second pass

DON'T:

    âŒ Try to memorize the Gini impurity formula word-for-word
    âŒ Get stuck on one confusing termâ€”flag it and move on, you'll see it again in context
    âŒ Skip the real-world examples thinking you'll return to them later
    âŒ Spend more than 25 minutes (you're reading too deeply if soâ€”this is orientation, not mastery)
    âŒ Expect to be able to build a Random Forest from scratch after this pre-read

Remember: This is orientation, not mastery. You're building a mental map of the terrain, not learning to navigate every street.
What's Next?

Now that you're oriented to splitting criteria, pruning, and ensemble methods, you're ready to:

Explore deeper:

    Dive into hands-on tutorials that walk you through building and tuning Random Forests using libraries like scikit-learn
    Experiment with real datasets (like the famous Titanic dataset) to see how different splitting criteria and tree depths affect performance

Practice:

    Try implementing a simple decision tree by hand on a small dataset to understand how splitting decisions are made
    Compare a single decision tree's performance to a Random Forest on the same problemâ€”observe the accuracy and stability differences

Continue learning:

    The next logical step is learning about hyperparameter tuning (tree depth, number of trees, minimum samples per leaf) and cross-validation for reliable model evaluation
    You'll also want to explore feature importance analysis to understand which features drive your model's predictions

Additional resources:

    Scikit-learn's official documentation on Decision Trees and Random Forests provides excellent worked examples and visual explanations
    Andrew Ng's Machine Learning course (available free on Coursera) has an exceptional module on decision trees and ensemble methods

Final Thought

You've just taken the first step into ensemble machine learningâ€”every data scientist working with Random Forests today started exactly where you are now, with curiosity and the foundational understanding you've just built. These techniques power countless production systems, and now you know why they work and when to use them.