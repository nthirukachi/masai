{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† MNIST Activation Functions Comparison\n",
                "\n",
                "## üß© Problem Statement\n",
                "\n",
                "### What Problem Are We Solving?\n",
                "\n",
                "We're building **three neural networks** (computer brains) to recognize handwritten digits (0-9) and comparing how different **activation functions** affect their performance.\n",
                "\n",
                "### Real-Life Analogy üè´\n",
                "\n",
                "Imagine three students learning to read handwritten numbers:\n",
                "- **Student A (Sigmoid)**: Wakes up slowly, like on a lazy Sunday\n",
                "- **Student B (Tanh)**: Wakes up moderately, like for school\n",
                "- **Student C (ReLU)**: Wakes up instantly, like on Christmas morning!\n",
                "\n",
                "We want to find which student learns fastest and gets the best grades.\n",
                "\n",
                "---\n",
                "\n",
                "## ü™ú Steps to Solve the Problem\n",
                "\n",
                "```mermaid\n",
                "flowchart TD\n",
                "    A[üì• Load MNIST Data] --> B[üîß Preprocess Data]\n",
                "    B --> C[üèóÔ∏è Build 3 Models]\n",
                "    C --> D[üìö Train All Models]\n",
                "    D --> E[üìä Compare Results]\n",
                "    E --> F[üî¨ Analyze Gradients]\n",
                "    F --> G[üìù Write Report]\n",
                "```\n",
                "\n",
                "1. **Load Data**: Get 60,000 training images of handwritten digits\n",
                "2. **Preprocess**: Flatten 28x28 images to 784 numbers, normalize to 0-1\n",
                "3. **Build Models**: Create 3 identical networks with different activations\n",
                "4. **Train**: Each model learns for 20 epochs\n",
                "5. **Compare**: Visualize accuracy, loss, and training time\n",
                "6. **Analyze**: Check gradient flow to understand vanishing gradients\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Expected Output\n",
                "\n",
                "| Model | Expected Accuracy | Training Speed | Gradient Flow |\n",
                "|-------|-------------------|----------------|---------------|\n",
                "| Sigmoid | ~97-98% | Slowest | Weak |\n",
                "| Tanh | ~97-98% | Medium | Medium |\n",
                "| ReLU | ~98% | Fastest | Strong |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìö Section 1: Imports\n",
                "\n",
                "### üîπ Import TensorFlow\n",
                "\n",
                "#### 2.1 What the line does\n",
                "Imports TensorFlow, the main deep learning framework, with alias `tf`.\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "TensorFlow provides tools to build, train, and evaluate neural networks. It's industry standard and has Keras built-in.\n",
                "- **Alternative**: PyTorch (equally popular)\n",
                "- **Why TensorFlow**: Keras integration makes it beginner-friendly\n",
                "\n",
                "#### 2.3 When to use it\n",
                "At the start of any deep learning project.\n",
                "\n",
                "#### 2.4 Where to use it\n",
                "AI/ML companies like Google, Netflix, Uber use TensorFlow.\n",
                "\n",
                "#### 2.5 How to use it\n",
                "```python\n",
                "import tensorflow as tf  # Convention is to use 'tf' alias\n",
                "```\n",
                "\n",
                "#### 2.6 How it works internally\n",
                "TensorFlow builds computation graphs and runs them on CPU/GPU.\n",
                "\n",
                "#### 2.7 Output\n",
                "Makes `tf.*` functions available (tf.keras, tf.GradientTape, etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Import NumPy\n",
                "\n",
                "#### 2.1 What the line does\n",
                "Imports NumPy for numerical operations on arrays.\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "NumPy is faster than Python lists for mathematical operations. Essential for data manipulation.\n",
                "- **Alternative**: Pure Python lists\n",
                "- **Why NumPy**: 10-100x faster for large arrays\n",
                "\n",
                "#### 2.3 When to use it\n",
                "Always in data science and ML projects.\n",
                "\n",
                "#### 2.4 Where to use it\n",
                "Every data science project uses NumPy.\n",
                "\n",
                "#### 2.5 How to use it\n",
                "```python\n",
                "import numpy as np  # Convention is 'np' alias\n",
                "```\n",
                "\n",
                "#### 2.6 How it works internally\n",
                "Uses C-optimized code for fast array operations.\n",
                "\n",
                "#### 2.7 Output\n",
                "Makes `np.*` functions available (np.mean, np.abs, etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "print(f\"NumPy Version: {np.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Import Matplotlib\n",
                "\n",
                "#### 2.1 What the line does\n",
                "Imports matplotlib.pyplot for creating visualizations.\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "We need to create plots comparing model performance. pyplot is the most common interface.\n",
                "\n",
                "#### 2.3 When to use it\n",
                "When creating charts, graphs, or any visual output.\n",
                "\n",
                "#### 2.4 Where to use it\n",
                "Reports, research papers, dashboards.\n",
                "\n",
                "#### 2.5 How to use it\n",
                "```python\n",
                "import matplotlib.pyplot as plt  # Convention is 'plt' alias\n",
                "```\n",
                "\n",
                "#### 2.6 How it works internally\n",
                "Creates figure objects and renders them as images.\n",
                "\n",
                "#### 2.7 Output\n",
                "Makes `plt.*` functions available (plt.plot, plt.savefig, etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "import os\n",
                "\n",
                "# Enable inline plotting\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚öôÔ∏è Section 2: Configuration\n",
                "\n",
                "### üîπ Training Hyperparameters\n",
                "\n",
                "**Hyperparameters** are like cooking settings - temperature, time, portion size. They control HOW the model trains.\n",
                "\n",
                "| Parameter | Value | Real-Life Analogy |\n",
                "|-----------|-------|-------------------|\n",
                "| EPOCHS | 20 | Number of times to study all flashcards |\n",
                "| BATCH_SIZE | 128 | Number of flashcards to study before taking a break |\n",
                "| LEARNING_RATE | 0.001 | How big of a step to take when learning |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training hyperparameters\n",
                "EPOCHS = 20           # How many times to go through all data\n",
                "BATCH_SIZE = 128      # Samples per weight update\n",
                "LEARNING_RATE = 0.001 # Step size for optimizer\n",
                "\n",
                "# Output directory\n",
                "OUTPUT_DIR = \"../outputs\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Epochs: {EPOCHS}\")\n",
                "print(f\"Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"Learning Rate: {LEARNING_RATE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üì• Section 3: Load and Preprocess Data\n",
                "\n",
                "### üîπ Loading MNIST Dataset\n",
                "\n",
                "#### 2.1 What the line does\n",
                "Loads the MNIST dataset - 70,000 images of handwritten digits.\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "MNIST is the \"Hello World\" of machine learning. It's:\n",
                "- Small (easy to download)\n",
                "- Fast to train\n",
                "- Great for learning\n",
                "\n",
                "#### 2.3 When to use it\n",
                "When learning ML basics or benchmarking simple models.\n",
                "\n",
                "#### 2.4 Where to use it\n",
                "Computer vision, digit recognition, banking (check reading).\n",
                "\n",
                "#### 2.5 How to use it\n",
                "```python\n",
                "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
                "```\n",
                "\n",
                "#### 2.6 How it works internally\n",
                "Downloads data from internet first time, caches locally after.\n",
                "\n",
                "#### 2.7 Output\n",
                "- `X_train`: 60,000 images of shape (28, 28)\n",
                "- `y_train`: 60,000 labels (0-9)\n",
                "- `X_test`: 10,000 images for testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load MNIST dataset\n",
                "print(\"Loading MNIST dataset...\")\n",
                "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
                "\n",
                "print(f\"Original training data shape: {X_train.shape}\")  # (60000, 28, 28)\n",
                "print(f\"Training labels shape: {y_train.shape}\")         # (60000,)\n",
                "print(f\"Test data shape: {X_test.shape}\")                # (10000, 28, 28)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Visualize Sample Images\n",
                "\n",
                "Let's see what the handwritten digits look like!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sample images\n",
                "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    ax.imshow(X_train[i], cmap='gray')\n",
                "    ax.set_title(f\"Label: {y_train[i]}\", fontsize=12)\n",
                "    ax.axis('off')\n",
                "plt.suptitle(\"Sample MNIST Images\", fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Preprocessing: Flatten and Normalize\n",
                "\n",
                "#### 2.1 What the line does\n",
                "1. **Reshape**: Changes 28x28 image to flat 784-length vector\n",
                "2. **Normalize**: Divides by 255 to get values between 0-1\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "- **Flatten**: Dense layers expect 1D input, not 2D images\n",
                "- **Normalize**: Neural networks train better with small values\n",
                "\n",
                "```mermaid\n",
                "flowchart LR\n",
                "    A[28x28 Image<br>Values: 0-255] --> B[Flatten]\n",
                "    B --> C[784 Vector<br>Values: 0-255]\n",
                "    C --> D[Normalize]\n",
                "    D --> E[784 Vector<br>Values: 0-1]\n",
                "```\n",
                "\n",
                "#### 2.5 How to use it\n",
                "```python\n",
                "X_train = X_train.reshape(-1, 784) / 255.0\n",
                "# -1 means \"figure out this dimension\" (will be 60000)\n",
                "# 784 = 28 * 28 pixels\n",
                "# / 255.0 normalizes to 0-1 range\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Flatten: (60000, 28, 28) -> (60000, 784)\n",
                "X_train = X_train.reshape(-1, 784)\n",
                "X_test = X_test.reshape(-1, 784)\n",
                "\n",
                "# Normalize: 0-255 -> 0-1\n",
                "X_train = X_train.astype('float32') / 255.0\n",
                "X_test = X_test.astype('float32') / 255.0\n",
                "\n",
                "print(f\"After preprocessing:\")\n",
                "print(f\"X_train shape: {X_train.shape}\")  # (60000, 784)\n",
                "print(f\"X_test shape: {X_test.shape}\")    # (10000, 784)\n",
                "print(f\"Value range: [{X_train.min():.2f}, {X_train.max():.2f}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üèóÔ∏è Section 4: Build Neural Network Models\n",
                "\n",
                "### Understanding Activation Functions\n",
                "\n",
                "An **activation function** decides how a neuron \"fires\" based on its input. It's like a volume control or filter.\n",
                "\n",
                "```mermaid\n",
                "flowchart LR\n",
                "    A[Input x] --> B[Weighted Sum<br>z = Wx + b]\n",
                "    B --> C[Activation<br>f(z)]\n",
                "    C --> D[Output]\n",
                "```\n",
                "\n",
                "| Activation | Formula | Range | Pros | Cons |\n",
                "|------------|---------|-------|------|------|\n",
                "| **Sigmoid** | 1/(1+e^-x) | (0, 1) | Smooth, probabilistic | Vanishing gradients |\n",
                "| **Tanh** | (e^x - e^-x)/(e^x + e^-x) | (-1, 1) | Zero-centered | Vanishing gradients |\n",
                "| **ReLU** | max(0, x) | [0, ‚àû) | Fast, no vanishing | Dead neurons |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Visualize Activation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize activation functions\n",
                "x = np.linspace(-5, 5, 100)\n",
                "\n",
                "# Compute activations\n",
                "sigmoid = 1 / (1 + np.exp(-x))\n",
                "tanh = np.tanh(x)\n",
                "relu = np.maximum(0, x)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "axes[0].plot(x, sigmoid, 'r-', linewidth=2)\n",
                "axes[0].set_title('Sigmoid: œÉ(x) = 1/(1+e^-x)', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Input (x)')\n",
                "axes[0].set_ylabel('Output')\n",
                "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(x, tanh, 'b-', linewidth=2)\n",
                "axes[1].set_title('Tanh: tanh(x)', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xlabel('Input (x)')\n",
                "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "axes[2].plot(x, relu, 'g-', linewidth=2)\n",
                "axes[2].set_title('ReLU: max(0, x)', fontsize=12, fontweight='bold')\n",
                "axes[2].set_xlabel('Input (x)')\n",
                "axes[2].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(OUTPUT_DIR, 'activation_functions.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Build Model Function\n",
                "\n",
                "#### 2.1 What the function does\n",
                "Creates a neural network with 3 layers: 784 ‚Üí 128 ‚Üí 64 ‚Üí 10\n",
                "\n",
                "#### 2.2 Why this architecture\n",
                "- **784 inputs**: One for each pixel (28√ó28 = 784)\n",
                "- **128 hidden**: Learn basic patterns (edges, curves)\n",
                "- **64 hidden**: Learn complex patterns (digit shapes)\n",
                "- **10 outputs**: One for each digit (0-9)\n",
                "\n",
                "```mermaid\n",
                "graph LR\n",
                "    A[784 Inputs<br>Pixels] --> B[128 Neurons<br>Hidden 1]\n",
                "    B --> C[64 Neurons<br>Hidden 2]\n",
                "    C --> D[10 Outputs<br>Digits 0-9]\n",
                "    \n",
                "    style A fill:#e1f5fe\n",
                "    style B fill:#fff3e0\n",
                "    style C fill:#fff3e0\n",
                "    style D fill:#e8f5e9\n",
                "```\n",
                "\n",
                "### ‚öôÔ∏è Function Arguments\n",
                "\n",
                "#### 3.1 activation_name (str)\n",
                "- **What**: String specifying which activation function to use\n",
                "- **Why**: Different activations have different properties\n",
                "- **Options**: 'sigmoid', 'tanh', 'relu'\n",
                "- **Effect**: Changes how neurons compute output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_model(activation_name):\n",
                "    \"\"\"\n",
                "    Build a neural network with specified activation function.\n",
                "    \n",
                "    Architecture: 784 ‚Üí 128 ‚Üí 64 ‚Üí 10\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    activation_name : str\n",
                "        'sigmoid', 'tanh', or 'relu'\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    model : tf.keras.Model\n",
                "        Compiled model ready for training\n",
                "    \"\"\"\n",
                "    model = tf.keras.Sequential([\n",
                "        # Hidden Layer 1: 784 -> 128 neurons\n",
                "        tf.keras.layers.Dense(\n",
                "            units=128,                    # Number of neurons\n",
                "            activation=activation_name,   # Activation function\n",
                "            input_shape=(784,)            # Input shape (only for first layer)\n",
                "        ),\n",
                "        \n",
                "        # Hidden Layer 2: 128 -> 64 neurons\n",
                "        tf.keras.layers.Dense(\n",
                "            units=64,\n",
                "            activation=activation_name\n",
                "        ),\n",
                "        \n",
                "        # Output Layer: 64 -> 10 neurons (one per digit)\n",
                "        tf.keras.layers.Dense(\n",
                "            units=10,\n",
                "            activation='softmax'  # Converts to probabilities\n",
                "        )\n",
                "    ])\n",
                "    \n",
                "    # Compile model\n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
                "        loss='sparse_categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Test building a model\n",
                "test_model = build_model('relu')\n",
                "test_model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìö Section 5: Training with Time Tracking\n",
                "\n",
                "### üîπ Custom Callback for Timing\n",
                "\n",
                "A **callback** is code that runs at specific points during training. We use it to measure time per epoch.\n",
                "\n",
                "#### 2.1 What the code does\n",
                "Creates a custom callback that records start/end time of each epoch.\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "To compare training speed of different activation functions.\n",
                "\n",
                "#### 2.5 How to use it\n",
                "```python\n",
                "class TimeCallback(tf.keras.callbacks.Callback):\n",
                "    def on_epoch_begin(...)  # Called at start of epoch\n",
                "    def on_epoch_end(...)    # Called at end of epoch\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TimeCallback(tf.keras.callbacks.Callback):\n",
                "    \"\"\"\n",
                "    Custom callback to measure training time per epoch.\n",
                "    \n",
                "    Like a stopwatch for each study session!\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.times = []       # Store time for each epoch\n",
                "        self.epoch_start = 0  # Track when epoch started\n",
                "        \n",
                "    def on_epoch_begin(self, epoch, logs=None):\n",
                "        \"\"\"Start the timer when epoch begins.\"\"\"\n",
                "        self.epoch_start = time.time()\n",
                "        \n",
                "    def on_epoch_end(self, epoch, logs=None):\n",
                "        \"\"\"Record elapsed time when epoch ends.\"\"\"\n",
                "        elapsed = time.time() - self.epoch_start\n",
                "        self.times.append(elapsed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Training Function\n",
                "\n",
                "#### model.fit() Arguments Explained\n",
                "\n",
                "| Argument | Value | Why |\n",
                "|----------|-------|-----|\n",
                "| `X_train` | Training images | Model learns from these |\n",
                "| `y_train` | Correct labels | Model checks answers |\n",
                "| `epochs` | 20 | Number of full passes |\n",
                "| `batch_size` | 128 | Samples per update |\n",
                "| `validation_data` | (X_test, y_test) | Monitor overfitting |\n",
                "| `callbacks` | [TimeCallback()] | Track timing |\n",
                "| `verbose` | 1 | Show progress bar |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, X_train, y_train, X_test, y_test, model_name):\n",
                "    \"\"\"\n",
                "    Train model and track time per epoch.\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    history : Training history with metrics\n",
                "    epoch_times : List of times for each epoch\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Training {model_name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    time_callback = TimeCallback()\n",
                "    \n",
                "    history = model.fit(\n",
                "        X_train, y_train,\n",
                "        epochs=EPOCHS,\n",
                "        batch_size=BATCH_SIZE,\n",
                "        validation_data=(X_test, y_test),\n",
                "        callbacks=[time_callback],\n",
                "        verbose=1\n",
                "    )\n",
                "    \n",
                "    return history, time_callback.times"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üöÄ Section 6: Train All Three Models\n",
                "\n",
                "Now we train all three models and collect their results!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define models to compare\n",
                "activations = ['sigmoid', 'tanh', 'relu']\n",
                "model_names = ['Sigmoid Model', 'Tanh Model', 'ReLU Model']\n",
                "\n",
                "# Storage for results\n",
                "histories = []\n",
                "epoch_times_list = []\n",
                "models = []\n",
                "\n",
                "# Train each model\n",
                "for activation, name in zip(activations, model_names):\n",
                "    # Build fresh model\n",
                "    model = build_model(activation)\n",
                "    \n",
                "    # Train\n",
                "    history, epoch_times = train_model(model, X_train, y_train, X_test, y_test, name)\n",
                "    \n",
                "    # Store results\n",
                "    histories.append(history)\n",
                "    epoch_times_list.append(epoch_times)\n",
                "    models.append(model)\n",
                "    \n",
                "    # Evaluate\n",
                "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
                "    print(f\"\\n{name} - Test Accuracy: {test_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìä Section 7: Visualize Results\n",
                "\n",
                "### üîπ Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define colors for each model\n",
                "colors = ['#e74c3c', '#3498db', '#2ecc71']  # Red, Blue, Green\n",
                "\n",
                "# Create figure with 2x2 subplots\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# ================== Plot 1: Training & Validation Accuracy ==================\n",
                "ax1 = axes[0, 0]\n",
                "for i, (history, name) in enumerate(zip(histories, model_names)):\n",
                "    ax1.plot(history.history['accuracy'], color=colors[i], linestyle='-', \n",
                "             linewidth=2, label=f'{name} (Train)')\n",
                "    ax1.plot(history.history['val_accuracy'], color=colors[i], linestyle='--', \n",
                "             linewidth=2, label=f'{name} (Val)')\n",
                "\n",
                "ax1.set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Accuracy')\n",
                "ax1.legend(loc='lower right', fontsize=8)\n",
                "ax1.grid(True, alpha=0.3)\n",
                "ax1.set_ylim([0.8, 1.0])\n",
                "\n",
                "# ================== Plot 2: Training & Validation Loss ==================\n",
                "ax2 = axes[0, 1]\n",
                "for i, (history, name) in enumerate(zip(histories, model_names)):\n",
                "    ax2.plot(history.history['loss'], color=colors[i], linestyle='-', \n",
                "             linewidth=2, label=f'{name} (Train)')\n",
                "    ax2.plot(history.history['val_loss'], color=colors[i], linestyle='--', \n",
                "             linewidth=2, label=f'{name} (Val)')\n",
                "\n",
                "ax2.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Loss')\n",
                "ax2.legend(loc='upper right', fontsize=8)\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "# ================== Plot 3: Final Test Accuracy Bar Chart ==================\n",
                "ax3 = axes[1, 0]\n",
                "final_accuracies = [h.history['val_accuracy'][-1] for h in histories]\n",
                "bars = ax3.bar(model_names, final_accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
                "\n",
                "for bar, acc in zip(bars, final_accuracies):\n",
                "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
                "             f'{acc:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "ax3.set_title('Final Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "ax3.set_ylabel('Accuracy')\n",
                "ax3.set_ylim([0.95, 1.0])\n",
                "ax3.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# ================== Plot 4: Training Time ==================\n",
                "ax4 = axes[1, 1]\n",
                "avg_times = [np.mean(times) for times in epoch_times_list]\n",
                "bars = ax4.bar(model_names, avg_times, color=colors, edgecolor='black', linewidth=1.5)\n",
                "\n",
                "for bar, t in zip(bars, avg_times):\n",
                "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
                "             f'{t:.2f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "ax4.set_title('Average Training Time per Epoch', fontsize=14, fontweight='bold')\n",
                "ax4.set_ylabel('Time (seconds)')\n",
                "ax4.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üî¨ Section 8: Gradient Analysis\n",
                "\n",
                "### Understanding Vanishing Gradients\n",
                "\n",
                "**Gradients** are like feedback to the network. Strong gradients = learning well. Weak gradients = not learning!\n",
                "\n",
                "```mermaid\n",
                "flowchart LR\n",
                "    A[Output Error] -->|Backprop| B[Layer 2]\n",
                "    B -->|Multiply| C[Layer 1]\n",
                "    C -->|Multiply| D[Input Layer]\n",
                "    \n",
                "    style A fill:#ffcdd2\n",
                "    style B fill:#fff9c4\n",
                "    style C fill:#c8e6c9\n",
                "    style D fill:#bbdefb\n",
                "```\n",
                "\n",
                "**Problem**: Sigmoid's derivative is max 0.25. When multiplied: 0.25 √ó 0.25 = 0.0625. Gradients shrink!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_gradient_magnitude(model, X_batch, y_batch):\n",
                "    \"\"\"\n",
                "    Compute mean absolute gradient for first layer.\n",
                "    \n",
                "    Higher = stronger gradient flow = better learning!\n",
                "    Lower = vanishing gradients = learning struggles!\n",
                "    \"\"\"\n",
                "    with tf.GradientTape() as tape:\n",
                "        # Forward pass\n",
                "        predictions = model(X_batch, training=True)\n",
                "        \n",
                "        # Compute loss\n",
                "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, predictions)\n",
                "        loss = tf.reduce_mean(loss)\n",
                "    \n",
                "    # Get gradients for first layer\n",
                "    first_layer_weights = model.layers[0].trainable_weights\n",
                "    gradients = tape.gradient(loss, first_layer_weights)\n",
                "    \n",
                "    # Compute mean absolute gradient\n",
                "    gradient_magnitude = np.mean(np.abs(gradients[0].numpy()))\n",
                "    \n",
                "    return gradient_magnitude\n",
                "\n",
                "# Compute gradients for each model\n",
                "gradient_mags = []\n",
                "X_batch = X_train[:32]  # Use 32 samples\n",
                "y_batch = y_train[:32]\n",
                "\n",
                "for model, name in zip(models, model_names):\n",
                "    grad_mag = compute_gradient_magnitude(model, X_batch, y_batch)\n",
                "    gradient_mags.append(grad_mag)\n",
                "    print(f\"{name}: Gradient Magnitude = {grad_mag:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîπ Visualize Gradient Magnitudes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot gradient magnitudes\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "bars = ax.bar(model_names, gradient_mags, color=colors, edgecolor='black', linewidth=1.5)\n",
                "\n",
                "for bar, mag in zip(bars, gradient_mags):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
                "            f'{mag:.6f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
                "\n",
                "ax.set_title('Gradient Magnitude in First Layer\\n(Higher = Better Gradient Flow)', \n",
                "             fontsize=14, fontweight='bold')\n",
                "ax.set_ylabel('Mean Absolute Gradient')\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Add annotation\n",
                "ax.annotate('‚ö†Ô∏è Lower values = Vanishing Gradients',\n",
                "            xy=(0, gradient_mags[0]), xytext=(0.5, max(gradient_mags) * 0.7),\n",
                "            fontsize=10, ha='center',\n",
                "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(OUTPUT_DIR, 'gradient_magnitude_comparison.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìä Section 9: Results Summary\n",
                "\n",
                "### Final Comparison Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary table\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"RESULTS SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(f\"{'Model':<20} {'Accuracy':>12} {'Avg Time/Epoch':>18} {'Gradient Mag':>15}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "for i, name in enumerate(model_names):\n",
                "    acc = histories[i].history['val_accuracy'][-1]\n",
                "    avg_time = np.mean(epoch_times_list[i])\n",
                "    grad = gradient_mags[i]\n",
                "    print(f\"{name:<20} {acc:>12.4f} {avg_time:>15.2f}s {grad:>15.6f}\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üíº Interview Perspective\n",
                "\n",
                "### Common Interview Questions\n",
                "\n",
                "**Q1: Why is ReLU preferred over Sigmoid in hidden layers?**\n",
                "> ReLU doesn't suffer from vanishing gradients because its derivative is 1 for positive inputs, allowing gradients to flow unchanged. Sigmoid's max derivative is 0.25, causing gradients to shrink exponentially in deep networks.\n",
                "\n",
                "**Q2: When would you still use Sigmoid?**\n",
                "> For the output layer in binary classification (outputs probability 0-1). Also in gates of LSTM/GRU networks where we need values between 0-1.\n",
                "\n",
                "**Q3: What's the \"dying ReLU\" problem?**\n",
                "> If a neuron's input is always negative, ReLU outputs 0 and its gradient is 0. The neuron \"dies\" and stops learning. Use Leaky ReLU to fix this.\n",
                "\n",
                "**Q4: Why is Tanh better than Sigmoid?**\n",
                "> Tanh is zero-centered (outputs -1 to 1), making gradients more balanced. Sigmoid outputs only positive values (0-1), causing zigzag updates."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéì Conclusion\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **ReLU trains fastest** - Simple computation (just max(0,x))\n",
                "2. **ReLU has strongest gradients** - No vanishing gradient problem\n",
                "3. **All models achieve similar accuracy on MNIST** - Dataset is simple enough\n",
                "4. **Differences become more pronounced in deeper networks**\n",
                "\n",
                "### Recommendations\n",
                "\n",
                "| Use Case | Recommended Activation |\n",
                "|----------|------------------------|\n",
                "| Hidden layers (default) | ReLU |\n",
                "| Binary classification output | Sigmoid |\n",
                "| Multi-class output | Softmax |\n",
                "| If ReLU neurons are dying | Leaky ReLU |\n",
                "| RNN gates | Sigmoid/Tanh |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}