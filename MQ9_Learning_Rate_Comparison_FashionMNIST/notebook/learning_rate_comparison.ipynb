{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§© Problem Statement\n",
                "\n",
                "## The Problem: \"Too Fast vs. Just Right\"\n",
                "Imagine you are training a computer to recognize clothes (like teaching a child to sort laundry). You want it to learn as fast as possible, but:\n",
                "1.  **If it learns too fast (High Learning Rate):** It might jump effectively but miss the \"perfect\" understanding, making mistakes or getting confused (overshooting).\n",
                "2.  **If it learns too slow (Low Learning Rate):** It will be very careful but might take forever to finish.\n",
                "\n",
                "**Our Goal:** We need to find the \"sweet spot.\" We will race two models:\n",
                "-   **Model A (Sprinter):** Uses a higher learning rate (0.001).\n",
                "-   **Model B (Jogger):** Uses a lower learning rate (0.0005).\n",
                "\n",
                "We want to see which one gets the best results without crashing!\n",
                "\n",
                "## ðŸŒ Real-World Analogy\n",
                "Think of **parking a car**:\n",
                "-   **High Learning Rate:** You press the gas pedal hard. You get to the spot fast, but you might hit the curb or park crookedly.\n",
                "-   **Low Learning Rate:** You tap the gas gently. It takes longer, but you park perfectly in the lines.\n",
                "\n",
                "## ðŸªœ Steps to Solve the Problem\n",
                "1.  **Get the Clothes (Data):** Download the Fashion-MNIST dataset directly (t-shirts, dresses, sneakers).\n",
                "2.  **Build the Brain (Model):** Create a simple Neural Network (MLP) with 2 hidden layers to think about the images.\n",
                "3.  **Run Experiment A (Fast):** Train the model with Learning Rate = 0.001. Watch how it learns.\n",
                "4.  **Run Experiment B (Slow):** Train the *exact same* model with Learning Rate = 0.0005.\n",
                "5.  **Compare Results:** Draw a graph (Learning Curve) to see who won the race (Speed vs. Accuracy).\n",
                "\n",
                "## ðŸŽ¯ Expected Output (Overall)\n",
                "We will generate two main things:\n",
                "1.  **A Plot:** A chart showing two lines.\n",
                "    -   Red line (Fast LR): Might be jagged or bumpy.\n",
                "    -   Blue line (Slow LR): Should be smoother.\n",
                "2.  **A Decision:** A conclusion saying, \"I choose Model [A or B] because...\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Line Explanation\n",
                "#### 2.1 What the line does\n",
                "Importing necessary libraries including PyTorch (for the brain), TorchVision (for eyes/images), and Matplotlib (for drawing graphs).\n",
                "#### 2.2 Why it is used\n",
                "To build, train, and visualize the neural network.\n",
                "#### 2.3 When to use it\n",
                "At the start of every Data Science project.\n",
                "#### 2.4 Where to use it\n",
                "Top of the file.\n",
                "#### 2.5 How to use it\n",
                "`import [library_name]`\n",
                "#### 2.6 How it works internally\n",
                "Loads the code from these libraries into memory so we can use their functions.\n",
                "#### 2.7 Output with sample examples\n",
                "No visible output, but enables functionality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import random\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Line Explanation\n",
                "#### 2.1 What the line does\n",
                "Sets a fixed seed for random number generation to ensure reproducibility.\n",
                "#### 2.2 Why it is used\n",
                "So that every time you run this, you get the exact same results (no \"random luck\").\n",
                "#### 2.3 When to use it\n",
                "In scientific experiments or debugging.\n",
                "#### 2.4 Where to use it\n",
                "Before any random operations.\n",
                "#### 2.5 How to use it\n",
                "`set_seed(42)`\n",
                "#### 2.6 How it works internally\n",
                "It initializes the random number generator with a specific starting point called a 'seed'.\n",
                "#### 2.7 Output with sample examples\n",
                "\"Random seed set to: 42\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def set_seed(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "    print(f\"Random seed set to: {seed}\")\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Line Explanation\n",
                "#### 2.1 What the line does\n",
                "Checks for a GPU and selects it; otherwise falls back to CPU.\n",
                "#### 2.2 Why it is used\n",
                "GPUs are faster for deep learning.\n",
                "#### 2.3 When to use it\n",
                "Always in PyTorch scripts.\n",
                "#### 2.4 Where to use it\n",
                "Before creating models.\n",
                "#### 2.5 How to use it\n",
                "`device = torch.device(...)`\n",
                "#### 2.6 How it works internally\n",
                "Queries system hardware drivers.\n",
                "#### 2.7 Output with sample examples\n",
                "\"Using device: cuda\" or \"Using device: cpu\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Hyperparameters\n",
                "BATCH_SIZE = 128\n",
                "EPOCHS = 15"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Line Explanation\n",
                "#### 2.1 What the line does\n",
                "Downloads and prepares the Fashion-MNIST data, converting images to Tensors.\n",
                "#### 2.2 Why it is used\n",
                "We need data to train our model.\n",
                "#### 2.3 When to use it\n",
                "At the data loading stage.\n",
                "#### 2.4 Where to use it\n",
                "Before training.\n",
                "#### 2.5 How to use it\n",
                "Use `torchvision.datasets.FashionMNIST`.\n",
                "#### 2.6 How it works internally\n",
                "Downloads zip, extracts, reads binary pixel data.\n",
                "#### 2.7 Output with sample examples\n",
                "Dataset objects ready for loading."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "\n",
                "print(\"Loading Fashion-MNIST data...\")\n",
                "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
                "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
                "\n",
                "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Class Explanation\n",
                "#### 2.1 What the code does\n",
                "Defines the Neural Network architecture (Brain structure).\n",
                "#### 2.2 Why it is used\n",
                "To define how the input (image) is processed to produce an output (class prediction).\n",
                "#### 2.3 When to use it\n",
                "When building a custom Deep Learning model.\n",
                "#### 2.4 Where to use it\n",
                "Before training loop.\n",
                "#### 2.5 How to use it\n",
                "Subclass `nn.Module` and define `__init__` and `forward`.\n",
                "#### 2.6 How it works internally\n",
                "PyTorch builds a computational graph from these layers.\n",
                "#### 2.7 Output with sample examples\n",
                "A model object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleMLP(nn.Module):\n",
                "    \"\"\"\n",
                "    A simple Multi-Layer Perceptron (MLP).\n",
                "    Structure: Input (784) -> FC1 (256) -> ReLU -> FC2 (128) -> ReLU -> Output (10)\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        super(SimpleMLP, self).__init__()\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.fc1 = nn.Linear(784, 256)\n",
                "        self.relu1 = nn.ReLU()\n",
                "        self.fc2 = nn.Linear(256, 128)\n",
                "        self.relu2 = nn.ReLU()\n",
                "        self.fc3 = nn.Linear(128, 10)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.flatten(x)\n",
                "        x = self.relu1(self.fc1(x))\n",
                "        x = self.relu2(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Function Explanation: `train_one_epoch`\n",
                "#### 2.1 What the code does\n",
                "Runs the training process for one entire pass through the dataset (one epoch).\n",
                "#### 2.2 Why it is used\n",
                "To update the weights of the model using backpropagation.\n",
                "#### 2.3 When to use it\n",
                "Inside the main experiment loop.\n",
                "#### 2.4 Where to use it\n",
                "Called repetitively for `EPOCHS` times.\n",
                "#### 2.5 How to use it\n",
                "`avg_loss, avg_acc = train_one_epoch(model, loader, criterion, optimizer)`\n",
                "#### 2.6 How it works internally\n",
                "Iterates batch by batch, calculates loss, computes gradients, and updates weights.\n",
                "#### 2.7 Output with sample examples\n",
                "Returns average loss (e.g., 0.5) and accuracy (e.g., 85.0)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for images, labels in loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item() * images.size(0)\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        total += labels.size(0)\n",
                "        correct += (predicted == labels).sum().item()\n",
                "        \n",
                "    return running_loss / total, 100 * correct / total"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Function Explanation: `validate`\n",
                "#### 2.1 What the code does\n",
                "Evaluates the model on unseen data (validation set) without updating weights.\n",
                "#### 2.2 Why it is used\n",
                "To check if the model is actually learning or just memorizing (overfitting).\n",
                "#### 2.3 When to use it\n",
                "After every training epoch.\n",
                "#### 2.4 Where to use it\n",
                "In the main loop.\n",
                "#### 2.5 How to use it\n",
                "`loss, acc = validate(model, loader, criterion)`\n",
                "#### 2.6 How it works internally\n",
                "Uses `torch.no_grad()` to save memory and skip gradient calculation.\n",
                "#### 2.7 Output with sample examples\n",
                "Validation loss and accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            running_loss += loss.item() * images.size(0)\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "            \n",
                "    return running_loss / total, 100 * correct / total"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Function Explanation: `run_experiment`\n",
                "#### 2.1 What the code does\n",
                "Orchestrates the entire training process for a specific Learning Rate.\n",
                "#### 2.2 Why it is used\n",
                "To encapsulate the experiment logic so we can run it twice (Run A and Run B).\n",
                "#### 2.3 When to use it\n",
                "For each experiment configuration.\n",
                "#### 2.4 Where to use it\n",
                "In the main execution block.\n",
                "#### 2.5 How to use it\n",
                "`history = run_experiment(1e-3, \"Run A\")`\n",
                "#### 2.6 How it works internally\n",
                "Initializes model, optimizer, runs loop for EPOCHS, logs history.\n",
                "#### 2.7 Output with sample examples\n",
                "A dictionary containing lists of loss and accuracy values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(learning_rate, experiment_name):\n",
                "    print(f\"\\n--- Starting {experiment_name} with LR={learning_rate} ---\")\n",
                "    set_seed(42)\n",
                "    model = SimpleMLP().to(device)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "    \n",
                "    history = {'val_loss': [], 'val_acc': []}\n",
                "    \n",
                "    for epoch in range(EPOCHS):\n",
                "        _, _ = train_one_epoch(model, train_loader, criterion, optimizer)\n",
                "        v_loss, v_acc = validate(model, val_loader, criterion)\n",
                "        history['val_loss'].append(v_loss)\n",
                "        history['val_acc'].append(v_acc)\n",
                "        print(f\"Epoch {epoch+1}: Val Loss={v_loss:.4f}, Val Acc={v_acc:.2f}%\")\n",
                "        \n",
                "    return history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Execution Block\n",
                "#### 2.1 What the code does\n",
                "Runs the two experiments (Run A vs Run B) and keeps their history.\n",
                "#### 2.2 Why it is used\n",
                "To generate the data needed for comparison.\n",
                "#### 2.3 When to use it\n",
                "After defining all functions.\n",
                "#### 2.4 Where to use it\n",
                "Main body.\n",
                "#### 2.5 How to use it\n",
                "Just run the cell.\n",
                "#### 2.6 How it works internally\n",
                "Calls `run_experiment` twice sequentially.\n",
                "#### 2.7 Output with sample examples\n",
                "Printed logs of training progress."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history_a = run_experiment(learning_rate=1e-3, experiment_name=\"Run_A_HighLR\")\n",
                "history_b = run_experiment(learning_rate=5e-4, experiment_name=\"Run_B_LowLR\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Plotting Results\n",
                "#### 2.1 What the code does\n",
                "Visualizes the difference between the two learning rates.\n",
                "#### 2.2 Why it is used\n",
                "To provide evidence for which learning rate is better.\n",
                "#### 2.3 When to use it\n",
                "After experiments are done.\n",
                "#### 2.4 Where to use it\n",
                "At the end of the notebook.\n",
                "#### 2.5 How to use it\n",
                "Uses `plt.plot()`.\n",
                "#### 2.6 How it works internally\n",
                " Draws lines connecting the loss values for each epoch.\n",
                "#### 2.7 Output with sample examples\n",
                "A graph comparing Red (Fast) vs Blue (Slow)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(history_a['val_loss'], label='Run A (LR=1e-3)', color='red', marker='o')\n",
                "plt.plot(history_b['val_loss'], label='Run B (LR=5e-4)', color='blue', marker='s')\n",
                "plt.title('Validation Loss Comparison')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}