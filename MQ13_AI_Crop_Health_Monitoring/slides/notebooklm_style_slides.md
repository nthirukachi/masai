# ğŸŒ¾ AI-Based Crop Health Monitoring
## Using Drone Multispectral Data

---

# Slide 1: Title & Objective

## ğŸ¯ Project Goal

Build an AI system that:
- Analyzes drone-captured vegetation data
- Classifies crops as **Healthy** or **Stressed**
- Creates spatial heatmaps for inspection

**Key Outcome:** 95.76% F1-Score with Logistic Regression

---

# Slide 2: Problem Statement

## â“ The Challenge

```mermaid
graph LR
    A[Large Farm] --> B[Can't check every plant manually]
    B --> C[Some plants are stressed]
    C --> D[Stress invisible until too late]
    D --> E[Crop loss & financial damage]
```

**Solution:** Drones + AI = Automated Crop Monitoring

---

# Slide 3: Real-World Use Case

## ğŸŒ Where This Is Used

| Industry | Application |
|----------|-------------|
| **Agriculture** | Crop stress detection |
| **Forestry** | Tree health monitoring |
| **Research** | Phenotyping experiments |
| **Insurance** | Damage assessment |

**Market Size:** Precision agriculture worth $12B+ globally

---

# Slide 4: Input Data

## ğŸ“Š Dataset Overview

- **Samples:** 1,200 grid cells
- **Features:** 13 vegetation indices
- **Target:** Healthy (65%) vs Stressed (35%)

**Key Features:**
| Index | Measures |
|-------|----------|
| NDVI | Overall greenness |
| Moisture Index | Water content |
| Canopy Density | Leaf coverage |

---

# Slide 5: Concepts Used (High Level)

## ğŸ§  ML Pipeline

```mermaid
graph LR
    A[Drone Data] --> B[Preprocessing]
    B --> C[Train 5 Models]
    C --> D[Evaluate & Compare]
    D --> E[Best Model]
    E --> F[Spatial Heatmap]
```

**Models:** Logistic Regression, Decision Tree, Random Forest, SVM, KNN

---

# Slide 6: Concepts Breakdown

## ğŸ“š Key Concepts Simplified

| Concept | Simple Explanation |
|---------|-------------------|
| **NDVI** | Plant health score (0 to 1) |
| **Classification** | Sorting into categories |
| **Train-Test Split** | Study vs exam questions |
| **F1-Score** | Balance of precision & recall |
| **Heatmap** | Colorful stress map |

---

# Slide 7: Step-by-Step Solution

## ğŸªœ Pipeline Steps

1. **Load Data** â†’ 1,200 samples, 13 features
2. **Encode Labels** â†’ Healthy=0, Stressed=1
3. **Split Data** â†’ 80% train, 20% test (stratified)
4. **Scale Features** â†’ StandardScaler (mean=0, std=1)
5. **Train Models** â†’ 5 algorithms
6. **Evaluate** â†’ F1-Score comparison
7. **Spatial Analysis** â†’ Generate heatmap

---

# Slide 8: Code Logic Summary

## ğŸ’» Key Code Patterns

```python
# Data Split (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y)

# Scaling (fit on train only!)
scaler.fit_transform(X_train)
scaler.transform(X_test)

# Model Training
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)
```

---

# Slide 9: Important Functions

## âš™ï¸ Parameters That Matter

| Function | Key Parameter | Purpose |
|----------|---------------|---------|
| `train_test_split` | `stratify=y` | Keep class ratio |
| `StandardScaler` | fit on train only | Avoid data leakage |
| `RandomForest` | `n_estimators=100` | 100 trees voting |
| `KNN` | `n_neighbors=5` | Check 5 nearest |

---

# Slide 10: Execution Output

## ğŸ“ˆ Results

### Model Comparison
| Model | F1-Score | ROC-AUC |
|-------|----------|---------|
| **Logistic Regression** | **95.76%** | **99.81%** |
| SVM | 92.22% | 99.31% |
| Decision Tree | 90.59% | 92.95% |
| Random Forest | 89.57% | 98.17% |
| KNN | 86.96% | 97.44% |

ğŸ† **Winner: Logistic Regression**

---

# Slide 11: Observations & Insights

## ğŸ” Key Findings

```mermaid
pie title Stress Distribution
    "Healthy" : 60.5
    "Critical Stress" : 30.1
    "High Priority" : 3.1
    "Other" : 6.3
```

**Insights:**
- Simple model beat complex ensembles
- Stress clusters in specific zones
- 361 critical cells need immediate attention

---

# Slide 12: Advantages & Limitations

## âœ… Strengths

| Advantage | Evidence |
|-----------|----------|
| High accuracy | 97% overall |
| Fast inference | Logistic Regression |
| Interpretable | Coefficient analysis |

## âš ï¸ Limitations

| Limitation | Impact |
|------------|--------|
| Single snapshot | No temporal trends |
| Binary only | Can't identify stress cause |
| Weather dependent | Clouds affect readings |

---

# Slide 13: Interview Takeaways

## ğŸ’¼ Key Points to Remember

1. **Why Logistic Regression won?**
   - Well-engineered features â†’ linear boundary

2. **How to handle imbalance?**
   - Stratify split + F1-Score

3. **Data leakage prevention?**
   - Never fit scaler on test data

4. **Why this matters?**
   - Early stress detection saves crops!

---

# Slide 14: Conclusion

## ğŸ¯ Summary

âœ… Built end-to-end crop health ML pipeline
âœ… Compared 5 classification models
âœ… Logistic Regression achieved **95.76% F1-Score**
âœ… Identified **361 critical zones** for inspection
âœ… Created spatial heatmap for targeted action

## ğŸš€ Future Improvements
- Time-series analysis
- Multi-class stress identification
- Deep learning on raw imagery

---

# Thank You! ğŸŒ¾ğŸš

**Questions?**

```
Remember:
NDVI = (NIR - Red) / (NIR + Red)
F1 = 2 Ã— (P Ã— R) / (P + R)
```
