{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cluster-Distance Feature Boost for Binary Perceptron\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ§© Problem Statement\n",
                "\n",
                "**What problem are we solving?**\n",
                "\n",
                "Imagine a teacher trying to identify students in \"Class A\" using basic info like height and weight. A simple classifier (Perceptron) struggles with just these 2 features.\n",
                "\n",
                "**The Solution:** Add \"distance-to-centroid\" features from K-Means clustering. These tell us how close each student is to the \"average\" of each class.\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸªœ Steps to Solve\n",
                "\n",
                "1. Generate synthetic data with 3 clusters (900 points)\n",
                "2. Create binary labels (cluster 0 = 1, others = 0)\n",
                "3. Standardize features (mean=0, std=1)\n",
                "4. Fit K-Means to find 3 cluster centers\n",
                "5. Compute distance-to-centroid features\n",
                "6. Train baseline Perceptron (2 original features)\n",
                "7. Train enhanced Perceptron (2 original + 3 distance features)\n",
                "8. Compare metrics over 5 random splits\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Expected Output\n",
                "\n",
                "| Metric | Baseline | Enhanced | Improvement |\n",
                "|--------|----------|----------|-------------|\n",
                "| Accuracy | ~57% | ~92% | +35% |\n",
                "| Precision | ~32% | ~90% | +58% |\n",
                "| Recall | ~61% | ~89% | +28% |\n",
                "| ROC AUC | ~49% | ~98% | +49% |\n",
                "\n",
                "**Success Criteria:** At least one metric improves by â‰¥5 percentage points."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 1: Importing Libraries\n",
                "\n",
                "### 2.1 What: Import numpy\n",
                "NumPy is a library for working with arrays and mathematical operations.\n",
                "\n",
                "### 2.2 Why:\n",
                "We need arrays to store data efficiently. Python lists are slow for math operations, but numpy is 10-100x faster because it uses C code internally.\n",
                "\n",
                "### 2.3 When:\n",
                "Always import at the start of any data science or machine learning project.\n",
                "\n",
                "### 2.4 Where:\n",
                "Every ML/Data Science project uses numpy for numerical operations.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "import numpy as np  # 'np' is the standard abbreviation\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "NumPy stores data in contiguous memory blocks and uses compiled C code for operations, making it much faster than Python loops.\n",
                "\n",
                "### 2.7 Output:\n",
                "No visible output - just makes numpy available as 'np' for the rest of the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import numpy for numerical operations (arrays, math)\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Import make_blobs\n",
                "A function to generate synthetic clustered data.\n",
                "\n",
                "### 2.2 Why:\n",
                "We need test data with known cluster structure. Real-world data is messy, but synthetic data lets us understand concepts clearly first. It's like practicing with training wheels!\n",
                "\n",
                "### 2.3 When:\n",
                "For learning, testing algorithms, or when real data isn't available.\n",
                "\n",
                "### 2.4 Where:\n",
                "Tutorials, prototyping, algorithm comparisons.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "from sklearn.datasets import make_blobs\n",
                "X, y = make_blobs(n_samples=100, centers=3)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "1. Randomly places k center points in feature space\n",
                "2. Generates n_samples/k points around each center\n",
                "3. Adds Gaussian noise based on cluster_std\n",
                "\n",
                "### 2.7 Output:\n",
                "Returns X (features array) and y (cluster labels)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import make_blobs to generate synthetic clustered data\n",
                "from sklearn.datasets import make_blobs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Import StandardScaler\n",
                "A tool to normalize features to mean=0 and standard deviation=1.\n",
                "\n",
                "### 2.2 Why:\n",
                "Different features have different scales (e.g., age in years, salary in thousands). Scaling makes them comparable. Both Perceptron and K-Means are sensitive to feature scales!\n",
                "\n",
                "**Analogy:** It's like converting all currencies to dollars before comparing prices.\n",
                "\n",
                "### 2.3 When:\n",
                "Before training most ML models, especially distance-based ones.\n",
                "\n",
                "### 2.4 Where:\n",
                "Almost every ML pipeline includes scaling as a preprocessing step.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "For each value: `z = (value - mean) / std_deviation`\n",
                "\n",
                "### 2.7 Output:\n",
                "Transformed data where each feature has meanâ‰ˆ0, stdâ‰ˆ1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import StandardScaler to normalize features\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Import KMeans\n",
                "A clustering algorithm that groups data into k clusters.\n",
                "\n",
                "### 2.2 Why:\n",
                "We want to find natural groupings in data and use distances to these group centers as new features. This is the key to \"boosting\" our classifier!\n",
                "\n",
                "**Analogy:** Finding the \"center\" of each friend group in a school cafeteria.\n",
                "\n",
                "### 2.3 When:\n",
                "When you suspect data has natural clusters/groups.\n",
                "\n",
                "### 2.4 Where:\n",
                "Customer segmentation, image compression, feature engineering.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "kmeans = KMeans(n_clusters=3)\n",
                "kmeans.fit(X)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "1. Initialize k random centroids\n",
                "2. Assign each point to nearest centroid\n",
                "3. Move centroids to mean of assigned points\n",
                "4. Repeat until convergence\n",
                "\n",
                "### 2.7 Output:\n",
                "Cluster labels and centroid locations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import KMeans for clustering\n",
                "from sklearn.cluster import KMeans"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Import Perceptron\n",
                "The simplest neural network - just weights + bias.\n",
                "\n",
                "### 2.2 Why:\n",
                "It's a great baseline to show improvement from feature engineering. If we can boost a simple model, the technique works!\n",
                "\n",
                "**Analogy:** A teacher with one simple rule: \"If score > 50, pass. Otherwise, fail.\"\n",
                "\n",
                "### 2.3 When:\n",
                "As a baseline, for linearly separable data, for teaching.\n",
                "\n",
                "### 2.4 Where:\n",
                "First step in learning neural networks, simple classification.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "model = Perceptron()\n",
                "model.fit(X, y)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "Learns weights w such that `sign(wÂ·x + b)` predicts the class.\n",
                "\n",
                "### 2.7 Output:\n",
                "Trained model that can predict class labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Perceptron - the simplest neural network\n",
                "from sklearn.linear_model import Perceptron"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Import train_test_split\n",
                "A function to divide data into training and test sets.\n",
                "\n",
                "### 2.2 Why:\n",
                "We need separate data to train and evaluate the model. Training and testing on the same data gives false confidence (overfitting). It's like a student memorizing answers vs understanding concepts!\n",
                "\n",
                "### 2.3 When:\n",
                "Always before training any ML model.\n",
                "\n",
                "### 2.4 Where:\n",
                "Every ML project with supervised learning.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "Randomly shuffles data, then splits at the specified ratio (e.g., 75% train, 25% test).\n",
                "\n",
                "### 2.7 Output:\n",
                "Four arrays: training features, test features, training labels, test labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import train_test_split to divide data\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Import classification metrics\n",
                "Functions to measure how good our classifier is.\n",
                "\n",
                "### 2.2 Why:\n",
                "We need to measure performance. Different metrics tell us different things:\n",
                "- **Accuracy**: Overall correctness\n",
                "- **Precision**: Of predictions, how many are correct?\n",
                "- **Recall**: Did we find all positives?\n",
                "- **ROC AUC**: Overall ranking quality\n",
                "\n",
                "### 2.3 When:\n",
                "After making predictions on test data.\n",
                "\n",
                "### 2.4 Where:\n",
                "Every classification problem needs evaluation metrics.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "accuracy_score(y_true, y_pred)\n",
                "precision_score(y_true, y_pred)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "Compares predictions to actual labels, calculates ratios.\n",
                "\n",
                "### 2.7 Output:\n",
                "Numbers between 0 and 1 (higher is better)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import classification metrics\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 2: Generate Synthetic Data\n",
                "\n",
                "### 2.1 What: Create blob data with make_blobs\n",
                "We're generating 900 data points grouped into 3 clusters.\n",
                "\n",
                "### 2.2 Why:\n",
                "We need structured data where we know the \"ground truth\". This helps us understand if our algorithm is working correctly.\n",
                "\n",
                "**Analogy:** Creating a practice test where we already know all the answers.\n",
                "\n",
                "### 2.3 When:\n",
                "At the start of the project, before any training.\n",
                "\n",
                "### 2.4 Where:\n",
                "Any clustering or classification tutorial.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "X, cluster_ids = make_blobs(n_samples=900, centers=3, ...)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "1. Randomly picks 3 center points\n",
                "2. Generates 300 points around each center\n",
                "3. Adds Gaussian noise based on cluster_std\n",
                "\n",
                "### 2.7 Output:\n",
                "- X: (900, 2) array of coordinates\n",
                "- cluster_ids: (900,) array of cluster labels (0, 1, or 2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### make_blobs Parameter Explanation\n",
                "\n",
                "| Parameter | Value | Explanation |\n",
                "|-----------|-------|-------------|\n",
                "| `n_samples` | 900 | Total number of data points (300 per cluster) |\n",
                "| `centers` | 3 | Number of cluster centers (groups) |\n",
                "| `cluster_std` | [1.0, 1.2, 1.4] | Spread of each cluster (higher = more spread) |\n",
                "| `random_state` | 12 | Seed for reproducibility (same data every run) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic blob data with 3 clusters\n",
                "X, cluster_ids = make_blobs(\n",
                "    n_samples=900,      # 900 total points (300 per cluster)\n",
                "    centers=3,          # 3 cluster centers\n",
                "    cluster_std=[1.0, 1.2, 1.4],  # Different spreads per cluster\n",
                "    random_state=12,    # For reproducibility\n",
                ")\n",
                "\n",
                "print(f\"X shape: {X.shape}\")\n",
                "print(f\"First 5 points:\\n{X[:5]}\")\n",
                "print(f\"\\nCluster IDs: {np.unique(cluster_ids)} (0, 1, 2)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 What: Create binary labels\n",
                "Convert cluster labels (0, 1, 2) to binary (1, 0, 0).\n",
                "\n",
                "### 2.2 Why:\n",
                "We want a binary classification problem: \"Is this point in cluster 0 or not?\"\n",
                "\n",
                "**Analogy:** Asking \"Is this student in Class A?\" instead of \"Which of the 3 classes is this student in?\"\n",
                "\n",
                "### 2.3 When:\n",
                "When converting multi-class to binary problem.\n",
                "\n",
                "### 2.4 Where:\n",
                "One-vs-all classification, binary problems.\n",
                "\n",
                "### 2.5 How:\n",
                "```python\n",
                "y = (cluster_ids == 0).astype(int)\n",
                "```\n",
                "\n",
                "### 2.6 Internal Working:\n",
                "- `cluster_ids == 0` gives True/False array\n",
                "- `.astype(int)` converts Trueâ†’1, Falseâ†’0\n",
                "\n",
                "### 2.7 Output:\n",
                "~300 points with label 1, ~600 with label 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create binary labels: 1 if cluster 0, else 0\n",
                "y = (cluster_ids == 0).astype(int)\n",
                "\n",
                "print(f\"Binary labels: {np.unique(y)} (0 or 1)\")\n",
                "print(f\"Class 1 count: {sum(y)} (cluster 0 points)\")\n",
                "print(f\"Class 0 count: {len(y) - sum(y)} (clusters 1 and 2)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 3: Visualize the Data\n",
                "\n",
                "Let's see what our data looks like! This helps us understand why a simple Perceptron might struggle."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create a scatter plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "\n",
                "# Plot class 1 (cluster 0) in blue\n",
                "plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', label='Class 1 (cluster 0)', alpha=0.6)\n",
                "\n",
                "# Plot class 0 (clusters 1, 2) in red\n",
                "plt.scatter(X[y==0, 0], X[y==0, 1], c='red', label='Class 0 (clusters 1, 2)', alpha=0.6)\n",
                "\n",
                "plt.xlabel('Feature 1')\n",
                "plt.ylabel('Feature 2')\n",
                "plt.title('Original Data: Binary Classification Problem')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 4: Run the Full Experiment\n",
                "\n",
                "Now we'll run the complete experiment over 5 random splits to compare the baseline vs enhanced Perceptron."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper Function: Standardize Features\n",
                "\n",
                "This function scales features to have mean=0 and std=1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def standardize_features(X_train, X_test):\n",
                "    \"\"\"\n",
                "    Standardize features to mean=0, std=1.\n",
                "    \n",
                "    IMPORTANT: Fit on training data, transform both!\n",
                "    This prevents data leakage.\n",
                "    \"\"\"\n",
                "    scaler = StandardScaler()\n",
                "    X_train_scaled = scaler.fit_transform(X_train)  # Learn from train\n",
                "    X_test_scaled = scaler.transform(X_test)        # Apply to test\n",
                "    return X_train_scaled, X_test_scaled"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper Function: Create Distance Features\n",
                "\n",
                "This is the KEY function! It uses K-Means to compute distance-to-centroid features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_distance_features(X_train, X_test, n_clusters=3):\n",
                "    \"\"\"\n",
                "    Create distance-to-centroid features using K-Means.\n",
                "    \n",
                "    For each point, computes distance to each of the k cluster centers.\n",
                "    This gives us k new features!\n",
                "    \"\"\"\n",
                "    # Fit K-Means on training data only\n",
                "    kmeans = KMeans(n_clusters=n_clusters, random_state=12, n_init=10)\n",
                "    kmeans.fit(X_train)\n",
                "    \n",
                "    # Transform to get distances (this is the magic!)\n",
                "    train_distances = kmeans.transform(X_train)\n",
                "    test_distances = kmeans.transform(X_test)\n",
                "    \n",
                "    return train_distances, test_distances"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper Function: Train and Evaluate\n",
                "\n",
                "Train a Perceptron and compute all metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
                "    \"\"\"\n",
                "    Train Perceptron and return all metrics.\n",
                "    \"\"\"\n",
                "    # Create and train Perceptron\n",
                "    model = Perceptron(random_state=12, max_iter=1000, tol=1e-3)\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_scores = model.decision_function(X_test)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    return {\n",
                "        'accuracy': accuracy_score(y_test, y_pred),\n",
                "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
                "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
                "        'roc_auc': roc_auc_score(y_test, y_scores)\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Run the Experiment: 5 Random Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Storage for metrics\n",
                "baseline_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'roc_auc': []}\n",
                "enhanced_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'roc_auc': []}\n",
                "\n",
                "print(\"Running 5 random splits...\\n\")\n",
                "\n",
                "for split_idx in range(5):\n",
                "    # 1. Split data (different random state each time)\n",
                "    random_state = 42 + split_idx\n",
                "    X_train, X_test, y_train, y_test = train_test_split(\n",
                "        X, y, test_size=0.25, random_state=random_state\n",
                "    )\n",
                "    \n",
                "    # 2. Standardize features\n",
                "    X_train_scaled, X_test_scaled = standardize_features(X_train, X_test)\n",
                "    \n",
                "    # 3. Create distance features\n",
                "    train_dist, test_dist = create_distance_features(X_train_scaled, X_test_scaled)\n",
                "    \n",
                "    # 4. Create enhanced feature sets (original + distance)\n",
                "    X_train_enhanced = np.column_stack([X_train_scaled, train_dist])\n",
                "    X_test_enhanced = np.column_stack([X_test_scaled, test_dist])\n",
                "    \n",
                "    # 5. Train and evaluate BASELINE (original 2 features)\n",
                "    baseline_result = train_and_evaluate(\n",
                "        X_train_scaled, X_test_scaled, y_train, y_test\n",
                "    )\n",
                "    \n",
                "    # 6. Train and evaluate ENHANCED (5 features)\n",
                "    enhanced_result = train_and_evaluate(\n",
                "        X_train_enhanced, X_test_enhanced, y_train, y_test\n",
                "    )\n",
                "    \n",
                "    # Collect metrics\n",
                "    for metric in baseline_metrics:\n",
                "        baseline_metrics[metric].append(baseline_result[metric])\n",
                "        enhanced_metrics[metric].append(enhanced_result[metric])\n",
                "    \n",
                "    print(f\"Split {split_idx + 1}: Baseline Acc={baseline_result['accuracy']:.3f}, \"\n",
                "          f\"Enhanced Acc={enhanced_result['accuracy']:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 5: Results - Metric Comparison Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate averages\n",
                "baseline_avg = {k: np.mean(v) for k, v in baseline_metrics.items()}\n",
                "enhanced_avg = {k: np.mean(v) for k, v in enhanced_metrics.items()}\n",
                "\n",
                "# Print formatted results\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"RESULTS: AVERAGED OVER 5 RANDOM SPLITS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(f\"\\n{'Metric':<12} {'Baseline':>12} {'Enhanced':>12} {'Improvement':>14}\")\n",
                "print(\"-\" * 52)\n",
                "\n",
                "for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:\n",
                "    base = baseline_avg[metric]\n",
                "    enh = enhanced_avg[metric]\n",
                "    improvement = (enh - base) * 100\n",
                "    marker = \" [OK]\" if improvement >= 5 else \"\"\n",
                "    print(f\"{metric.upper():<12} {base:>12.4f} {enh:>12.4f} {improvement:>+12.2f}%{marker}\")\n",
                "\n",
                "print(\"-\" * 52)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 6: Why Distance Features Help (200-Word Explanation)\n",
                "\n",
                "The enhanced model outperforms the baseline because **distance-to-centroid features capture CLUSTER GEOMETRY** that the original 2D features cannot express.\n",
                "\n",
                "In the original feature space, the Perceptron tries to draw a single linear boundary (hyperplane) to separate class 1 (cluster 0) from class 0 (clusters 1, 2). However, cluster 0 may be positioned such that a simple line cannot cleanly separate it from the overlapping regions of clusters 1 and 2.\n",
                "\n",
                "By adding distance features, we transform each point into a **5D space** where:\n",
                "- Points **CLOSE** to cluster 0's center have **SMALL** distance to centroid 0\n",
                "- Points **FAR** from cluster 0's center have **LARGE** distance to centroid 0\n",
                "\n",
                "This **BOUNDARY SHIFT** is critical: in the enhanced space, the decision boundary can now leverage \"closeness to cluster 0\" as a feature. Points with small distance-to-centroid-0 are highly likely to be class 1, regardless of their original x,y position.\n",
                "\n",
                "The **cluster geometry** (tight cluster 0 with std=1.0 vs spread clusters 1,2 with std=1.2,1.4) means that distance-to-centroid-0 becomes a strong signal for class membership. The Perceptron's linear boundary in this enriched space effectively creates a **NON-LINEAR boundary** in the original 2D space."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Section 7: Success Criteria Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if we meet success criteria\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"SUCCESS CRITERIA CHECK\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "improvements = {\n",
                "    metric: (enhanced_avg[metric] - baseline_avg[metric]) * 100\n",
                "    for metric in baseline_avg\n",
                "}\n",
                "\n",
                "success = any(imp >= 5 for imp in improvements.values())\n",
                "\n",
                "if success:\n",
                "    print(\"\\n[OK] SUCCESS: At least one metric improved by >=5 percentage points!\\n\")\n",
                "    for metric, imp in improvements.items():\n",
                "        if imp >= 5:\n",
                "            print(f\"   - {metric.upper()}: +{imp:.2f}%\")\n",
                "else:\n",
                "    print(\"\\n[X] FAILED: No metric improved by >=5 percentage points.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "| What We Did | Why It Worked |\n",
                "|-------------|---------------|\n",
                "| Added distance-to-centroid features | Captured cluster geometry |\n",
                "| Used K-Means transform() | Computed distances efficiently |\n",
                "| Combined original + distance features | Gave Perceptron more information |\n",
                "| Linear boundary in 5D | = Non-linear boundary in 2D |\n",
                "\n",
                "**Key Takeaway:** Feature engineering can dramatically improve even simple classifiers!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}