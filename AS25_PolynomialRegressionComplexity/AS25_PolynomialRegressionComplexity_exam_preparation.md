# AS25: Polynomial Regression and Model Complexity - Exam Preparation

> ðŸ“š **Exam Preparation Guide** for Polynomial Regression & Bias-Variance Trade-off
> ðŸ“Š Includes: MCQs, MSQs, Numerical Problems, Shortcuts, Quick Revision

---

## Section A: Multiple Choice Questions (MCQ) - 15 Questions

### MCQ 1
**Question:** In polynomial regression Y = Î²â‚€ + Î²â‚X + Î²â‚‚XÂ², what is the degree of the polynomial?

**Options:**
- A) 0
- B) 1
- C) 2
- D) 3

**âœ… Correct Answer:** C

**ðŸ“– Explanation:** The degree is the highest power of X in the equation. Here, XÂ² has power 2.

**âŒ Why Others Are Wrong:**
- A) Degree 0 would be just Î²â‚€ (constant)
- B) Degree 1 is simple linear regression (only Î²â‚€ + Î²â‚X)
- D) Degree 3 would require XÂ³ term

---

### MCQ 2
**Question:** Why is polynomial regression still called "linear" regression?

**Options:**
- A) Because the curve is a straight line
- B) Because the model is linear in weights (Î² values)
- C) Because X values are linear
- D) Because the error term is linear

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** The model is a linear combination of weights (Î²â‚€ + Î²â‚Ã—featureâ‚ + Î²â‚‚Ã—featureâ‚‚...). The non-linearity is only in features (X, XÂ²...), not in weights.

---

### MCQ 3
**Question:** What does HIGH BIAS indicate?

**Options:**
- A) Overfitting
- B) Underfitting
- C) Perfect fit
- D) High variance

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** High bias means the model is too simple to capture the underlying pattern = UNDERFITTING.

---

### MCQ 4
**Question:** What does HIGH VARIANCE indicate?

**Options:**
- A) Underfitting
- B) Model is too simple
- C) Overfitting
- D) Low training error and low testing error

**âœ… Correct Answer:** C

**ðŸ“– Explanation:** High variance means model is too sensitive to training data = OVERFITTING. Low train error but high test error.

---

### MCQ 5
**Question:** What is the total error formula in bias-variance decomposition?

**Options:**
- A) Total Error = Bias + Variance
- B) Total Error = BiasÂ² + Variance + Irreducible Error
- C) Total Error = Bias Ã— Variance
- D) Total Error = BiasÂ² - Variance

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** Total Error = BiasÂ² + Variance + Irreducible Error. Bias is squared in the formula.

---

### MCQ 6
**Question:** You have 10 data points. Which polynomial degree will give ZERO training error?

**Options:**
- A) Degree 5
- B) Degree 8
- C) Degree 9
- D) Degree 10

**âœ… Correct Answer:** C

**ðŸ“– Explanation:** With n points, degree = n-1 gives zero training error. 10 points â†’ degree 9.

---

### MCQ 7
**Question:** Which sklearn function creates polynomial features?

**Options:**
- A) PolynomialRegression
- B) PolynomialFeatures
- C) PolynomialTransform
- D) PowerFeatures

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** `sklearn.preprocessing.PolynomialFeatures` creates X, XÂ², XÂ³... from input X.

---

### MCQ 8
**Question:** What is the purpose of a VALIDATION set?

**Options:**
- A) Train model parameters
- B) Final evaluation
- C) Tune hyperparameters like polynomial degree
- D) Store test results

**âœ… Correct Answer:** C

**ðŸ“– Explanation:** Validation set is used to select best hyperparameters (like degree) without touching the test set.

---

### MCQ 9
**Question:** Training MSE keeps decreasing as degree increases, but validation MSE starts increasing. This indicates:

**Options:**
- A) Underfitting
- B) Good fit
- C) Overfitting
- D) Irreducible error

**âœ… Correct Answer:** C

**ðŸ“– Explanation:** When training error decreases but validation error increases = model is overfitting (memorizing training data).

---

### MCQ 10
**Question:** What is Runge Phenomenon?

**Options:**
- A) Polynomial underfitting at center
- B) High-degree polynomials oscillate wildly at boundaries
- C) Linear regression failing
- D) Variance becoming zero

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** Runge phenomenon: high-degree polynomials become unstable and oscillate at edges of data range.

---

### MCQ 11
**Question:** Irreducible error can be reduced by:

**Options:**
- A) Using higher degree polynomial
- B) Getting more data
- C) Using regularization
- D) Cannot be reduced

**âœ… Correct Answer:** D

**ðŸ“– Explanation:** Irreducible error is inherent noise in data - no model can reduce it.

---

### MCQ 12
**Question:** In sklearn, when should you use transform() instead of fit_transform()?

**Options:**
- A) On training data
- B) On validation and test data
- C) Never
- D) Always

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** fit_transform() on training, transform() on validation/test. This prevents data leakage.

---

### MCQ 13
**Question:** Model with degree=1 has high training AND testing error. What's the problem?

**Options:**
- A) Overfitting
- B) Underfitting
- C) Too much data
- D) Validation error

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** Both errors high = model too simple = underfitting. Solution: increase degree.

---

### MCQ 14
**Question:** Polynomial regression vs Multiple Linear Regression - key difference?

**Options:**
- A) Both use multiple different features
- B) Polynomial uses powers of ONE feature, MLR uses multiple DIFFERENT features
- C) Both use same feature
- D) No difference

**âœ… Correct Answer:** B

**ðŸ“– Explanation:** Polynomial: X, XÂ², XÂ³ (one variable, different powers). MLR: Xâ‚, Xâ‚‚, Xâ‚ƒ (different variables).

---

### MCQ 15
**Question:** The "sweet spot" in the bias-variance trade-off is:

**Options:**
- A) Minimum training error
- B) Maximum complexity
- C) Minimum total error (bottom of U-curve)
- D) Zero error point

**âœ… Correct Answer:** C

**ðŸ“– Explanation:** The sweet spot is at the bottom of the U-shaped total error curve, balancing bias and variance.

---

## Section B: Multiple Select Questions (MSQ) - 10 Questions

### MSQ 1
**Question:** Which are symptoms of OVERFITTING? (Select ALL that apply)

**Options:**
- âœ… A) Very low training error
- âŒ B) High training error
- âœ… C) High testing/validation error
- âœ… D) Large gap between train and test error
- âŒ E) Model is too simple

**ðŸ“– Explanation:** Overfitting = low train error + high test error + large gap. Model is too COMPLEX, not simple.

---

### MSQ 2
**Question:** Which are hyperparameters in polynomial regression? (Select ALL)

**Options:**
- âœ… A) Polynomial degree
- âŒ B) Coefficients (Î² values)
- âœ… C) Learning rate (if using gradient descent)
- âŒ D) Intercept value
- âœ… E) Regularization strength

**ðŸ“– Explanation:** Hyperparameters are set BEFORE training. Î² and intercept are LEARNED during training.

---

### MSQ 3
**Question:** Which help DETECT overfitting? (Select ALL)

**Options:**
- âœ… A) Comparing train vs validation error
- âœ… B) Cross-validation
- âœ… C) Plotting learning curves
- âŒ D) Only looking at training error
- âœ… E) Using held-out test set

---

### MSQ 4
**Question:** When does underfitting occur? (Select ALL)

**Options:**
- âœ… A) Polynomial degree too low
- âœ… B) Model too simple for data complexity
- âŒ C) Model too complex
- âœ… D) High bias
- âŒ E) High variance

---

### MSQ 5
**Question:** Valid data splitting strategies: (Select ALL)

**Options:**
- âœ… A) 60% train, 20% validation, 20% test
- âœ… B) 70% train, 15% validation, 15% test
- âŒ C) 100% training, use same for testing
- âœ… D) 80% train, 20% test (when using cross-validation)
- âŒ E) Use test set to select hyperparameters

---

### MSQ 6
**Question:** What increases with polynomial degree? (Select ALL)

**Options:**
- âœ… A) Number of parameters to learn
- âŒ B) Bias
- âœ… C) Risk of overfitting
- âœ… D) Model flexibility
- âœ… E) Variance

---

### MSQ 7
**Question:** Why is polynomial regression computationally cheaper than MLR with many features?

**Options:**
- âœ… A) Still works in 2D space (one input variable)
- âŒ B) Uses different algorithm
- âœ… C) Features (X, XÂ², ...) are from same variable, not independent
- âŒ D) Doesn't use matrices
- âœ… E) No multicollinearity from different independent sources

---

### MSQ 8
**Question:** What can reduce VARIANCE? (Select ALL)

**Options:**
- âœ… A) Decrease polynomial degree
- âœ… B) Get more training data
- âœ… C) Use regularization
- âŒ D) Increase model complexity
- âœ… E) Cross-validation for model selection

---

### MSQ 9
**Question:** Components of Total Error: (Select ALL)

**Options:**
- âœ… A) BiasÂ²
- âœ… B) Variance
- âœ… C) Irreducible Error
- âŒ D) Training Error only
- âŒ E) Validation Error only

---

### MSQ 10
**Question:** When should you STOP increasing polynomial degree? (Select ALL)

**Options:**
- âœ… A) When validation error starts increasing
- âœ… B) When train-validation gap becomes large
- âŒ C) When training error reaches zero
- âœ… D) When further increases don't improve validation
- âŒ E) Never - higher is always better

---

## Section C: Numerical/Calculation Questions - 6 Questions

### Numerical 1
**Question:** Given n = 8 data points, what polynomial degree will give zero training error?

**Solution:**
```
Rule: Degree = n - 1 for zero training error
Degree = 8 - 1 = 7
```

**âœ… Final Answer:** Degree = 7

---

### Numerical 2
**Question:** For polynomial Y = 3 + 2X + 4XÂ² - XÂ³, if X = 2, calculate Y.

**Solution:**
```
Y = 3 + 2(2) + 4(2Â²) - (2Â³)
Y = 3 + 4 + 4(4) - 8
Y = 3 + 4 + 16 - 8
Y = 15
```

**âœ… Final Answer:** Y = 15

---

### Numerical 3
**Question:** PolynomialFeatures(degree=3) on input X. How many features are created (including bias)?

**Solution:**
```
For single input X with degree 3:
Features = 1 (bias) + XÂ¹ + XÂ² + XÂ³ = 4 features

Or formula: For single variable, features = degree + 1
Features = 3 + 1 = 4
```

**âœ… Final Answer:** 4 features (1, X, XÂ², XÂ³)

---

### Numerical 4
**Question:** Train MSE = 0.5, Validation MSE = 5.0. What is the train-validation gap?

**Solution:**
```
Gap = Validation MSE - Train MSE
Gap = 5.0 - 0.5 = 4.5

Interpretation: Large gap indicates OVERFITTING
```

**âœ… Final Answer:** Gap = 4.5 (indicates overfitting)

---

### Numerical 5
**Question:** If Total Error = 10, Variance = 4, Irreducible Error = 2, what is Bias?

**Solution:**
```
Total Error = BiasÂ² + Variance + Irreducible Error
10 = BiasÂ² + 4 + 2
10 = BiasÂ² + 6
BiasÂ² = 4
Bias = 2
```

**âœ… Final Answer:** Bias = 2

---

### Numerical 6
**Question:** Dataset has 100 samples. Using 60-20-20 split, how many samples in each set?

**Solution:**
```
Training: 100 Ã— 0.60 = 60 samples
Validation: 100 Ã— 0.20 = 20 samples
Testing: 100 Ã— 0.20 = 20 samples
```

**âœ… Final Answer:** Train=60, Val=20, Test=20

---

## Section D: Fill in the Blanks - 5 Questions

### Fill 1
**Question:** The phenomenon where high-degree polynomials oscillate wildly at data boundaries is called _______.

**Answer:** Runge Phenomenon

---

### Fill 2
**Question:** Error that cannot be reduced by any model is called _______ error.

**Answer:** Irreducible

---

### Fill 3
**Question:** In sklearn, use _______ on validation/test data instead of fit_transform() to avoid data leakage.

**Answer:** transform()

---

### Fill 4
**Question:** Polynomial regression is linear in _______ but non-linear in _______.

**Answer:** weights (coefficients) ; features (X powers)

---

### Fill 5
**Question:** The _______ set is used to tune hyperparameters, while _______ set is for final evaluation.

**Answer:** validation ; test

---

## ðŸ“š Quick Revision Points

### Key Formulas

| Formula | Meaning |
|---------|---------|
| Y = Î²â‚€ + Î²â‚X + Î²â‚‚XÂ² + ... + Î²dXáµˆ | Polynomial regression equation |
| Total Error = BiasÂ² + Variance + Irreducible | Error decomposition |
| n points â†’ degree (n-1) = zero train error | Overfitting rule |
| Features = degree + 1 (single variable) | Feature count |

### Comparison Table

| Aspect | Underfitting | Good Fit | Overfitting |
|--------|--------------|----------|-------------|
| **Bias** | HIGH | Balanced | LOW |
| **Variance** | LOW | Balanced | HIGH |
| **Train Error** | HIGH | Medium | LOW (~0) |
| **Test Error** | HIGH | Medium | HIGH |
| **Degree** | Too low | Just right | Too high |
| **Action** | Increase degree | Keep | Decrease degree |

---

## ðŸš€ Section E: Shortcuts & Cheat Codes

### âš¡ One-Liner Shortcuts

| Concept | Shortcut | When to Use |
|---------|----------|-------------|
| Zero train error | Degree = n-1 | Checking overfitting potential |
| Feature count | Degree + 1 | Estimating complexity |
| Overfitting signal | Val MSEâ†‘ while Train MSEâ†“ | Model selection |
| Underfitting signal | Both errors HIGH | Need more complexity |

### ðŸŽ¯ Memory Tricks (Mnemonics)

1. **BIAS-UNDER:** Bias â†’ Underfitting (both have letter 'i')
2. **VARIANCE-OVER:** Variance â†’ Overfitting (both have letter 'v')
3. **U-CURVE:** Total error is U-shaped, sweet spot at bottom
4. **TVT = Train, Validate, Test:** Order of data usage

### ðŸ“ Last-Minute Formula Sheet

```
ðŸ“Œ Total Error = BiasÂ² + Variance + Irreducible
ðŸ“Œ Zero Train Error when Degree = n - 1
ðŸ“Œ Features = Degree + 1 (for single variable)
ðŸ“Œ Split Ratio: 60% train, 20% val, 20% test
```

### ðŸŽ“ Interview One-Liners

| Question | Answer Template |
|----------|-----------------|
| "What is bias-variance trade-off?" | "As model complexity increases, bias decreases but variance increases. Goal is to find optimal balance." |
| "When to use polynomial regression?" | "When relationship is non-linear but has smooth curvature. Check residual plots for patterns." |
| "How to choose degree?" | "Use validation set - pick degree with lowest validation error." |
| "Why is polynomial LR still linear?" | "Because it's linear in weights (coefficients), not in features." |

### âš ï¸ "If You Forget Everything, Remember This"

1. **HIGH BIAS = Underfitting = Increase complexity**
2. **HIGH VARIANCE = Overfitting = Decrease complexity**
3. **Use VALIDATION set for hyperparameter tuning, TEST set for final evaluation only**
4. **Sweet spot = Bottom of U-shaped error curve**

### ðŸ”„ Quick Decision Flowchart

```mermaid
flowchart TD
    A[Check Errors] --> B{Train Error?}
    B -->|HIGH| C[Underfitting!]
    B -->|LOW| D{Test/Val Error?}
    C --> C1[Increase Degree]
    D -->|HIGH| E[Overfitting!]
    D -->|LOW| F[Good Fit! âœ“]
    E --> E1[Decrease Degree]
```

---

Good luck with your exams! ðŸŽ“âœ¨
