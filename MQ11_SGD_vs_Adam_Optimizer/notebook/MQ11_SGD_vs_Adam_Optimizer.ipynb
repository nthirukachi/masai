{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§© Problem Statement\n",
                "\n",
                "### 1. The Problem\n",
                "A regression model's loss is plateauing (stuck), and we want to see if switching from **SGD** to **Adam** helps it learn faster.\n",
                "\n",
                "### 2. Steps to Solve\n",
                "1.  Generate synthetic regression data.\n",
                "2.  Scale features and targets.\n",
                "3.  Build a 3-Layer MLP in PyTorch.\n",
                "4.  Train with SGD (Run A) and Adam (Run B).\n",
                "5.  Compare results.\n",
                "\n",
                "### 3. Expected Output\n",
                "Comparative plots show Adam converging faster."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Library Imports\n",
                "#### 2.1 What the line does\n",
                "Imports necessary libraries for Deep Learning (PyTorch), Data Handling (Pandas/Numpy), and Plotting (Matplotlib).\n",
                "#### 2.2 Why it is used\n",
                "To access pre-built functions so we don't have to write everything from scratch.\n",
                "#### 2.3 When to use it\n",
                "At the start of every script.\n",
                "#### 2.4 Where to use it\n",
                "Top of the file.\n",
                "#### 2.5 How to use it\n",
                "`import torch`\n",
                "#### 2.6 How it works internally\n",
                "Loads the compiled C++/CUDA code into Python memory.\n",
                "#### 2.7 Output with sample examples\n",
                "No visible output, but functions become available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.datasets import make_regression\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import copy\n",
                "import os\n",
                "\n",
                "os.makedirs('outputs', exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Reproducibility (Seeding)\n",
                "#### 2.1 What the line does\n",
                "Sets fixed seeds for random number generators.\n",
                "#### 2.2 Why it is used\n",
                "To ensure fair comparison; both runs must start with identical randomness.\n",
                "#### 2.3 When to use it\n",
                "In any scientific experiment involving randomness.\n",
                "#### 2.4 Where to use it\n",
                "Before any random operations.\n",
                "#### 2.5 How to use it\n",
                "`torch.manual_seed(42)`\n",
                "#### 2.6 How it works internally\n",
                "Initializes the pseudo-random state vector.\n",
                "#### 2.7 Output with sample examples\n",
                "Consistent random numbers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SEED = 42\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Data Preparation Function\n",
                "#### 2.1 What the line does\n",
                "Defines a function to generate, split, and scale data.\n",
                "#### 2.2 Why it is used\n",
                "To keep the main code clean and reusable.\n",
                "#### 2.3 When to use it\n",
                "When processing data steps are complex.\n",
                "#### 2.4 Where to use it\n",
                "Before the training loop.\n",
                "#### 2.5 How to use it\n",
                "`loaders, dim, scaler = prepare_data()`\n",
                "#### 2.6 How it works internally\n",
                "Calls sklearn functions and wraps result in PyTorch DataLoaders.\n",
                "#### 2.7 Output with sample examples\n",
                "Returns iterators for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_data():\n",
                "    X, y = make_regression(n_samples=2000, n_features=40, noise=15, random_state=SEED)\n",
                "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
                "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED)\n",
                "    \n",
                "    # Feature Scaling\n",
                "    scaler_x = StandardScaler()\n",
                "    X_train = scaler_x.fit_transform(X_train)\n",
                "    X_val = scaler_x.transform(X_val)\n",
                "    X_test = scaler_x.transform(X_test)\n",
                "    \n",
                "    # Target Scaling (CRITICAL)\n",
                "    scaler_y = StandardScaler()\n",
                "    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
                "    y_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
                "    y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
                "    \n",
                "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train).unsqueeze(1))\n",
                "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val).unsqueeze(1))\n",
                "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test).unsqueeze(1))\n",
                "    \n",
                "    loaders = {\n",
                "        'train': DataLoader(train_dataset, batch_size=64, shuffle=True),\n",
                "        'val': DataLoader(val_dataset, batch_size=64, shuffle=False),\n",
                "        'test': DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
                "    }\n",
                "    return loaders, X_train.shape[1], scaler_y"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Model Definition (MLP)\n",
                "#### 2.1 What the line does\n",
                "Defines the Neural Network architecture.\n",
                "#### 2.2 Why it is used\n",
                "To tell PyTorch how to process inputs.\n",
                "#### 2.3 When to use it\n",
                "For any Deep Learning task.\n",
                "#### 2.4 Where to use it\n",
                "Inside a class inheriting from `nn.Module`.\n",
                "#### 2.5 How to use it\n",
                "`model = MLP(input_dim)`\n",
                "#### 2.6 How it works internally\n",
                "Registers layers to the computational graph.\n",
                "#### 2.7 Output with sample examples\n",
                "A model object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MLP(nn.Module):\n",
                "    def __init__(self, input_dim):\n",
                "        super(MLP, self).__init__()\n",
                "        self.layers = nn.Sequential(\n",
                "            nn.Linear(input_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, 1)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.layers(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Training Loop\n",
                "#### 2.1 What the line does\n",
                "Orchestrates the learning process: Forward -> Loss -> Backward -> Step.\n",
                "#### 2.2 Why it is used\n",
                "To update weights and minimize error.\n",
                "#### 2.3 When to use it\n",
                "For training models.\n",
                "#### 2.4 Where to use it\n",
                "In the main logic.\n",
                "#### 2.5 How to use it\n",
                "Calls `optimizer.step()`.\n",
                "#### 2.6 How it works internally\n",
                "Calculates gradients via backpropagation and applies optimizer rules.\n",
                "#### 2.7 Output with sample examples\n",
                "Returns a history of losses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model_class, input_dim, optimizer_name, learning_rate, momentum=0.0):\n",
                "    torch.manual_seed(SEED)\n",
                "    model = model_class(input_dim)\n",
                "    criterion = nn.MSELoss()\n",
                "    \n",
                "    if optimizer_name == 'SGD':\n",
                "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
                "    else:\n",
                "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "        \n",
                "    history = {'train_loss': [], 'val_loss': [], 'val_rmse': []}\n",
                "    best_val_rmse = float('inf')\n",
                "    loaders, _, _ = prepare_data()\n",
                "    \n",
                "    for epoch in range(40):\n",
                "        model.train()\n",
                "        running_loss = 0.0\n",
                "        for X_batch, y_batch in loaders['train']:\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            running_loss += loss.item()\n",
                "        \n",
                "        avg_train_loss = running_loss / len(loaders['train'])\n",
                "        \n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        with torch.no_grad():\n",
                "            for X_val, y_val in loaders['val']:\n",
                "                outputs = model(X_val)\n",
                "                loss = criterion(outputs, y_val)\n",
                "                val_loss += loss.item()\n",
                "        \n",
                "        avg_val_loss = val_loss / len(loaders['val'])\n",
                "        val_rmse = np.sqrt(avg_val_loss)\n",
                "        \n",
                "        history['train_loss'].append(avg_train_loss)\n",
                "        history['val_loss'].append(avg_val_loss)\n",
                "        history['val_rmse'].append(val_rmse)\n",
                "        \n",
                "        if val_rmse < best_val_rmse:\n",
                "            best_val_rmse = val_rmse\n",
                "            \n",
                "    print(f\"Finished {optimizer_name}: Best Val RMSE: {best_val_rmse:.4f}\")\n",
                "    return history, best_val_rmse"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¹ Execution & Visualization\n",
                "#### 2.1 What the line does\n",
                "Runs the experiment for SGD and Adam, then plots results.\n",
                "#### 2.2 Why it is used\n",
                "To analyze the difference in performance.\n",
                "#### 2.3 When to use it\n",
                "At the end of the script.\n",
                "#### 2.4 Where to use it\n",
                "Main block.\n",
                "#### 2.5 How to use it\n",
                "Loops through configs and calls train.\n",
                "#### 2.6 How it works internally\n",
                "Calls the training function 2 times.\n",
                "#### 2.7 Output with sample examples\n",
                "Graphs and Tables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizers_to_run = [\n",
                "    {'name': 'SGD', 'lr': 5e-3, 'momentum': 0.9},\n",
                "    {'name': 'Adam', 'lr': 1e-3, 'momentum': 0.0}\n",
                "]\n",
                "\n",
                "results = {}\n",
                "loaders, input_dim, scaler_y = prepare_data()\n",
                "\n",
                "for opt_config in optimizers_to_run:\n",
                "    print(f\"Training with {opt_config['name']}...\")\n",
                "    hist, best_rmse = train_model(\n",
                "        MLP, \n",
                "        input_dim, \n",
                "        opt_config['name'], \n",
                "        opt_config['lr'], \n",
                "        opt_config['momentum']\n",
                "    )\n",
                "    results[opt_config['name']] = {'history': hist, 'best_rmse': best_rmse}\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(results['SGD']['history']['train_loss'], label='SGD')\n",
                "plt.plot(results['Adam']['history']['train_loss'], label='Adam')\n",
                "plt.title('Training Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(results['SGD']['history']['val_rmse'], label='SGD')\n",
                "plt.plot(results['Adam']['history']['val_rmse'], label='Adam')\n",
                "plt.title('Validation RMSE')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}