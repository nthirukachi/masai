# MLP Decision Boundaries: Comparing Activation Functions
## NotebookLM-Style Presentation

---

# Slide 1: Title & Objective

## ðŸ§  MLP Decision Boundaries
### Comparing Activation Functions on make_moons

**Objective**: Understand how different activation functions create different decision boundaries in neural networks.

**Key Questions**:
- How do ReLU, Sigmoid, and Tanh shape decision boundaries?
- Which activation works best for non-linear data?
- Why does the choice matter?

---

# Slide 2: Problem Statement

## ðŸ§© The Challenge

**Scenario**: Classify points in the "two moons" pattern

```
     Class 1 (Moon 1)
        .-"""-.
      .'       '.
     /           \
    ;             ;   <-- Non-linear boundary needed!
    |_____________|
         Class 0 (Moon 2)
```

**Challenge**: A straight line CANNOT separate these!

**Solution**: Use neural networks with different activation functions

---

# Slide 3: Real-World Use Case

## ðŸŒ Where This Applies

| Domain | Example |
|--------|---------|
| **Medical** | Classify tumors as benign/malignant |
| **Email** | Spam vs legitimate email |
| **Finance** | Fraud vs normal transactions |
| **Vision** | Cat vs dog in images |

**Key Insight**: Real data is rarely linearly separable!

---

# Slide 4: Input Data

## ðŸ“Š The make_moons Dataset

```python
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=300, noise=0.2, random_state=42)
```

| Parameter | Value | Purpose |
|-----------|-------|---------|
| n_samples | 300 | Total data points |
| noise | 0.2 | Adds realism |
| random_state | 42 | Reproducibility |

**Result**: 150 points per moon, 2D features

---

# Slide 5: Concepts Used

## ðŸ”‘ Key Concepts

1. **Neural Network (MLP)**
   - Learns complex patterns
   - Layers: Input â†’ Hidden â†’ Output

2. **Activation Functions**
   - ReLU, Sigmoid, Tanh
   - Transform neuron outputs non-linearly

3. **Decision Boundary**
   - Where prediction changes class
   - Visualizes how model "sees" data

---

# Slide 6: Activation Functions Breakdown

## âš¡ The Three Activations

| Activation | Formula | Range | Shape |
|------------|---------|-------|-------|
| **ReLU** | max(0, x) | [0, âˆž) | Angular |
| **Logistic** | 1/(1+e^-x) | (0, 1) | Smooth |
| **Tanh** | (e^x-e^-x)/(e^x+e^-x) | (-1, 1) | Smooth |

**Analogy**:
- ReLU = One-way valve
- Sigmoid = Dimmer switch
- Tanh = Centered dimmer

---

# Slide 7: Step-by-Step Solution

## ðŸªœ Solution Flow

```
Step 1: Generate Data
    â†“
Step 2: Create 3 MLP Models
    â”œâ”€â”€ Model 1: ReLU
    â”œâ”€â”€ Model 2: Logistic
    â””â”€â”€ Model 3: Tanh
    â†“
Step 3: Train All Models
    â†“
Step 4: Visualize Boundaries
    â†“
Step 5: Compare & Analyze
```

---

# Slide 8: Code Logic Summary

## ðŸ’» Key Code Components

```python
# 1. Data Generation
X, y = make_moons(n_samples=300, noise=0.2, random_state=42)

# 2. Model Creation
model = MLPClassifier(
    hidden_layer_sizes=(8,),    # 1 layer, 8 neurons
    activation='relu',           # or 'logistic', 'tanh'
    random_state=42
)

# 3. Training
model.fit(X, y)

# 4. Visualization
Z = model.predict(meshgrid_points)
plt.contourf(xx, yy, Z)
```

---

# Slide 9: Important Functions & Parameters

## âš™ï¸ Critical Parameters

### MLPClassifier
| Parameter | Our Value | Effect |
|-----------|-----------|--------|
| hidden_layer_sizes | (8,) | 1 layer, 8 neurons |
| activation | varies | Boundary shape |
| solver | 'adam' | Optimization |
| max_iter | 1000 | Training cycles |
| random_state | 42 | Fair comparison |

### make_moons
| Parameter | Value | Effect |
|-----------|-------|--------|
| n_samples | 300 | Dataset size |
| noise | 0.2 | Difficulty level |

---

# Slide 10: Execution Output

## ðŸ“ˆ Results

### Accuracy Comparison

| Activation | Accuracy | Rank |
|------------|----------|------|
| **ReLU** | 88.33% | ðŸ¥‡ |
| **Tanh** | 86.33% | ðŸ¥ˆ |
| **Logistic** | 85.67% | ðŸ¥‰ |

### Boundary Visualization
- ReLU: Angular, piecewise-linear edges
- Sigmoid: Smooth, curved boundary
- Tanh: Smooth but steeper transitions

---

# Slide 11: Observations & Insights

## ðŸ” Key Observations

1. **ReLU creates ANGULAR boundaries**
   - Due to piecewise-linear nature

2. **Sigmoid/Tanh create SMOOTH boundaries**
   - Due to continuous, curved functions

3. **All accuracies similar (~85-88%)**
   - Dataset is "easy" for 8-neuron network

4. **ReLU wins by small margin**
   - Advantages more visible in deep networks

---

# Slide 12: Advantages & Limitations

## âš–ï¸ Trade-offs

### ReLU
| âœ… Advantages | âŒ Limitations |
|--------------|----------------|
| No vanishing gradient | Dead neurons possible |
| Fast computation | Not zero-centered |
| Modern default | |

### Sigmoid/Tanh
| âœ… Advantages | âŒ Limitations |
|--------------|----------------|
| Bounded output | Vanishing gradient |
| Probability interpretation | Slower |
| Smooth gradients | |

---

# Slide 13: Interview Key Takeaways

## ðŸ’¼ Remember These!

1. **ReLU = max(0, x)** â†’ Default for hidden layers
2. **Sigmoid = 1/(1+e^-x)** â†’ Binary output layers
3. **Tanh** â†’ RNNs, zero-centered needed
4. **Vanishing gradient** â†’ Sigmoid/Tanh problem
5. **hidden_layer_sizes=(8,)** â†’ Tuple notation!
6. **random_state** â†’ For reproducibility

### Quick Answer Template
> "ReLU is preferred because it avoids vanishing gradients and is computationally efficient. Use Sigmoid for probability outputs."

---

# Slide 14: Conclusion

## ðŸŽ¯ Summary

### What We Learned
- Different activations create different boundary shapes
- ReLU: Angular, Sigmoid/Tanh: Smooth
- For simple data, differences are small

### Key Recommendations
| Scenario | Use |
|----------|-----|
| Hidden layers | ReLU |
| Binary output | Sigmoid |
| RNNs | Tanh |
| Deep networks | ReLU (avoid vanishing gradient) |

### Final Thought
> "Activation choice matters more in deep networks than shallow ones."

---

## ðŸ“š Resources

- Code: `src/mlp_decision_boundaries.py`
- Notebook: `notebook/mlp_decision_boundaries.ipynb`
- Documentation: `documentation/`
- Output: `outputs/decision_boundaries.png`
