{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perceptron From Scratch: A Complete Beginner's Guide\n",
        "\n",
        "### \ud83e\udde9 Problem Statement\n",
        "- **What problem is being solved?** We are predicting whether a student will **Pass** or **Fail** an exam based on two features: **Study Hours** and **Attendance Percentage**.\n",
        "- **Why it matters?** This is a classic **Binary Classification** problem. It helps us understand how machines can learn a simple \"Yes/No\" decision rule from data, which is the foundation of all Neural Networks and Deep Learning.\n",
        "- **Real-world relevance**: Similar logic is used in:\n",
        "    - **Bank Loans**: Approve (Yes) or Reject (No) based on Income and Debt.\n",
        "    - **Medical Diagnosis**: Sick (Yes) or Healthy (No) based on Symptoms.\n",
        "    - **Spam Detection**: Spam (Yes) or Not Spam (No) based on email words.\n",
        "\n",
        "### \ud83e\ude9c Steps to Solve the Problem\n",
        "1.  **Generate Data**: Create synthetic data for 100 students (Study Hours, Attendance) with known Pass/Fail labels.\n",
        "2.  **Build Perceptron**: Write the code for a Perceptron (a single neuron) from scratch.\n",
        "3.  **Train Model**: Feed the data to the Perceptron so it learns the weights (importance) of Study Hours and Attendance.\n",
        "4.  **Visualize**: Plot the \"Decision Boundary\" (the line that separates Pass from Fail).\n",
        "5.  **Test**: Predict outcomes for new students to see if the model works.\n",
        "\n",
        "### \ud83c\udfaf Expected Output (OVERALL)\n",
        "- **Learned Weights**: The model will tell us how much each feature contributes to the result.\n",
        "- **Accuracy**: A percentage score (e.g., 100%) indicating how many students were correctly classified.\n",
        "- **Plots**: \n",
        "    - A scatter plot showing students colored by Pass/Fail with a line separating them.\n",
        "    - A convergence plot showing how the model made fewer mistakes over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Libraries and Setup\n",
        "\n",
        "We need to import the necessary tools to built our project.\n",
        "\n",
        "### \ud83d\udd39 Line Explanation\n",
        "\n",
        "#### `import numpy as np`\n",
        "- **2.1 What the line does**: Imports the **NumPy** library and gives it the alias `np`.\n",
        "- **2.2 Why it is used**: NumPy is the standard library for numerical computing. It provides support for arrays (matrices), which are much faster and more efficient than Python lists for mathematical operations.\n",
        "- **2.3 When to use it**: Whenever you need to perform mathematical calculations on lists of numbers, matrix multiplication, or generate random numbers.\n",
        "- **2.4 Where to use it**: Used in almost every Data Science, Machine Learning, and Scientific Computing project.\n",
        "- **2.5 How to use it**: `import numpy as np` -> `arr = np.array([1, 2, 3])`.\n",
        "- **2.6 How it works internally**: NumPy arrays are stored in contiguous memory blocks (unlike Python lists which are arrays of pointers), allowing the CPU to process them very quickly (Vectorization).\n",
        "- **2.7 Output**: Nothing is printed, but the `np` module is loaded into memory.\n",
        "\n",
        "#### `import matplotlib.pyplot as plt`\n",
        "- **2.1 What the line does**: Imports the `pyplot` module from the **Matplotlib** library and gives it the alias `plt`.\n",
        "- **2.2 Why it is used**: It is the most popular library for creating visualizations (charts, graphs, plots) in Python.\n",
        "- **2.3 When to use it**: When you need to visualize data, show trends, or plot results (like our decision boundary).\n",
        "- **2.4 Where to use it**: Data Analysis reports, research papers, dashboard prototyping.\n",
        "- **2.5 How to use it**: `plt.plot(x, y)`, `plt.show()`.\n",
        "- **2.6 How it works internally**: It creates figure objects and axes objects that render lines, shapes, and text onto an image backend.\n",
        "- **2.7 Output**: `plt` module loaded.\n",
        "\n",
        "#### `np.random.seed(42)`\n",
        "- **2.1 What the line does**: Sets the \"seed\" for the random number generator to the number 42.\n",
        "- **2.2 Why it is used**: Random numbers are not truly random; they are generated by a predictable algorithm starting from a seed. Setting the seed ensures that **every time we run this code, we get the exact same random numbers**. This makes our results **reproducible**.\n",
        "- **2.3 When to use it**: In experiments, debugging, and tutorials where you want consistent results.\n",
        "- **2.4 Where to use it**: Research papers, ML model initialization, splitting data.\n",
        "- **2.5 How to use it**: `np.random.seed(integer)`.\n",
        "- **2.6 How it works internally**: Initializes the internal state of the Mersenne Twister algorithm (or similar PRNG) to a specific value.\n",
        "- **2.7 Output**: No visible output, but the random sequence is fixed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udccc Sample Example\n",
        "No visible output is expected from this cell. \n",
        "\n",
        "### \ud83d\udcca Expected Output (Internal State)\n",
        "- `numpy` is loaded as `np`.\n",
        "- `matplotlib.pyplot` is loaded as `plt`.\n",
        "- Random seed is fixed to 42."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Generation\n",
        "\n",
        "We need data to train our model. Since we don't have a real file, we will \"fake\" strict but realistic student data.\n",
        "\n",
        "### \u2699\ufe0f Function Arguments Explanation: `generate_data`\n",
        "\n",
        "#### `n_samples`\n",
        "- **3.1 What it does**: Specifies how many student records to generate.\n",
        "- **3.2 Why it is used**: To control the size of our dataset. Tests with small data (e.g., 10) are easy to read; learning needs more (e.g., 100).\n",
        "- **3.3 When to use it**: When creating synthetic datasets.\n",
        "- **3.4 Where to use it**: Unit testing data pipelines, teaching examples.\n",
        "- **3.5 How to use it**: `generate_data(n_samples=50)`.\n",
        "- **3.6 How it affects execution internally**: Determines the length of the arrays created by `np.random`.\n",
        "- **3.7 Output impact**: If 100, we get 100 rows of data. If 1000, we get 1000 rows.\n",
        "\n",
        "### \ud83d\udd39 Line Explanation\n",
        "\n",
        "#### `study_hours = np.random.randint(0, 100, n_samples)`\n",
        "- **2.1 What the line does**: Generates `n_samples` random integers between 0 (inclusive) and 100 (exclusive) representing study hours.\n",
        "- **2.2 Why it is used**: simulate variety in student behavior.\n",
        "- **2.5 How to use it**: `np.random.randint(low, high, size)`.\n",
        "- **2.7 Output**: An array like `[42, 12, 99, ...]`.\n",
        "\n",
        "#### `labels = ((study_hours + 0.5 * attendance) > 75).astype(int)`\n",
        "- **2.1 What the line does**: Defines the \"Ground Truth\". A student passes (1) ONLY IF their score (Hours + 0.5 * Attendance) is strictly greater than 75.\n",
        "- **2.2 Why it is used**: We need a target label for the computer to try and learn.\n",
        "- **2.6 How it works internally**: Creates a boolean array `[True, False...]` then converts `True` -> `1`, `False` -> `0`.\n",
        "\n",
        "#### `X = np.column_stack((study_hours, attendance))`\n",
        "- **2.1 What the line does**: Stacks the two 1D arrays (Vectors) side-by-side to create a 2D matrix (Table).\n",
        "- **2.2 Why it is used**: Scikit-learn and most ML models expect input `X` to be a matrix of shape `(samples, features)`.\n",
        "- **2.6 How it works internally**: Takes `[h1, h2]` and `[a1, a2]` and makes `[[h1, a1], [h2, a2]]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data(n_samples=100):\n",
        "    \"\"\"\n",
        "    Generates synthetic dataset for student pass/fail classification.\n",
        "    \"\"\"\n",
        "    # Random study hours between 0 and 100\n",
        "    study_hours = np.random.randint(0, 100, n_samples)\n",
        "    # Random attendance between 40 and 100\n",
        "    attendance = np.random.randint(40, 100, n_samples)\n",
        "    \n",
        "    # Create labels: pass (1) if study_hours + 0.5*attendance > 75, else fail (0)\n",
        "    # This is the \"Secret Rule\" the perceptron must learn\n",
        "    labels = ((study_hours + 0.5 * attendance) > 75).astype(int)\n",
        "    \n",
        "    # Combine features into a single matrix (X)\n",
        "    X = np.column_stack((study_hours, attendance))\n",
        "    y = labels\n",
        "    \n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udccc Sample Example\n",
        "```python\n",
        "X, y = generate_data(3)\n",
        "# X might look like:\n",
        "# array([[80, 90],  -> Score: 80 + 45 = 125 (>75) -> Pass\n",
        "#        [20, 50],  -> Score: 20 + 25 = 45  (<75) -> Fail\n",
        "#        [60, 80]]) -> Score: 60 + 40 = 100 (>75) -> Pass\n",
        "# y: array([1, 0, 1])\n",
        "```\n",
        "### \ud83d\udcca Expected Output\n",
        "- `generate_data` function is defined and ready to be called.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The Perceptron Class\n",
        "\n",
        "This is the core of our project. We will build a single artificial neuron.\n",
        "\n",
        "### \u2699\ufe0f Function Arguments Explanation: `__init__`\n",
        "\n",
        "#### `learning_rate` (default=0.01)\n",
        "- **3.1 What it does**: Controls how big of a step the model takes when adjusting weights.\n",
        "- **3.2 Why it is used**: If too big, learning is unstable. If too small, learning is too slow.\n",
        "- **3.7 Output impact**: High rate = fast but jerky convergence. Low rate = slow but smooth.\n",
        "\n",
        "#### `epochs` (default=100)\n",
        "- **3.1 What it does**: How many times the model sees the *entire* dataset.\n",
        "- **3.2 Why it is used**: One look isn't enough to learn. Repetition creates memory.\n",
        "\n",
        "### \ud83d\udd39 Line Explanation\n",
        "\n",
        "#### `np.dot(x_i, self.weights) + self.bias`\n",
        "- **2.1 What the line does**: The fundamental formula of Linear Machine Learning: $y = w \\cdot x + b$.\n",
        "- **2.2 Why it is used**: Combines inputs and importance (weights) into a single score.\n",
        "- **2.6 How it works internally**: Multiples each feature by its weight and sums them up.\n",
        "\n",
        "#### `update = self.learning_rate * (y[fn_idx] - y_predicted)`\n",
        "- **2.1 What the line does**: Calculates the correction amount.\n",
        "- **2.2 Why it is used**: This is the \"Learning Rule\".\n",
        "- **2.6 How it works internally**:\n",
        "    - If prediction is correct ($y - \\hat{y} = 0$), update is 0 (Don't change anything).\n",
        "    - If prediction is wrong, update moves weights towards the correct answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, epochs=100):\n",
        "        \"\"\"\n",
        "        Initializes the Perceptron model.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.errors_ = []\n",
        "\n",
        "    def step_function(self, z):\n",
        "        \"\"\"\n",
        "        Step activation function: returns 1 if z >= 0, else 0.\n",
        "        \"\"\"\n",
        "        return np.where(z >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the Perceptron on the provided data.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        self.errors_ = []\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            errors_in_epoch = 0\n",
        "            for fn_idx, x_i in enumerate(X):\n",
        "                # Calculate linear output\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                # Apply activation function\n",
        "                y_predicted = self.step_function(linear_output)\n",
        "                \n",
        "                # Calculate update\n",
        "                # (Target - Predicted) is the error term\n",
        "                update = self.learning_rate * (y[fn_idx] - y_predicted)\n",
        "                \n",
        "                # Update weights and bias\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "                \n",
        "                # Count errors (if update is non-zero, it means there was an error)\n",
        "                if update != 0:\n",
        "                    errors_in_epoch += 1\n",
        "            \n",
        "            self.errors_.append(errors_in_epoch)\n",
        "            \n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts class labels for new data.\n",
        "        \"\"\"\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.step_function(linear_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udccc Sample Example\n",
        "```python\n",
        "p = Perceptron()\n",
        "# p.fit(X, y) will teach the model\n",
        "```\n",
        "### \ud83d\udcca Expected Output\n",
        "- The `Perceptron` class is defined. No output yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization Functions\n",
        "\n",
        "We need to see what's happening inside the \"Black Box\".\n",
        "\n",
        "### \ud83d\udd39 Line Explanation\n",
        "\n",
        "#### `x2_values = -(classifier.weights[0] * x1_values + classifier.bias) / classifier.weights[1]`\n",
        "- **2.1 What the line does**: This is the algebra to rearrange $w_1 x_1 + w_2 x_2 + b = 0$ into $x_2 = mx_1 + c$ form.\n",
        "- **2.2 Why it is used**: Matplotlib plots lines using $y$ vs $x$ (or $x_2$ vs $x_1$). We need $x_2$ on one side to plot the boundary line.\n",
        "- **2.5 How to use it**: Standard algebraic manipulation code.\n",
        "\n",
        "#### `plt.savefig(...)`\n",
        "- **2.1 What the line does**: Saves the current plot image to the hard drive.\n",
        "- **2.2 Why it is used**: So we have a permanent record of the result in our `outputs/` folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_decision_boundary(X, y, classifier, title=\"Perceptron Decision Boundary\"):\n",
        "    \"\"\"\n",
        "    Plots the decision boundary and data points.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot data points\n",
        "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', marker='o', label='Fail (0)')\n",
        "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', marker='x', label='Pass (1)')\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    # w1*x1 + w2*x2 + b = 0  =>  x2 = -(w1*x1 + b) / w2\n",
        "    x1_min, x1_max = X[:, 0].min() - 5, X[:, 0].max() + 5\n",
        "    x1_values = np.linspace(x1_min, x1_max, 100)\n",
        "    \n",
        "    if classifier.weights[1] != 0:\n",
        "        x2_values = -(classifier.weights[0] * x1_values + classifier.bias) / classifier.weights[1]\n",
        "        plt.plot(x1_values, x2_values, 'k--', label='Decision Boundary')\n",
        "    \n",
        "    plt.xlabel('Study Hours')\n",
        "    plt.ylabel('Attendance %')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(title)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    \n",
        "    # Save the plot\n",
        "    plt.savefig('c:/masai/Perceptron_From_Scratch/outputs/decision_boundary.png')\n",
        "    # Show the plot in notebook\n",
        "    plt.show()\n",
        "\n",
        "def plot_convergence(errors):\n",
        "    \"\"\"\n",
        "    Plots the number of errors vs epochs.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Number of Updates (Errors)')\n",
        "    plt.title('Convergence Analysis')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('c:/masai/Perceptron_From_Scratch/outputs/convergence_plot.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Expected Output\n",
        "- Visual functions defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Running the Experiment\n",
        "\n",
        "Now we put everything together.\n",
        "\n",
        "### \ud83e\ude9c Execution Steps\n",
        "1.  **Generate** 100 fake students.\n",
        "2.  **Initialize** the Perceptron.\n",
        "3.  **Fit** (Train) the model on the students.\n",
        "4.  **Visualize** the result.\n",
        "5.  **Test** on specific examples (Student A, B, C).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Generating data...\")\n",
        "    X, y = generate_data(n_samples=100)\n",
        "    print(f\"Data generated. Feature shape: {X.shape}, Label shape: {y.shape}\")\n",
        "    \n",
        "    print(\"\\nTraining Perceptron...\")\n",
        "    model = Perceptron(learning_rate=0.01, epochs=100)\n",
        "    model.fit(X, y)\n",
        "    print(\"Training complete.\")\n",
        "    print(f\"Learned Weights: {model.weights}\")\n",
        "    print(f\"Learned Bias: {model.bias}\")\n",
        "    \n",
        "    print(\"\\nEvaluating on Training Data...\")\n",
        "    predictions = model.predict(X)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    print(f\"Training Accuracy: {accuracy * 100:.2f}%\")\n",
        "    \n",
        "    print(\"\\nGenerating Visualizations...\")\n",
        "    plot_decision_boundary(X, y, model)\n",
        "    plot_convergence(model.errors_)\n",
        "    print(\"Plots saved to outputs/ directory.\")\n",
        "    \n",
        "    print(\"\\nTesting on specific students:\")\n",
        "    # Student A: 80 study hours, 90% attendance\n",
        "    # Student B: 30 study hours, 60% attendance\n",
        "    # Student C: 50 study hours, 85% attendance\n",
        "    test_students = np.array([\n",
        "        [80, 90],\n",
        "        [30, 60],\n",
        "        [50, 85]\n",
        "    ])\n",
        "    \n",
        "    student_names = ['Student A', 'Student B', 'Student C']\n",
        "    test_preds = model.predict(test_students)\n",
        "    \n",
        "    for name, features, pred in zip(student_names, test_students, test_preds):\n",
        "        status = \"Pass\" if pred == 1 else \"Fail\"\n",
        "        print(f\"{name} (Hours: {features[0]}, Attendance: {features[1]}%): Predicted -> {status}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcca Expected Output\n",
        "You should see:\n",
        "1.  Text confirming data generation.\n",
        "2.  Training accuracy report (likely 100% since our rule is simple).\n",
        "3.  **Two Plots** appearing below the code.\n",
        "4.  Predictions for Students A, B, and C.\n",
        "\n",
        "### \ud83d\udcbc Interview Perspective\n",
        "- **Q:** How do you know the model converged?\n",
        "- **A:** Look at the Convergence Plot; if the errors drop to zero and stay there, it converged.\n",
        "- **Q:** Why did we use specific learning rate 0.01?\n",
        "- **A:** It is a standard starting point. If training was unstable, we would lower it.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}