# Original Problem: ReLU Activation Function

## Question 14: Compare Activation Functions Mathematically and Visually [CODING]

Implement and analyze the three major activation functions: Sigmoid, Tanh, and ReLU.

### Tasks:

1. **Implement the following functions from scratch (no built-in activation functions):**
   - sigmoid(z) and sigmoid_derivative(z)
   - tanh(z) and tanh_derivative(z)
   - relu(z) and relu_derivative(z)

2. **Create comprehensive visualizations:**
   - Plot all three activation functions on the same graph for input range [-6, 6]
   - Plot their derivatives on a separate graph
   - Create a side-by-side comparison showing both

3. **Numerical analysis:**
   - Create a table showing the output of each activation for inputs: [-5, -2, -0.5, 0, 0.5, 2, 5]
   - For each function, identify the input range where gradients are strongest (> 0.1)
   - Calculate and display the gradient values at x = -2, 0, 2 for each function

4. **Written analysis (in comments or markdown):**
   - Explain the vanishing gradient problem based on your derivative plots
   - Describe situations where each activation function would be preferred
   - Identify the saturation regions for sigmoid and tanh

---

## This Project: RELU (RECTIFIED LINEAR UNIT) ACTIVATION FUNCTION

This project focuses specifically on the **ReLU** activation function implementation and analysis.
