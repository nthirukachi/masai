{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ReLU Activation Function - From Scratch\n",
                "\n",
                "## Question 14: Compare Activation Functions Mathematically and Visually\n",
                "\n",
                "---\n",
                "\n",
                "### ðŸ§© Problem Statement\n",
                "\n",
                "**What problem is being solved?**\n",
                "- Sigmoid/Tanh suffer from vanishing gradient\n",
                "- Deep networks (10+ layers) couldn't train effectively\n",
                "- ReLU solves this with gradient = 1 for all positive inputs\n",
                "\n",
                "**Key Formula:** f(z) = max(0, z)\n",
                "\n",
                "**Revolutionary Insight:** Gradient never shrinks for positive inputs!\n",
                "\n",
                "**Trade-off:** Dead neurons (gradient = 0 when z <= 0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Implement ReLU Function\n",
                "\n",
                "### ðŸ”¹ Line Explanation: `return np.maximum(0, z)`\n",
                "\n",
                "#### 2.1 What the line does\n",
                "Returns max(0, z) - zero for negatives, z itself for positives.\n",
                "\n",
                "#### 2.2 Why it is used\n",
                "Simplest non-linear activation that completely eliminates vanishing gradient for positive inputs. Gradient is EXACTLY 1 for all z > 0.\n",
                "\n",
                "#### 2.3 When to use it\n",
                "Default choice for hidden layers in modern deep networks.\n",
                "\n",
                "#### 2.4 Where to use it\n",
                "CNNs, Transformers, ResNets, virtually all modern architectures.\n",
                "\n",
                "#### 2.5-2.7 Usage\n",
                "```python\n",
                "relu(-5)  # Returns 0 (blocked)\n",
                "relu(5)   # Returns 5 (passes through)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def relu(z):\n",
                "    \"\"\"\n",
                "    ReLU activation function.\n",
                "    Formula: f(z) = max(0, z)\n",
                "    Output: 0 for negative, z for positive\n",
                "    \"\"\"\n",
                "    return np.maximum(0, z)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test ReLU\n",
                "print(\"relu(-5) =\", relu(-5))    # Expected: 0\n",
                "print(\"relu(0) =\", relu(0))      # Expected: 0\n",
                "print(\"relu(5) =\", relu(5))      # Expected: 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Implement ReLU Derivative\n",
                "\n",
                "### ðŸ”¹ Formula: f'(z) = 1 if z > 0, else 0\n",
                "\n",
                "**Key insight:** Gradient is EXACTLY 1 for ALL positive inputs - never decays!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def relu_derivative(z):\n",
                "    \"\"\"\n",
                "    Derivative of ReLU function.\n",
                "    Formula: f'(z) = 1 if z > 0, else 0\n",
                "    Key: Gradient is 1 for ALL positive inputs!\n",
                "    \"\"\"\n",
                "    return np.where(z > 0, 1, 0).astype(float)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test derivative\n",
                "print(\"relu_derivative(-5) =\", relu_derivative(-5))  # Expected: 0 (dead)\n",
                "print(\"relu_derivative(0) =\", relu_derivative(0))    # Expected: 0\n",
                "print(\"relu_derivative(5) =\", relu_derivative(5))    # Expected: 1 (perfect!)\n",
                "print(\"relu_derivative(100) =\", relu_derivative(100)) # Expected: 1 (still 1!)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "z_range = np.linspace(-6, 6, 200)\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# ReLU function\n",
                "ax1.plot(z_range, relu(z_range), 'r-', linewidth=2, label='ReLU')\n",
                "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
                "ax1.set_xlabel('Input (z)')\n",
                "ax1.set_ylabel('Output')\n",
                "ax1.set_title('ReLU Function: max(0, z)')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "ax1.set_ylim(-1, 7)\n",
                "\n",
                "# Derivative\n",
                "ax2.plot(z_range, relu_derivative(z_range), 'b-', linewidth=2, label='Derivative')\n",
                "ax2.axhline(y=1.0, color='green', linestyle=':', alpha=0.7, label='Gradient=1')\n",
                "ax2.axhline(y=0, color='red', linestyle=':', alpha=0.7, label='Dead zone')\n",
                "ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
                "ax2.set_xlabel('Input (z)')\n",
                "ax2.set_ylabel('Gradient')\n",
                "ax2.set_title('ReLU Derivative: 1 for positive, 0 for negative')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('outputs/relu_combined.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 5: Numerical Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_inputs = np.array([-5, -2, -0.5, 0, 0.5, 2, 5])\n",
                "\n",
                "print(\"RELU NUMERICAL ANALYSIS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"{'Input':<10} {'ReLU':<15} {'Derivative':<15}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for z in test_inputs:\n",
                "    print(f\"{z:<10.1f} {relu(z):<15.1f} {relu_derivative(z):<15.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Why ReLU Revolutionized Deep Learning\n",
                "\n",
                "### Gradient Comparison at z=5\n",
                "\n",
                "| Function | Gradient at z=5 | Status |\n",
                "|----------|-----------------|--------|\n",
                "| Sigmoid | 0.0066 | Vanishing! |\n",
                "| Tanh | 0.0002 | Vanishing! |\n",
                "| **ReLU** | **1.0** | **Perfect!** |\n",
                "\n",
                "ReLU enabled training of 100+ layer networks that were impossible before!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ’¼ Interview Key Points\n",
                "\n",
                "1. **Formula**: max(0, z) - simplest activation\n",
                "2. **Gradient = 1** for ALL positive inputs (no decay!)\n",
                "3. **Dead neurons**: gradient = 0 for negative inputs\n",
                "4. **Fix dead neurons**: Use LeakyReLU or He initialization\n",
                "5. **Use case**: Default for hidden layers in deep networks"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}