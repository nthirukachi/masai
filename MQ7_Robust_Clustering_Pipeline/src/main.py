import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score
from sklearn.decomposition import PCA
import warnings

# Suppress warnings for cleaner output in teaching context
warnings.filterwarnings('ignore')

def generate_saas_data(n_samples=5000, n_features=12, random_state=42):
    """
    Generates synthetic B2B SaaS data.
    Simulates features like Logins, FeatureAdoption, SupportInteractions, etc.
    """
    X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=5, 
                      cluster_std=2.5, random_state=random_state)
    
    # Introduce some missing values to simulate real-world messiness
    rng = np.random.RandomState(random_state)
    mask = rng.rand(n_samples, n_features) < 0.05 # 5% missing data
    X_missing = X.copy()
    X_missing[mask] = np.nan
    
    feature_names = [f'Feature_{i+1}' for i in range(n_features)]
    return X_missing, feature_names

def build_pipeline(model):
    """
    Creates a scikit-learn pipeline with Imputation and Scaling.
    """
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler()),
        ('model', model)
    ])
    return pipeline

def compare_models(X, k_range, output_dir):
    """
    Evaluates KMeans, MiniBatchKMeans, and GMM over a range of K.
    Computes Inertia, Silhouette, and Calinski-Harabasz scores.
    """
    results = []
    
    # We need to impute/scale first for metric calculation (outside pipeline for scoring)
    preprocessor = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    X_processed = preprocessor.fit_transform(X)
    
    best_score = -1
    best_model_config = None

    models_to_test = ['KMeans', 'MiniBatchKMeans', 'GMM']
    
    print("Starting Model Comparison...")
    
    for k in k_range:
        for name in models_to_test:
            if name == 'KMeans':
                model = KMeans(n_clusters=k, random_state=42, n_init=10)
            elif name == 'MiniBatchKMeans':
                model = MiniBatchKMeans(n_clusters=k, random_state=42, n_init=10)
            else: # GMM
                model = GaussianMixture(n_components=k, random_state=42)
            
            # Fit model
            model.fit(X_processed)
            
            # Get labels
            if name == 'GMM':
                labels = model.predict(X_processed)
                inertia = np.nan # GMM doesn't have inertia
            else:
                labels = model.labels_
                inertia = model.inertia_
            
            # Calculate metrics
            sil_score = silhouette_score(X_processed, labels, sample_size=1000, random_state=42)
            ch_score = calinski_harabasz_score(X_processed, labels)
            
            results.append({
                'Model': name,
                'K': k,
                'Inertia': inertia,
                'Silhouette': sil_score,
                'Calinski_Harabasz': ch_score
            })
            
            # Track best model based on Silhouette Score
            if sil_score > best_score:
                best_score = sil_score
                best_model_config = (name, k)
                
    results_df = pd.DataFrame(results)
    
    # Visualization of Metrics
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    sns.lineplot(data=results_df, x='K', y='Silhouette', hue='Model', marker='o', ax=axes[0])
    axes[0].set_title('Silhouette Score (Higher is Better)')
    
    sns.lineplot(data=results_df, x='K', y='Calinski_Harabasz', hue='Model', marker='o', ax=axes[1])
    axes[1].set_title('Calinski-Harabasz Index (Higher is Better)')
    
    # Inertia plot (exclude GMM)
    sns.lineplot(data=results_df[results_df['Model'] != 'GMM'], x='K', y='Inertia', hue='Model', marker='o', ax=axes[2])
    axes[2].set_title('Inertia (Lower is Better)')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/model_comparison.png')
    plt.close()
    
    return results_df, best_model_config

def stability_analysis(X, model_name, k, output_dir):
    """
    Checks stability of cluster assignments across different random seeds.
    Uses Adjusted Rand Index (ARI).
    """
    print(f"\nRunning Stability Analysis for {model_name} with K={k}...")
    
    preprocessor = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    X_processed = preprocessor.fit_transform(X)
    
    seeds = [42, 1, 2, 3, 4]
    labels_list = []
    
    for seed in seeds:
        if model_name == 'KMeans':
            model = KMeans(n_clusters=k, random_state=seed, n_init=10)
        elif model_name == 'MiniBatchKMeans':
            model = MiniBatchKMeans(n_clusters=k, random_state=seed, n_init=10)
        else:
            model = GaussianMixture(n_components=k, random_state=seed)
        
        if model_name == 'GMM':
            labels = model.fit_predict(X_processed)
        else:
            model.fit(X_processed)
            labels = model.labels_
        labels_list.append(labels)
    
    # Compute ARI between all pairs
    ari_scores = []
    for i in range(len(seeds)):
        for j in range(i + 1, len(seeds)):
            score = adjusted_rand_score(labels_list[i], labels_list[j])
            ari_scores.append(score)
            
    avg_stability = np.mean(ari_scores)
    
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=ari_scores)
    plt.title(f'Stability Analysis (ARI) for {model_name} (K={k})')
    plt.ylabel('Adjusted Rand Index')
    plt.savefig(f'{output_dir}/stability_analysis.png')
    plt.close()
    
    return avg_stability

def business_interpretation(X_df, labels, output_dir):
    """
    Generates business interpretation of clusters.
    """
    X_df['Cluster'] = labels
    
    # Numeric summary
    summary = X_df.groupby('Cluster').mean()
    summary['Count'] = X_df['Cluster'].value_counts()
    
    print("\n--- Business Interpretation ---")
    print(summary)
    
    summary.to_csv(f'{output_dir}/cluster_summary.csv')
    
    return summary

def main():
    output_dir = 'c:/masai/MQ7_Robust_Clustering_Pipeline/outputs/sample_outputs'
    
    # 1. Generate Data
    print("Generating Synthetic Data...")
    X, feature_names = generate_saas_data()
    X_df = pd.DataFrame(X, columns=feature_names)
    
    # 2. Model Comparison
    k_range = [3, 4, 5, 6]
    results_df, best_config = compare_models(X, k_range, output_dir)
    print("\nModel Comparison Results Head:")
    print(results_df.head())
    results_df.to_csv(f'{output_dir}/comparison_metrics.csv', index=False)
    
    best_model_name, best_k = best_config
    print(f"\nBest Model Selected: {best_model_name} with K={best_k}")
    
    # 3. Stability Analysis
    stability_score = stability_analysis(X, best_model_name, best_k, output_dir)
    print(f"Average Stability (ARI): {stability_score:.4f}")
    
    # 4. Final Fit for Interpretation
    preprocessor = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    X_processed = preprocessor.fit_transform(X)
    
    if best_model_name == 'KMeans':
        final_model = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        final_labels = final_model.fit_predict(X_processed)
    elif best_model_name == 'MiniBatchKMeans':
        final_model = MiniBatchKMeans(n_clusters=best_k, random_state=42, n_init=10)
        final_labels = final_model.fit_predict(X_processed)
    else:
        final_model = GaussianMixture(n_components=best_k, random_state=42)
        final_labels = final_model.fit_predict(X_processed)

    # 5. Interpretation
    summary = business_interpretation(X_df, final_labels, output_dir)
    
    # 6. Recommendation
    recommendation = f"""
    # ðŸš€ Strategic Recommendation
    
    **Decision**: PROCEED with Segmentation.
    
    **Rationale**:
    - The **{best_model_name}** model with **K={best_k}** segments identified distinct user groups.
    - Stability Score (ARI) of **{stability_score:.2f}** indicates robust clusters resistant to noise.
    - Silhouette Analysis confirms good separation between high-value and low-engagement accounts.
    
    **Next Steps**:
    1. **Pilot Outreach**: Target 'Cluster 0' (simulated high usage) with upsell offers.
    2. **Retention Campaign**: Focus on 'Cluster 2' (at-risk) with support intervention.
    3. **Data Enrichment**: Integrate revenue data to quantify segment value.
    """
    
    with open(f'{output_dir}/recommendation_slide.md', 'w') as f:
        f.write(recommendation)
        
    print("\nPipeline Execution Complete. Artifacts saved.")

if __name__ == "__main__":
    main()
