# ğŸ§  Sigmoid vs ReLU Activation Comparison

## ğŸ“‹ Project Overview

This teaching project compares **Sigmoid (Logistic)** and **ReLU** activation functions in a shallow neural network (MLP) trained on the **make_moons** dataset.

### ğŸ¯ What You'll Learn

1. **Activation Functions** - How different neurons "activate" or "turn on"
2. **Convergence Speed** - How fast each network learns the pattern
3. **Decision Boundaries** - How each network draws lines to separate classes
4. **Gradient Behavior** - Why ReLU often trains faster than Sigmoid

---

## ğŸ“ Project Structure

```
Sigmoid_vs_ReLU_Activation/
â”‚
â”œâ”€â”€ ğŸ“ notebook/
â”‚   â””â”€â”€ sigmoid_vs_relu.ipynb          # Teaching notebook
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â””â”€â”€ sigmoid_vs_relu.py             # Python implementation
â”‚
â”œâ”€â”€ ğŸ“ documentation/
â”‚   â”œâ”€â”€ Original_Problem.md            # Exact problem statement
â”‚   â”œâ”€â”€ problem_statement.md           # Simplified explanation
â”‚   â”œâ”€â”€ concepts_explained.md          # Deep dive into concepts
â”‚   â”œâ”€â”€ observations_and_conclusion.md # Results analysis
â”‚   â”œâ”€â”€ interview_questions.md         # Q&A for interviews
â”‚   â”œâ”€â”€ exam_preparation.md            # MCQ/MSQ/Numerical
â”‚   â””â”€â”€ interview_preparation.md       # Quick revision
â”‚
â”œâ”€â”€ ğŸ“ slides/
â”‚   â””â”€â”€ slides.md                      # NotebookLM-style presentation
â”‚
â”œâ”€â”€ ğŸ“ outputs/
â”‚   â”œâ”€â”€ loss_curves.png                # Combined loss plot
â”‚   â”œâ”€â”€ confusion_matrices.png         # Both confusion matrices
â”‚   â””â”€â”€ metrics_table.md               # Accuracy comparison
â”‚
â””â”€â”€ README.md                          # This file
```

---

## ğŸš€ How to Run

### Using UV (Recommended)
```powershell
cd c:\masai\Sigmoid_vs_ReLU_Activation
uv run python src/sigmoid_vs_relu.py
```

### Using Regular Python
```powershell
cd c:\masai\Sigmoid_vs_ReLU_Activation
python src/sigmoid_vs_relu.py
```

---

## ğŸ“Š Key Findings

| Metric | Sigmoid (Logistic) | ReLU |
|--------|-------------------|------|
| Final Accuracy | TBD | TBD |
| Convergence Speed | Slower | Faster |
| Gradient Vanishing | Yes (common) | No |
| Training Iterations | â‰¤300 | â‰¤300 |

---

## ğŸ“ Prerequisites

- Python 3.8+
- scikit-learn
- matplotlib
- numpy

---

## ğŸ‘¨â€ğŸ« For Beginners

Think of activation functions like **light switches**:
- **Sigmoid**: Like a dimmer switch - smoothly goes from OFF to ON
- **ReLU**: Like a regular switch - OFF until a point, then fully ON

This project helps you understand which "switch" works better for different problems!
