{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Activation Functions Comparison\n",
                "\n",
                "## Question 14: Compare Activation Functions Mathematically and Visually\n",
                "\n",
                "---\n",
                "\n",
                "### ðŸ§© Problem Statement\n",
                "\n",
                "Compare Sigmoid, Tanh, and ReLU side-by-side to understand:\n",
                "- When to use each\n",
                "- Vanishing gradient problem\n",
                "- Trade-offs between them\n",
                "\n",
                "### Summary Table\n",
                "\n",
                "| Function | Formula | Max Gradient | Use Case |\n",
                "|----------|---------|--------------|----------|\n",
                "| Sigmoid | 1/(1+e^-z) | 0.25 | Binary output |\n",
                "| Tanh | (e^z-e^-z)/(e^z+e^-z) | 1.0 | RNNs |\n",
                "| ReLU | max(0, z) | 1.0 always | Hidden layers |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 1: Import and Define All Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Sigmoid\n",
                "def sigmoid(z):\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "def sigmoid_derivative(z):\n",
                "    s = sigmoid(z)\n",
                "    return s * (1 - s)\n",
                "\n",
                "# Tanh\n",
                "def tanh(z):\n",
                "    exp_z = np.exp(z)\n",
                "    exp_neg_z = np.exp(-z)\n",
                "    return (exp_z - exp_neg_z) / (exp_z + exp_neg_z)\n",
                "\n",
                "def tanh_derivative(z):\n",
                "    t = tanh(z)\n",
                "    return 1 - t ** 2\n",
                "\n",
                "# ReLU\n",
                "def relu(z):\n",
                "    return np.maximum(0, z)\n",
                "\n",
                "def relu_derivative(z):\n",
                "    return np.where(z > 0, 1, 0).astype(float)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 2: Plot All Functions on Same Graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "z_range = np.linspace(-6, 6, 200)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(z_range, sigmoid(z_range), 'b-', linewidth=2, label='Sigmoid')\n",
                "plt.plot(z_range, tanh(z_range), 'g-', linewidth=2, label='Tanh')\n",
                "plt.plot(z_range, relu(z_range), 'r-', linewidth=2, label='ReLU')\n",
                "\n",
                "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
                "\n",
                "plt.xlabel('Input (z)', fontsize=12)\n",
                "plt.ylabel('Output', fontsize=12)\n",
                "plt.title('Comparison of Activation Functions', fontsize=14)\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.ylim(-1.5, 6)\n",
                "\n",
                "plt.savefig('outputs/all_activations.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 3: Plot All Derivatives (Gradients)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(z_range, sigmoid_derivative(z_range), 'b-', linewidth=2, label=\"Sigmoid' (max=0.25)\")\n",
                "plt.plot(z_range, tanh_derivative(z_range), 'g-', linewidth=2, label=\"Tanh' (max=1.0)\")\n",
                "plt.plot(z_range, relu_derivative(z_range), 'r-', linewidth=2, label=\"ReLU' (0 or 1)\")\n",
                "\n",
                "plt.axhline(y=0.25, color='blue', linestyle=':', alpha=0.5)\n",
                "plt.axhline(y=1.0, color='gray', linestyle=':', alpha=0.5)\n",
                "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
                "\n",
                "plt.xlabel('Input (z)', fontsize=12)\n",
                "plt.ylabel('Gradient', fontsize=12)\n",
                "plt.title('Derivatives Comparison - Shows Vanishing Gradient!', fontsize=14)\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.savefig('outputs/all_derivatives.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 4: Complete Numerical Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_inputs = np.array([-5, -2, -0.5, 0, 0.5, 2, 5])\n",
                "\n",
                "print(\"COMPLETE NUMERICAL COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"{'Input':<8} {'Sigmoid':<10} {'Sig.Grad':<10} {'Tanh':<10} {'Tanh.Grad':<10} {'ReLU':<8} {'ReLU.Grad':<10}\")\n",
                "print(\"-\" * 78)\n",
                "\n",
                "for z in test_inputs:\n",
                "    print(f\"{z:<8.1f} {sigmoid(z):<10.4f} {sigmoid_derivative(z):<10.4f} \"\n",
                "          f\"{tanh(z):<10.4f} {tanh_derivative(z):<10.4f} {relu(z):<8.1f} {relu_derivative(z):<10.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Step 5: Gradient Analysis at Key Points"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"GRADIENT COMPARISON AT KEY POINTS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for x in [-2, 0, 2, 5]:\n",
                "    print(f\"\\nAt x = {x}:\")\n",
                "    print(f\"  Sigmoid: {sigmoid_derivative(x):.6f}\")\n",
                "    print(f\"  Tanh:    {tanh_derivative(x):.6f}\")\n",
                "    print(f\"  ReLU:    {relu_derivative(x):.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Insights\n",
                "\n",
                "### At z = 5 (far from center):\n",
                "- Sigmoid gradient: **0.0066** (vanishing!)\n",
                "- Tanh gradient: **0.0002** (vanishing!)\n",
                "- ReLU gradient: **1.0** (perfect!)\n",
                "\n",
                "### This is why ReLU enabled deep learning!\n",
                "\n",
                "---\n",
                "\n",
                "## Decision Guide\n",
                "\n",
                "| Need | Use |\n",
                "|------|-----|\n",
                "| Binary classification output | Sigmoid |\n",
                "| Multi-class output | Softmax |\n",
                "| Hidden layers (default) | ReLU |\n",
                "| RNNs/LSTMs | Tanh |\n",
                "| Dead neurons problem | LeakyReLU |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ’¼ Interview Summary\n",
                "\n",
                "1. **Sigmoid**: max grad 0.25, use for binary OUTPUT only\n",
                "2. **Tanh**: max grad 1.0, zero-centered, use for RNNs\n",
                "3. **ReLU**: grad = 1 for ALL positive, enabled deep learning\n",
                "4. **Trade-off**: Sigmoid/Tanh = vanishing gradient, ReLU = dead neurons\n",
                "5. **Winner**: ReLU for hidden layers (vanishing gradient is worse than dead neurons)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}