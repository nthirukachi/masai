{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß© K-Means Feature Augmentation + Perceptron Cross-Validation\n",
                "\n",
                "## üß© Problem Statement\n",
                "\n",
                "**What problem are we solving?**\n",
                "\n",
                "We want to answer an important question in machine learning: **Can we improve a simple classifier by adding cluster information as new features?**\n",
                "\n",
                "Think of it this way:\n",
                "- You're trying to predict if a wine belongs to a specific type (Class 0) or not.\n",
                "- The classifier only knows chemical measurements (like alcohol content, color intensity, etc.).\n",
                "- But what if we tell the classifier: *\"Hey, this wine is similar to wines in Group 2, and very different from wines in Group 0\"*?\n",
                "- Would that extra information help make better predictions?\n",
                "\n",
                "**Why does this matter?**\n",
                "\n",
                "In real-world machine learning:\n",
                "- Raw features alone might not be enough\n",
                "- **\"Feature Engineering\"** (creating new features) can boost performance\n",
                "- But it also adds complexity‚Äîwe need to prove it's worth it!\n",
                "\n",
                "**Real-World Relevance:**\n",
                "- **Customer Segmentation:** Group customers into types (budget, premium, luxury), then use that grouping to predict churn\n",
                "- **Medical Diagnosis:** Cluster patients by symptoms, use cluster info to improve disease prediction\n",
                "- **Fraud Detection:** Group transactions into patterns, use pattern membership to detect anomalies\n",
                "\n",
                "---\n",
                "\n",
                "## ü™ú Steps to Solve the Problem\n",
                "\n",
                "**High-Level Approach:**\n",
                "\n",
                "1. **Load Data:** Get the Wine dataset (178 samples, 13 features, 3 classes)\n",
                "2. **Create Binary Labels:** Convert 3-class problem ‚Üí 2-class (Class 0 vs Others)\n",
                "3. **Set Up Cross-Validation:** Use 5-fold stratified splitting (fair evaluation)\n",
                "4. **For Each Fold:**\n",
                "   - **Standardize** features (fit on train only!)\n",
                "   - **Cluster** training data with K-Means (k=4)\n",
                "   - **Augment** features: Add cluster membership + distances to centroids\n",
                "   - Train **Baseline** Perceptron (original 13 features)\n",
                "   - Train **Enhanced** Perceptron (21 augmented features)\n",
                "   - Compare metrics (Accuracy, F1, Average Precision)\n",
                "5. **Statistical Testing:** Check if differences are significant or just luck\n",
                "6. **Recommendation:** Should we use this in production?\n",
                "\n",
                "**Plain-English Reasoning:**\n",
                "\n",
                "Imagine you're a teacher trying to predict which students will pass:\n",
                "- **Baseline:** You only know exam scores (13 subjects)\n",
                "- **Enhanced:** You also know student \"type\" (Nerd, Athlete, Artist, Socialite) and how similar each student is to each type\n",
                "- Question: Does knowing the \"type\" help you predict better?\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Expected Output (OVERALL)\n",
                "\n",
                "**What will we get at the end?**\n",
                "\n",
                "1. **Cross-Validation Metrics Table** (CSV file)\n",
                "   - Shows Accuracy, F1, Average Precision for both pipelines\n",
                "   - 5 rows (one per fold) + summary row with Mean ¬± Std\n",
                "\n",
                "2. **Comparison Bar Plot** (PNG image)\n",
                "   - Visual comparison of Baseline vs Enhanced\n",
                "   - Error bars showing variability\n",
                "\n",
                "3. **Statistical Test Results** (printed output)\n",
                "   - p-values for each metric\n",
                "   - **p < 0.05** = difference is significant! üéâ\n",
                "   - **p ‚â• 0.05** = difference could be random ü§∑\n",
                "\n",
                "4. **Executive Summary** (400-450 word TXT file)\n",
                "   - Professional recommendation\n",
                "   - \"Should we use K-Means augmentation in production?\"\n",
                "   - Considers statistical significance AND practical complexity\n",
                "\n",
                "**Success Criteria:**\n",
                "- Enhanced improves at least 2 metrics, OR\n",
                "- We have evidence-based reasons why it doesn't improve\n",
                "- Summary references statistical significance clearly\n",
                "- Discussion of operational impact (complexity, runtime, etc.)\n",
                "\n",
                "**Sample Interpretation:**\n",
                "- If Enhanced Accuracy = 0.95, Baseline = 0.88, p = 0.02:\n",
                "  - **Interpretation:** \"Enhanced is 7% better, and this is statistically significant (p=0.02 < 0.05). The improvement is REAL, not luck!\"\n",
                "- If Enhanced F1 = 0.91, Baseline = 0.90, p = 0.45:\n",
                "  - **Interpretation:** \"Enhanced is slightly better, but p=0.45 means this could easily be random chance. No strong evidence of improvement.\"\n",
                "\n",
                "---\n",
                "\n",
                "Now let's dive into the code! üöÄ"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# üì¶ SECTION 1: IMPORTS\n",
                "\n",
                "Before we can solve any problem, we need to bring in the tools (libraries) we'll use.\n",
                "\n",
                "Think of this like going to a toolbox:\n",
                "- **NumPy** = Calculator for arrays and numbers\n",
                "- **Pandas** = Excel for Python\n",
                "- **Matplotlib** = Drawing tools for charts\n",
                "- **Scikit-learn** = Machine learning toolkit\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Import NumPy - The Calculator for Arrays\n",
                "\n",
                "### 2.1 What does this line do?\n",
                "Imports NumPy library and gives it a short name `np` so we can use it easily.\n",
                "\n",
                "### 2.2 Why is NumPy used?\n",
                "- Python lists are slow for math operations\n",
                "- NumPy arrays are 100x faster (written in C language)\n",
                "- We need it for:\n",
                "  - Array operations (like adding all elements)\n",
                "  - Mathematical functions (mean, standard deviation, square root)\n",
                "  - Distance calculations (Euclidean distance)\n",
                "\n",
                "**Is this the only way?**\n",
                "- You could use pure Python lists and loops, but it would be MUCH slower\n",
                "- For 178 samples it's okay, but for 1 million samples, NumPy is essential\n",
                "- **Why NumPy is better:** Speed + less code + built-in functions\n",
                "\n",
                "### 2.3 When to use NumPy?\n",
                "- When working with numbers, matrices, or mathematical operations\n",
                "- Almost always in data science and machine learning\n",
                "\n",
                "### 2.4 Where is NumPy used in real projects?\n",
                "- Image processing (images are arrays of pixels)\n",
                "- Financial analysis (stock prices, calculations)\n",
                "- Scientific computing (physics simulations)\n",
                "- Machine learning (all data is stored as arrays)\n",
                "\n",
                "### 2.5 How to use NumPy?\n",
                "**Syntax:**\n",
                "```python\n",
                "import numpy as np\n",
                "```\n",
                "\n",
                "**Example:**\n",
                "```python\n",
                "# Create an array\n",
                "arr = np.array([1, 2, 3, 4, 5])\n",
                "\n",
                "# Calculate mean\n",
                "mean = np.mean(arr)  # Result: 3.0\n",
                "\n",
                "# Square all elements\n",
                "squared = arr ** 2  # Result: [1, 4, 9, 16, 25]\n",
                "```\n",
                "\n",
                "### 2.6 How does NumPy work internally?\n",
                "- NumPy stores data in contiguous memory blocks (all together, not scattered)\n",
                "- Operations are done in compiled C code (not slow Python loops)\n",
                "- Uses SIMD (Single Instruction Multiple Data) for parallel processing\n",
                "- Example: Adding two arrays of 1000 elements happens in one CPU instruction!\n",
                "\n",
                "### 2.7 Output with sample example\n",
                "```python\n",
                "import numpy as np\n",
                "print(np.__version__)  # Shows NumPy version, e.g., '1.24.3'\n",
                "print(type(np))        # Shows: <class 'module'>\n",
                "```\n",
                "\n",
                "No visible output when importing, but `np` is now available to use throughout the code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã Import Pandas - Excel for Python\n",
                "\n",
                "### 2.1 What does this line do?\n",
                "Imports the Pandas library with the standard nickname `pd`.\n",
                "\n",
                "### 2.2 Why is Pandas used?\n",
                "- NumPy is great for arrays, but Pandas is great for **tables with labels**\n",
                "- We need it to:\n",
                "  - Create tables (DataFrames) for our metrics\n",
                "  - Save results to CSV files\n",
                "  - Display data in a nice, readable format\n",
                "  - Handle row/column names (like Excel)\n",
                "\n",
                "**Is this the only way?**\n",
                "- Could use pure NumPy arrays, but no column names or labels\n",
                "- Could write CSV manually with Python's `csv` module, but much more code\n",
                "- **Why Pandas is better:** Built-in CSV support, pretty printing, column names\n",
                "\n",
                "### 2.3 When to use Pandas?\n",
                "- When you have tabular data (rows and columns with labels)\n",
                "- When you need to save/load CSV, Excel files\n",
                "- When you want to display results in a table format\n",
                "\n",
                "### 2.4 Where is Pandas used in real projects?\n",
                "- Data analysis (like Excel but programmable)\n",
                "- Business reports (sales data, metrics dashboards)\n",
                "- Data cleaning (removing duplicates, handling missing values)\n",
                "- Financial modeling (stock portfolios, risk analysis)\n",
                "\n",
                "### 2.5 How to use Pandas?\n",
                "**Syntax:**\n",
                "```python\n",
                "import pandas as pd\n",
                "```\n",
                "\n",
                "**Example:**\n",
                "```python\n",
                "# Create a DataFrame (table)\n",
                "df = pd.DataFrame({\n",
                "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
                "    'Age': [25, 30, 35],\n",
                "    'Score': [88, 92, 85]\n",
                "})\n",
                "\n",
                "# Save to CSV\n",
                "df.to_csv('results.csv', index=False)\n",
                "\n",
                "# Display table\n",
                "print(df)\n",
                "```\n",
                "\n",
                "### 2.6 How does Pandas work internally?\n",
                "- Built on top of NumPy (uses NumPy arrays underneath)\n",
                "- Adds labels (row index + column names) to NumPy arrays\n",
                "- Provides convenient methods for common operations\n",
                "- Example: `df.mean()` calculates mean of each column automatically\n",
                "\n",
                "### 2.7 Output with sample example\n",
                "```python\n",
                "import pandas as pd\n",
                "print(pd.__version__)  # Shows Pandas version, e.g., '2.0.3'\n",
                "```\n",
                "\n",
                "No visible output when importing, but now we can create DataFrames!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Import Matplotlib - The Plotting Library\n",
                "\n",
                "### 2.1 What does this line do?\n",
                "Imports the `pyplot` module from Matplotlib and nicknames it `plt`.\n",
                "\n",
                "### 2.2 Why is Matplotlib used?\n",
                "- We need to **visualize** our results (a picture is worth 1000 numbers!)\n",
                "- Will create a bar chart comparing baseline vs enhanced metrics\n",
                "- Humans understand charts faster than tables of numbers\n",
                "- We'll save the plot as a PNG file for the report\n",
                "\n",
                "**Is this the only way?**\n",
                "- Alternatives: Seaborn (prettier but built on Matplotlib), Plotly (interactive)\n",
                "- **Why Matplotlib is better here:** Most versatile, works everywhere, standard in ML\n",
                "\n",
                "### 2.3 When to use Matplotlib?\n",
                "- Whenever you need to create charts, graphs, or visualizations\n",
                "- Line plots, scatter plots, bar charts, histograms, etc.\n",
                "\n",
                "### 2.4 Where is Matplotlib used in real projects?\n",
                "- Research papers (all figures and charts)\n",
                "- Business dashboards (sales trends, KPI visualizations)\n",
                "- Machine learning (loss curves, confusion matrices)\n",
                "- Scientific visualization (experiment results)\n",
                "\n",
                "### 2.5 How to use Matplotlib?\n",
                "**Syntax:**\n",
                "```python\n",
                "import matplotlib.pyplot as plt\n",
                "```\n",
                "\n",
                "**Example:**\n",
                "```python\n",
                "# Create simple bar chart\n",
                "plt.bar(['Baseline', 'Enhanced'], [0.85, 0.92])\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Model Comparison')\n",
                "plt.savefig('comparison.png')\n",
                "plt.show()\n",
                "```\n",
                "\n",
                "### 2.6 How does Matplotlib work internally?\n",
                "- Creates a \"figure\" object (like a canvas)\n",
                "- Adds \"axes\" (the actual plot area)\n",
                "- Renders graphics using a backend (AGG for PNG, TkAgg for interactive)\n",
                "- Can save as vector formats (PDF, SVG) or raster (PNG, JPG)\n",
                "\n",
                "### 2.7 Output with sample example\n",
                "```python\n",
                "import matplotlib.pyplot as plt\n",
                "# No output when importing, but plotting functions are now available\n",
                "```\n",
                "\n",
                "After import, we can create plots!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üç∑ Import load_wine - The Wine Dataset\n",
                "\n",
                "### 2.1 What does this line do?\n",
                "Imports the `load_wine` function from scikit-learn's datasets module.\n",
                "\n",
                "### 2.2 Why is this used?\n",
                "- This is our **DATA SOURCE**!\n",
                "- The Wine dataset contains:\n",
                "  - 178 samples (wines)\n",
                "  - 13 features (chemical measurements like alcohol, acidity, color)\n",
                "  - 3 classes (types of wine: 0, 1, 2)\n",
                "- It's built into scikit-learn, so no download needed\n",
                "- Perfect for learning and experiments\n",
                "\n",
                "**Is this the only way?**\n",
                "- Could load from CSV file using `pd.read_csv()`\n",
                "- Could download from UCI Machine Learning Repository\n",
                "- **Why load_wine is better here:** Instant access, standardized format, clean data\n",
                "\n",
                "### 2.3 When to use load_wine?\n",
                "- When practicing classification algorithms\n",
                "- When you need a small, clean, multi-class dataset\n",
                "- For educational demonstrations\n",
                "\n",
                "### 2.4 Where is this dataset used?\n",
                "- Machine learning courses (like this one!)\n",
                "- Algorithm benchmarking\n",
                "- Research on classification methods\n",
                "- Wine industry (originally collected for wine origin analysis)\n",
                "\n",
                "### 2.5 How to use load_wine?\n",
                "**Syntax:**\n",
                "```python\n",
                "from sklearn.datasets import load_wine\n",
                "```\n",
                "\n",
                "**Example:**\n",
                "```python\n",
                "wine = load_wine()\n",
                "X = wine.data          # Features (178, 13)\n",
                "y = wine.target        # Labels (178,) with values 0, 1, 2\n",
                "names = wine.feature_names  # Names of the 13 features\n",
                "\n",
                "# Print first sample\n",
                "print(X[0])  # Array of 13 chemical measurements\n",
                "print(y[0])  # Class label (0, 1, or 2)\n",
                "```\n",
                "\n",
                "### 2.6 How does load_wine work internally?\n",
                "- The data is embedded in scikit-learn's installation files\n",
                "- When called, it loads from disk into memory\n",
                "- Returns a \"Bunch\" object (dictionary-like)\n",
                "- Keys: `data`, `target`, `feature_names`, `DESCR` (description)\n",
                "\n",
                "### 2.7 Output with sample example\n",
                "```python\n",
                "from sklearn.datasets import load_wine\n",
                "wine = load_wine()\n",
                "print(wine.data.shape)     # (178, 13) - 178 samples, 13 features\n",
                "print(wine.target.shape)   # (178,) - 178 labels\n",
                "print(wine.target[:5])     # [0, 0, 0, 0, 0] - first 5 labels\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import load_wine"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìè Import StandardScaler - Feature Normalization\n",
                "\n",
                "### 2.1 What does this line do?\n",
                "Imports the `StandardScaler` class from scikit-learn's preprocessing module.\n",
                "\n",
                "### 2.2 Why is StandardScaler used?\n",
                "**The Problem:**\n",
                "- The wine dataset has features on different scales:\n",
                "  - Alcohol: 11-14% (range of ~3)\n",
                "  - Proline: 278-1680 (range of ~1400)\n",
                "- K-Means uses **Euclidean distance**: ‚àö((x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤)\n",
                "- If proline ranges from 278-1680, it will DOMINATE the distance calculation!\n",
                "- Alcohol (range 3) would barely matter\n",
                "\n",
                "**The Solution:**\n",
                "- StandardScaler transforms each feature to have:\n",
                "  - Mean = 0\n",
                "  - Standard Deviation = 1\n",
                "- Now all features contribute equally to distance calculations\n",
                "\n",
                "**Real-life analogy:**\n",
                "Imagine comparing students:\n",
                "- Student A: Math=90, English=85, Science=88\n",
                "- Student B: Math=80, English=82, Science=85\n",
                "- But Math is out of 100, English out of 100, Science out of 1000 (unfair!)\n",
                "- StandardScaler would convert Science from 0-1000 to the same scale as Math and English\n",
                "\n",
                "**Is this the only way?**\n",
                "- **MinMaxScaler:** Scales to [0, 1] range\n",
                "  - Use when: You want bounded values (e.g., image pixels 0-255 ‚Üí 0-1)\n",
                "- **RobustScaler:** Uses median and IQR (less sensitive to outliers)\n",
                "  - Use when: Your data has extreme outliers\n",
                "- **Why StandardScaler is better here:**\n",
                "  - K-Means assumes circular/spherical clusters (StandardScaler preserves shape)\n",
                "  - Wine data doesn't have extreme outliers\n",
                "  - Industry standard for distance-based algorithms\n",
                "\n",
                "### 2.3 When to use StandardScaler?\n",
                "- **ALWAYS** before K-Means, KNN, SVM (distance-based algorithms)\n",
                "- **ALWAYS** before Perceptron, Logistic Regression (gradient descent benefits from scaling)\n",
                "- **NOT NEEDED** for tree-based models (Decision Trees, Random Forest‚Äîthey split by thresholds, not distances)\n",
                "\n",
                "### 2.4 Where is StandardScaler used in real projects?\n",
                "- Customer segmentation (clustering on age, income, purchases‚Äîall different scales)\n",
                "- Recommendation systems (collaborative filtering with different feature types)\n",
                "- Medical diagnosis (lab values: blood pressure 120, glucose 100, cholesterol 200‚Äîdifferent scales)\n",
                "\n",
                "### 2.5 How to use StandardScaler?\n",
                "**Syntax:**\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "```\n",
                "\n",
                "**Example:**\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import numpy as np\n",
                "\n",
                "# Sample data (before scaling)\n",
                "X = np.array([[1, 1000],\n",
                "              [2, 2000],\n",
                "              [3, 3000]])\n",
                "\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(X)  # Learn mean and std from data\n",
                "X_scaled = scaler.transform(X)  # Apply transformation\n",
                "\n",
                "print(\"Before scaling:\")\n",
                "print(X)\n",
                "# [[   1 1000]\n",
                "#  [   2 2000]\n",
                "#  [   3 3000]]\n",
                "\n",
                "print(\"\\nAfter scaling:\")\n",
                "print(X_scaled)\n",
                "# [[-1.22 -1.22]\n",
                "#  [ 0.    0.  ]\n",
                "#  [ 1.22  1.22]]\n",
                "\n",
                "# Check: mean is now ~0, std is now ~1\n",
                "print(\"\\nMean:\", X_scaled.mean(axis=0))  # [0, 0]\n",
                "print(\"Std:\", X_scaled.std(axis=0))      # [1, 1]\n",
                "```\n",
                "\n",
                "### 2.6 How does StandardScaler work internally?\n",
                "**Step-by-step process:**\n",
                "\n",
                "1. **During `.fit(X_train)`:**\n",
                "   - Calculate mean: Œº = (x‚ÇÅ + x‚ÇÇ + ... + x‚Çô) / n\n",
                "   - Calculate standard deviation: œÉ = ‚àö(Œ£(x·µ¢ - Œº)¬≤ / n)\n",
                "   - Store Œº and œÉ for each feature\n",
                "\n",
                "2. **During `.transform(X)`:**\n",
                "   - For each feature: x_scaled = (x - Œº) / œÉ\n",
                "   - This centers data (mean=0) and scales variance (std=1)\n",
                "\n",
                "**Why fit on train only?**\n",
                "- In production, we won't know test data's mean/std\n",
                "- Must use training mean/std to transform test data\n",
                "- Otherwise we \"leak\" information from test to train\n",
                "\n",
                "**Math Example:**\n",
                "```\n",
                "Feature: [100, 200, 300]\n",
                "Mean Œº = 200\n",
                "Std œÉ = 81.65\n",
                "\n",
                "Transform:\n",
                "100 ‚Üí (100 - 200) / 81.65 = -1.22\n",
                "200 ‚Üí (200 - 200) / 81.65 =  0.00\n",
                "300 ‚Üí (300 - 200) / 81.65 = +1.22\n",
                "```\n",
                "\n",
                "### 2.7 Output with sample example\n",
                "```python\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "# No output when importing, but now we can create scalers\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Import KMeans - The Clustering Algorithm\n",
                "\n",
                "### 2.1 What does this line do?\n",
                "Imports the `KMeans` class from scikit-learn's cluster module.\n",
                "\n",
                "### 2.2 Why is KMeans used?\n",
                "**What is K-Means?**\n",
                "- An **unsupervised** algorithm that groups data into k clusters\n",
                "- \"Unsupervised\" means it doesn't need labels‚Äîit finds patterns on its own!\n",
                "- We'll use it to find k=4 natural groups in the wine data\n",
                "\n",
                "**Why do we need clustering here?**\n",
                "- We want to create NEW features from these clusters:\n",
                "  1. **One-hot membership:** \"This wine belongs to cluster 2\"\n",
                "  2. **Distances to centroids:** \"This wine is very close to cluster 2 center, far from cluster 0\"\n",
                "- These new features give the Perceptron more information!\n",
                "\n",
                "**Real-life analogy:**\n",
                "Imagine grouping students:\n",
                "- Give K-Means student grades ‚Üí it finds 4 types: Nerds, Athletes, Artists, Socialites\n",
                "- Now when predicting \"will pass exam?\", you can say:\n",
                "  - \"Student is a Nerd (cluster 0)\"\n",
                "  - \"Student is 90% similar to Nerd type, 10% to Athlete type\"\n",
                "- This helps the predictor make better decisions!\n",
                "\n",
                "**Is this the only way?**\n",
                "- **Hierarchical Clustering:** Builds a tree of clusters (good for dendrograms)\n",
                "  - Use when: You want to see cluster hierarchy, not sure about k\n",
                "- **DBSCAN:** Finds arbitrary-shaped clusters, handles noise\n",
                "  - Use when: Clusters aren't spherical, lots of outliers\n",
                "- **Gaussian Mixture Models (GMM):** Probabilistic clustering\n",
                "  - Use when: You want soft assignments (\"30% cluster A, 70% cluster B\")\n",
                "- **Why K-Means is better here:**\n",
                "  - Fast and simple (scales to large datasets)\n",
                "  - Works well when clusters are roughly spherical (wine data is)\n",
                "  - Problem specifies k=4 (K-Means requires k, others don't)\n",
                "  - Gives hard assignments (easier to create one-hot features)\n",
                "\n",
                "### 2.3 When to use K-Means?\n",
                "- When you want to find k groups in unlabeled data\n",
                "- When clusters are roughly circular/spherical\n",
                "- When you know (or can estimate) k\n",
                "- When speed is important (K-Means is very fast)\n",
                "\n",
                "### 2.4 Where is K-Means used in real projects?\n",
                "- **Customer Segmentation:** Group customers by behavior (high spenders, bargain hunters, etc.)\n",
                "- **Image Compression:** Group similar colors together (reduce 16 million colors to 16)\n",
                "- **Document Clustering:** Group news articles by topic\n",
                "- **Anomaly Detection:** Samples far from all centroids are outliers\n",
                "- **Feature Engineering:** (Like this project!) Use cluster info as new features\n",
                "\n",
                "### 2.5 How to use K-Means?\n",
                "**Syntax:**\n",
                "```python\n",
                "from sklearn.cluster import KMeans\n",
                "```\n",
                "\n",
                "**Example:**\n",
                "```python\n",
                "from sklearn.cluster import KMeans\n",
                "import numpy as np\n",
                "\n",
                "# Sample data: 2D points\n",
                "X = np.array([[1, 2], [1, 4], [1, 0],\n",
                "              [10, 2], [10, 4], [10, 0]])\n",
                "\n",
                "# Create KMeans model with k=2 clusters\n",
                "kmeans = KMeans(n_clusters=2, random_state=42)\n",
                "\n",
                "# Fit model (find centroids)\n",
                "kmeans.fit(X)\n",
                "\n",
                "# Get cluster assignments\n",
                "labels = kmeans.labels_\n",
                "print(\"Cluster assignments:\", labels)\n",
                "# Output: [0, 0, 0, 1, 1, 1] - first 3 in cluster 0, next 3 in cluster 1\n",
                "\n",
                "# Get centroids\n",
                "print(\"Centroids:\", kmeans.cluster_centers_)\n",
                "# Output: [[ 1.  2.]\n",
                "#          [10.  2.]] - two cluster centers\n",
                "\n",
                "# Predict cluster for new data\n",
                "new_point = [[0, 0]]\n",
                "print(\"New point cluster:\", kmeans.predict(new_point))\n",
                "# Output: [0] - closer to first cluster\n",
                "```\n",
                "\n",
                "### 2.6 How does K-Means work internally?\n",
                "**The Algorithm (Lloyd's Algorithm):**\n",
                "\n",
                "1. **Initialization:**\n",
                "   - Randomly place k centroids in the data space\n",
                "   - (Or use smarter methods like K-Means++)\n",
                "\n",
                "2. **Assignment Step:**\n",
                "   - For each data point:\n",
                "     - Calculate distance to each centroid\n",
                "     - Assign to nearest centroid\n",
                "   - Example: point (5, 5) is closer to centroid at (6, 6) than (1, 1) ‚Üí assign to cluster 1\n",
                "\n",
                "3. **Update Step:**\n",
                "   - For each cluster:\n",
                "     - Calculate mean of all assigned points\n",
                "     - Move centroid to this mean position\n",
                "   - Example: Cluster has points [(1,2), (2,3), (3,4)] ‚Üí new centroid at (2, 3)\n",
                "\n",
                "4. **Repeat:**\n",
                "   - Keep alternating Assignment and Update\n",
                "   - Stop when centroids don't move (converged)\n",
                "   - Or when max iterations reached\n",
                "\n",
                "**Visual Example:**\n",
                "```\n",
                "Iteration 0: Random centroids\n",
                "  ‚óè(centroid) ..(points)\n",
                "  \n",
                "Iteration 1: Assign points to nearest centroid\n",
                "  ‚óè.. (cluster A)\n",
                "     ‚óè.. (cluster B)\n",
                "     \n",
                "Iteration 2: Move centroids to cluster means\n",
                "  .‚óè. (centroid moved to middle of cluster A)\n",
                "     .‚óè. (centroid moved to middle of cluster B)\n",
                "     \n",
                "... repeat until stable\n",
                "```\n",
                "\n",
                "**Why n_init=10?**\n",
                "- K-Means can get stuck in local optima (bad starting centroids)\n",
                "- Solution: Run 10 times with different random starts\n",
                "- Keep the best result (lowest \"inertia\" = sum of distances to centroids)\n",
                "\n",
                "**What is inertia?**\n",
                "- Sum of squared distances from each point to its centroid\n",
                "- Inertia = Œ£(distance to centroid)¬≤\n",
                "- Lower inertia = tighter, better clusters\n",
                "\n",
                "### 2.7 Output with sample example\n",
                "```python\n",
                "from sklearn.cluster import KMeans\n",
                "import numpy as np\n",
                "\n",
                "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
                "kmeans = KMeans(n_clusters=2, random_state=42)\n",
                "kmeans.fit(X)\n",
                "\n",
                "print(\"Labels:\", kmeans.labels_)\n",
                "# Output: [0 0 0 1 1 1]\n",
                "# Means: first 3 points in cluster 0, next 3 in cluster 1\n",
                "\n",
                "print(\"Centroids shape:\", kmeans.cluster_centers_.shape)\n",
                "# Output: (2, 2)\n",
                "# Means: 2 centroids, each is a 2D point\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}